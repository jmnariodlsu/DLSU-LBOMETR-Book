[["index.html", "LBOMETR Course Book 1 Introduction 1.1 About Me", " LBOMETR Course Book Jem Marie M. Nario 2026-02-15 1 Introduction Welcome to the LBOMETR Course Book! This book is designed to guide students through the course by providing all necessary resources, materials, and instructions. LBOMETR This course book is intended to ensure that DLSU Carlos L. Tiu-School of Economics students will be able to learn more about Econometrics using R. You will find sections on the syllabus, course assessments, and group projects, as well as guidance for navigating the course effectively. 1.1 About Me My name is Jem Marie M. Nario, and I am your lecturer for this course. I am excited to guide you through this journey of learning and discovery since I am also on a journey of learning and discovery while teaching part-time. This book is a trial version whnb ich will be updated along the course as it also serves as a practice for me. Email: jem.nario@dlsu.edu.ph LinkedIn: linkedin.com/in/jmnario/ Feel free to reach out with any questions or concerns throughout the course. "],["syllabus.html", "2 Syllabus 2.1 General Course Description 2.2 Specific Course Description 2.3 Final Course Outputs 2.4 Generative AI-Use Policy 2.5 Classroom Policies", " 2 Syllabus 2.1 General Course Description This course introduces Economics majors to more advanced commands and techniques used in econometric software package R commonly used in empirical research. 2.2 Specific Course Description This course familiarizes Economics majors with advanced techniques in R for empirical research. Students will also gather and analyze their own real-world data to investigate a specific economic problem, applying econometric methods and presenting their findings in a final project. 2.3 Final Course Outputs CLOs Output Due Date CLO 1,2,4 Data Storytelling (OR) Week 10 CLO 1-4 Data Story Archive (PF/PR) 18:00 Tuesday Week 14 CLO 1-3 Problem Sets (QA) 18:00 Weeks 6 and 12 CLO 1,3 Short Quizzes (QA) Weeks 2-6, 8, 11-12 CLO 4 Class Participation (TW/OB) Weeks 1-12 2.4 Generative AI-Use Policy For each component of the final grade defined above, students must identify the generative AI usage policy level. The levels are: Free to Use – AI may be used without restriction. Allowed in Specific Contexts – AI may be used only for clearly defined purposes and must be cited. Banned – AI use is strictly prohibited. Grade Component Usage Policy Level Notes Data Storytelling (OR) Allowed in Specific Contexts Students may use generative AI only to brainstorm ideas, create outlines, and design slides. AI must not be used to generate or check final code or analysis. Data Story Archive (PF/PR) Banned AI use is strictly prohibited for writing, coding, or analyzing the archive. Work must reflect independent technical skill. Problem Sets (QA) Banned Students must independently write and debug code; AI may not generate or check solutions. Short Quizzes (QA) Banned AI use is not allowed during quizzes. Class Participation (TW/OB) Banned All participation must be based on students’ own understanding and contributions. This course is an applied coding and econometrics laboratory, designed to develop students’ ability to analyze real-world data, write reproducible code, and interpret econometric results independently. To support student learning and promote the responsible use of emerging technologies, students may use generative AI tools (e.g., ChatGPT, Gemini, etc.) only for conceptual or exploratory coding assistance, such as: ● Understanding function syntax or usage ● Exploring coding strategies ● Learning general debugging approaches The use of generative AI is strictly prohibited for the following: ● Generating or checking final code for the Data Story Archive (report and code) ● Generating or checking code for Problem Sets ● Short quizzes ● Any submission intended to reflect independent technical skill Overreliance on AI may hinder students’ understanding of key concepts, which are essential to success in this course. Any unauthorized use of AI will be treated as academic dishonesty, in accordance with the university’s academic integrity policy. Students are encouraged to consult the instructor if they are unsure about the appropriate use of AI for specific tasks. 2.5 Classroom Policies 1. Assessments All major assessments are described below. Deadlines, formats, and submission requirements are non-negotiable. Assessment Format &amp; Submission Notes Short Quizzes In-class, 2 coding questions per session. Students work with a buddy to check each other’s answers. Each student underlines economic theory and econometric reasoning in their buddy’s paper and places check marks for each. No quizzes on Weeks 6,7, 8, 9, 10, 12, 13, 14. Problem Set 1 Group submission (hard copy). Handwritten discussion + typed step-by-step R code + printed/pasted graphs. Covers material up to descriptive statistics. Due Week 6. Questions are provided on Day 1 in the LBOMETR Course Book. Problem Set 2 Group submission (hard copy). Handwritten discussion + typed step-by-step R code + printed/pasted graphs. Covers material from descriptive statistics to formal tests of assumptions. Due Week 12. Questions are provided on Day 1 in the LBOMETR Course Book. Data Storytelling (Oral Presentation) Group presentation in HyFlex classroom. Must include R visualizations and clear explanation of methodology, results, and conclusions. Graded individually, based on contribution, clarity, and engagement. Data Story Archive Group submission (hard copy) compiling R scripts, analyses, visualizations, and interpretations. Demonstrates independent mastery; must be concise, reproducible, and complete. AI or outside generation is not allowed. Hard copy must be submitted by Week 14. 2. Groupings Students will be divided into 5 groups of 6 on the first day of class. Groups are assigned based on in-class skill surveys: comfort with R and economic theory. Groups remain fixed throughout the semester. Roles (e.g., introduction, data cleaning, analysis, visualization, discussion) may rotate within the group for fairness, except for the assigned monitor if used. 3. Appointments &amp; Consultations Data Story Topic Consultation: Groups must meet with the professor before Week 5. Topic, methodology, or scope changes after Week 6 without notice will incur a 50% deduction on the Data Story grade. Mock Presentation &amp; Archive Consultation: Groups are encouraged to schedule 15-minute in-person sessions after Mock Presentation submission to improve the Data Story Archive. Slots are booked via a provided link. Other Consultations: Can be done via email. Lecturer responds only between 8 AM – 6 PM. 4. Grading Notes Perfect scores (100%) are very rare. Excellent students typically achieve up to ~95%. Grades reflect demonstrated mastery, quality of outputs, and adherence to rubrics. Attendance, participation, buddy system compliance, and group contributions are part of the class participation grade. No general incentives or extra credit are provided. 5. Class Monitor Responsibilities One student monitor per session, rotating weekly or biweekly, will oversee the buddy system. Responsibilities: Collect all buddy-checked papers after class. Record for each student: Buddy’s name who checked the paper. Number of checks received (0, 1, or 2). Ensure all buddy system procedures are followed (underlines and check marks). Submit a tally sheet to the professor by 6 PM of class day. Important: The monitor does not grade anything. Failure to follow these steps may affect participation points for the monitor or the class. 6. Attendance Attendance is monitored during in-person sessions. Attendance points contribute to class participation. Unexcused absence will not be able to have full marks in class participation and might miss short quizzes. Excused absence: no deduction (documentation required). 7. Buddy System Students must complete buddy checks for short quizzes as described in Section 1. Compliance is verified by the class monitor and reflected in participation scores. 8. Problem Sets Students are expected to complete and submit the assigned problem sets on time, including discussion, code, and visualizations as described above. 9. Data Storytelling &amp; Data Story Archive Follow all guidelines for oral presentations and written archive submissions, including hard copy submission and proper documentation of R code. Consultations are highly recommended to refine presentations and archive outputs. 10. Use of Course Materials All learning and teaching materials provided in this class are for the exclusive use of students enrolled in this course, Term 2, AY2025-26. Students are not allowed to upload, share, or distribute these materials publicly or to anyone other than their groupmates/classmates without the instructor’s permission. 2.5.1 EXCUSED ABSENCES POLICY: Students must process requests for excused absences from their respective Associate Deans. For SOE students, the Associate Dean of the School of Economics will only process requests for excused absences due to medical and mental health reasons. Covid-related leave requests are no longer accepted as of August 30, 2023. Students must provide official documentation from the Office of Student Affairs (OSA) for absences related to official university functions.  Procedure: Timing: Submit requests immediately upon returning to campus and no later than seven working days from the return date.  Request Letter: Write a letter to the Associate Dean including: ○ Course details (course name, section, faculty names, and emails) ○ Dates of absence(s) ○ Reason for absence with relevant details Supporting Documents: Attach validated documents from the appropriate university offices. Submission: Combine the letter and supporting documents into a single PDF and upload viathis form. Approved absences will be communicated to you and your professors within three working days. Note that processing is done only during regular weekday office hours.  Important: Only complete requests will be processed. Falsifying records is a major offense as per the Student Handbook (Section 9, pp. 85-87). Note: The syllabus is unique to the course per Term, per AY. Final syllabus uploaded in Animospace. "],["course-assessments.html", "3 Course Assessments 3.1 Short Quizzes 3.2 Problem Sets (1 and 2) 3.3 Data Storytelling 3.4 Data Story Archive 3.5 Participation 3.6 Grading System 3.7 Grading Scale", " 3 Course Assessments 3.1 Short Quizzes Format: In-class, handwritten, two questions per session. Questions involve: Coding: Write R code to solve the problem. Discussion: Explain your solution in terms of: Economic theory behind the answer. Econometric reasoning (why variables/methods were chosen). Visuals can be shown to your buddy, who will check your work. Buddy System: Each student pairs with a seatmate to verify the discussion: Underline economic theory and econometric reasoning in peer’s paper. Place a check for each component found. Class Monitor Responsibilities One student monitor per session, rotating weekly or biweekly, will oversee the buddy system. Responsibilities: Collect all buddy-checked papers after class. Record for each student: Buddy’s name who checked the paper. Number of checks received (0, 1, or 2). Ensure all buddy system procedures are followed (underlines and check marks). Submit a tally sheet to the professor by 6 PM of class day. Important: The monitor does not grade anything. Failure to follow these steps may affect participation points for the monitor or the class. Expectations: Both checks must be present to receive full participation credit. No shortcuts; discussion must reflect understanding. 3.2 Problem Sets (1 and 2) Topic Assignment and Scope Topics and scope for Problem Sets 1 and 2 will be assigned randomly on the first day of class. Assignment will be done by drawing from prepared topic papers. Assigned topics are final and will be finalized by Week 2. Students may not change, narrow, or expand their assigned scope without instructor approval. If two groups converge on substantively similar topics or datasets, both submissions will be penalized. Format: Hard copy submission only, with handwritten discussion. R code can be typed and printed for submission. Graphics must be printed and pasted into the submission. Guidelines: Problem Sets 1 &amp; 2 questions are provided in the LBOMETR Course Book on the first day of class. Each problem must include: Discussion: Handwritten explanation of the solution. R Script: Step-by-step, well-commented, reproducible code. Graphics: Printed plots, properly labeled. Submission deadlines: PS1: Week 6 (covers material until Descriptive Statistics). PS2: Week 12 (covers material from Descriptive Statistics to Formal Tests of Assumptions). Expectations: Students must demonstrate understanding of economic theory and econometric reasoning in discussion. The same groupings apply in all group-related outputs. Only essential outputs should be included; do not submit unnecessary or repetitive results. Problem Set Rubric (Group Graded) Criteria Exemplary (90–100) Satisfactory (78–89) Developing (72–77) Beginning (&lt;72) Weight Data Cleaning &amp; Preparation Data is fully imported, cleaned, and transformed. Missing values, inconsistencies, and errors are handled effectively. Decisions are logical and appropriate for messy datasets. Mostly clean; minor errors or inconsistencies remain. Basic understanding of cleaning demonstrated. Partially cleaned; several errors remain. Some steps missing or unclear. Data largely unclean or incorrectly processed. Minimal understanding of preparation. 25 Coding &amp; Step-by-Step Process Code is correct, reproducible, well-organized, and clearly commented. Workflow is logical and easy to follow. Mostly correct; minor errors or inefficiencies. Workflow mostly clear. Code partially correct or poorly organized. Workflow unclear or incomplete. Code largely incorrect, not reproducible, or disorganized. Workflow missing. 30 Analysis &amp; Interpretation (Discussion) Answers are complete, insightful, and clearly interpret results in context. Discussion shows critical thinking. Answers mostly correct; interpretation present but limited. Some insights missing. Answers partially correct; interpretation weak or incomplete. Answers incorrect or missing; interpretation absent. 25 Graphics &amp; Visualization Visuals are clear, professional, and effectively support the discussion. Correctly printed and pasted. Visuals mostly clear and relevant; minor issues. Basic or unclear visuals; limited relevance. Missing, incorrect, or irrelevant visuals. 10 Presentation &amp; Documentation Submission is neat, organized, and easy to follow. Typed code separated from handwritten discussion. All required components included. Submission mostly organized; minor issues in clarity or order. Submission somewhat disorganized; some components missing or hard to follow. Submission poorly organized, incomplete, or difficult to follow. 10 Total 100 points (group) 3.3 Data Storytelling Format: Live in-person presentation, slides submitted electronically. Mock Presentation Slides submitted via email at 21:00 a day before the mock presentation. Final Presentation Slides submitted as a link via email at 21:00 a day before the scheduled presentation. Guidelines: Presentation duration: 10 minutes, followed by 5-minute Q&amp;A. Structure: Introduction: Topic, research question, significance Methods: Data and analysis methodology Results: Key findings using R-generated visualizations Discussion &amp; Conclusion: Implications and actionable recommendations Each group member must actively participate, meaning, speak during the presentation. No cue cards or reading from their laptops, or cellphones; presenters must be familiar with their slides. Slides must reflect economic reasoning and econometric reasoning. Room to be used will be reserved for both online and in-person audiences. Feedback will be given during the Data Storytelling and must be considered when making the Data Story Archive. Data Storytelling Rubric (Individually Graded) Criteria Exemplary Satisfactory Developing Beginning Weight Content &amp; Narrative Quality Student clearly owns a specific section. Introduction, methods, and results are clear, concise, logically presented, and analytically interpreted. Conclusions or implications are insightful and actionable. Section is clear and correct but mostly descriptive. Interpretation is present but limited. Conclusions are solid but not fully actionable. Section is vague, rushed, or weakly connected. Interpretation is superficial. Conclusions are simplistic or partially missing. Section is unclear, disorganized, or incorrect. Major parts of discussion or conclusions are missing. 40 Visualizations &amp; Analytical Support Student demonstrates clear ownership of visuals or analytical elements. Visuals are professional, polished, and strongly support the story. Any dynamic elements are used effectively. Visuals are appropriate and support the analysis, but design or relevance could be improved. Minor flaws or missed opportunities. Visuals are basic, poorly designed, or loosely connected. Some key visual aids missing. Visuals are missing, irrelevant, or poorly designed. 30 Delivery, Engagement &amp; Individual Contribution Student presents confidently, speaks naturally, and clearly explains their section. Demonstrates preparation and understanding. Contribution is observable in the presentation itself. Delivery is generally clear. Student shows understanding but occasionally relies on notes. Contribution is evident but not fully developed. Delivery is hesitant or partially scripted. Understanding of section is weak. Contribution is unclear. Delivery is minimal or absent. Student cannot demonstrate understanding of their section. 30 Total 100 points (individual) 3.4 Data Story Archive Format: Single hard copy PDF submission for the instructor. Content Requirements: Cover Page: Title, group members, submission date. Table of Contents: Clear page references. Data Story Report: Maximum 12 pages. Introduction: Problem statement and research question. Methods: Data sources, methodology, analysis techniques. Results: Key findings with R-generated visuals. Discussion &amp; Conclusion: Implications and recommendations. References: Expected to have more than 10 references; APA format Appendix: Maximum 5 pages, supporting tables or plots only. R Scripts: Maximum 10 pages, rendered from Quarto Markdown. Must include data cleaning steps, outputs, and plots. Group Reflection: Strictly 2 pages. Discuss teamwork, learning outcomes, and growth in data analysis during the whole duration of the course. Submission Instructions: Deadline: 18:00 TUESDAY OF WEEK 14 Expectations: Archive must reflect independent group work. No generative AI or external assistance in final output. Data Story Archive Rubric (Group Graded) Criteria Exemplary Satisfactory Developing Beginning Weight Content &amp; Storytelling (Report) Report is clear, concise, logically structured, and fully explains the analysis. Results are interpreted correctly, insights are highlighted, and conclusions are actionable. No irrelevant or redundant material included. Report is generally clear and correct but may be somewhat descriptive or slightly unorganized. Interpretation of results is present but not fully analytical. Report has gaps in clarity, logic, or completeness. Results are mostly descriptive. Conclusions are weak or partially missing. Report is unclear, incomplete, or disorganized. Major parts of analysis, results, or conclusions are missing or incorrect. 40 Technical Work (Code &amp; Analysis) Code is correct, fully reproducible, neat, well-commented, and logically structured. Step-by-step workflow is clear. Only relevant outputs are shown; no unnecessary tables or raw dumps. Code is mostly correct and reproducible, but minor inefficiencies, clutter, or documentation gaps exist. Some irrelevant outputs may be present. Code runs but has errors, poor structure, weak documentation, or unclear workflow. Outputs may be excessive or partly irrelevant. Code is largely incorrect, incomplete, not reproducible, or disorganized. Outputs are missing or meaningless. 45 Presentation &amp; Organization (Archive) Archive is professional, well-organized, and easy to navigate. Report and code are clearly separated. File naming and readability are consistent and logical. Archive is generally organized but may have minor inconsistencies or minor clutter. Separation of report and code is acceptable. Archive is somewhat confusing or inconsistent. Report and code separation is unclear. Archive is poorly organized, confusing, or incomplete. Report and code are difficult to access or understand. 15 Total 100 points (group) 3.5 Participation Buddy System Form: Attendance &amp; Engagement and Buddy System Participation will be monitored and checked through the Buddy System Evaluation Form. Individual Evaluation Form: Each student must also complete a Group Work Evaluation Form assessing group mates . The form must be signed to certify accuracy and truthfulness. Submission Requirements: Submit the signed evaluation forms alongside your Data Story Archive Report. Grading Notes: Attendance &amp; Engagement: Tracked via buddy form (presence in class/workshop, active participation). Buddy System Participation: Accuracy of buddy checks, initials, and verification. Group Work Contribution: Quality, quantity, and timeliness of contributions to problem sets and presentations. Collaboration &amp; Communication: Professional, constructive, and coordinated communication within the group. Participation Rubric (Individually Graded) Criteria Exemplary (90–100) Satisfactory (78–89) Developing (72–77) Beginning (&lt;72) Weight Attendance &amp; Engagement Attends all meetings/classes and actively participates in discussions, activities, and workshops. Demonstrates initiative and preparedness. Attends most meetings/classes (≥80%) and participates adequately. Minor lapses in engagement. Attends some meetings/classes (≥60%) or participates inconsistently. Minimal contribution in discussions. Frequently absent (&lt;60%) or disengaged. Rarely participates. 25% Buddy System Participation Always performs buddy responsibilities accurately: checks seatmate’s discussion, underlines components, initials verification, ensures accountability. Usually performs buddy responsibilities correctly; occasional minor lapses. Sometimes performs buddy responsibilities; inconsistencies in marking or verifying. Rarely or never performs buddy responsibilities; no accountability. 20% Group Work Contribution Actively contributes to group activities (problem sets, data storytelling). Attends meetings, shares workload equitably, helps group succeed. Participates in group activities most of the time. Completes assigned tasks with minor support needed. Limited participation in group activities. Contributes only partially to assigned tasks. Minimal or no participation in group activities. Does not contribute to group tasks. 30% Collaboration &amp; Communication Communicates effectively and professionally within the group. Supports peers, resolves conflicts constructively, and helps coordinate tasks. Communicates adequately within the group. Minor issues in coordination or collaboration. Communication within the group is inconsistent; conflicts or coordination issues sometimes occur. Poor or absent communication. Causes confusion or conflict in group. 25% Total 100 points 3.6 Grading System Data Storytelling 25% Data Story Archive 35% Problem Sets (2) 15% Short Quizzes 15% Class Participation 10% Total 100% 3.7 Grading Scale 97.00 - 100.00 4.0 90.00 – 96.99 3.5 85.00 – 89.99 3.0 80.00 – 84.99 2.5 75.00 – 79.99 2.0 70.00 – 74.99 1.5 65.00 – 69.99 1.0 00.00 – 64.99 0.0 Note: For course credit, students should get a minimum score of 65.00% (equivalent to a grade of 1.0) "],["problem-sets.html", "4 Problem Sets 4.1 Introduction 4.2 Assigned Topics List (General and Specific) 4.3 Problem Set 1 - Data Management to Descriptive Statistics and Conceptual Endogeneity 4.4 Problem Set 2 - Testable Hypotheses, OLS, Diagnostics, Robustness 4.5 Reminders:", " 4 Problem Sets 4.1 Introduction You are a team assigned to conduct research on one of the following topics. Each group will receive a specific topic randomly from the list below. Your tasks are divided into two Problem Sets: Problem Set 1: Data Management Visualization Descriptive Statistics Conceptual Endogeneity Problem Set 2 Formulate Testable Hypotheses OLS Estimation Diagnostics Sensitivity/Robustness Checks Important: All datasets must come from PSA OpenSTAT or World Bank Open Data. If you want to use other datasets, you must inform the lecturer by Week 2. No Kaggle or any cleaned datasets allowed. 4.2 Assigned Topics List (General and Specific) Trade and Economic Outcomes a. Export volume and regional economic growth b. Imports, domestic production, and household income c. Trade openness and employment in key sectors Agriculture and Productivity a. Crop yield differences by farm size b. Fertilizer input and productivity c. Regional specialization in agriculture Money, Banking and Household Finance a. Household access to banking and income b. Regional credit availability and small business activity c. Income, savings, and household financial stability Stocks and Capital Markets a. Stock market index movements and GDP b. Stock volatility and investment or savings c. Economic shocks and market performance Education and Human Capital a. Educational attainment and earnings b. Regional schooling differences and income disparities c. Education spending and enrollment/completion rates Health and Economic Outcomes a. Health expenditure and labor productivity b. Regional health access and income c. Health outcomes and employment 4.3 Problem Set 1 - Data Management to Descriptive Statistics and Conceptual Endogeneity Deadline: Week 6 Note: HW means handwritten. Data Management and Cleaning Acquire dataset(s) for your assigned topic Clean data (handle missing values, recode variables, reshape, etc.) Report observations before and after cleaning (HW) Data Visualization Create at least 3 visualizations For each visualization: Explain why you chose it (HW) Discuss what it shows in economic terms (HW) Interpret patterns, trends, or anomalies (HW) Descriptive Statistics Compute summary statistics (mean, median, SD) Compare across groups, regions, or categories Discuss findings in economic terms (HW) Conceptual Endogeneity / Confounding Variables Identify at least one variable that might confound relationships (HW) Discuss how it could bias interpretation (HW) Economic Discussion Summarize your findings clearly (HW) Link patterns to economic reasoning or policy (HW) 4.4 Problem Set 2 - Testable Hypotheses, OLS, Diagnostics, Robustness Deadline: Week 12 Note: HW means handwritten. Formulate Testable Hypotheses Clearly define dependent and independent variables OLS Estimation Run bivariate OLS Run multivariate OLS with controls Report coefficients, standard errors, and R2 (Note: This can be printed and pasted) Discuss (HW): Economic interpretation of coefficients Can the OLS be interpreted causally? Why or why not? Potential sources of bias Sensitivity/ Robustness Checks Test whether results hold with different sets of control variables Perform subsample analysis (i.e., gender, region, time period) Discuss which results are robust, which change and why (HW) Please emphasize your discussion with economic interpretation (HW) Diagnostics or Formal Tests Multicollinearity (VIF) Heteroskedasticity (BP or White) Functional form (RESET) Discuss the implications of the results (HW) Economic Discussion (HW) Compare bivariate vs multivariate results and the sensitivity checks Summarize findings with Problem Set 1, policy, limitations 4.5 Reminders: Hard copy submission is the only submission accepted. All with (HW) means these are handwritten. Please write legibly. Typed R code printed and included. Plots are printed and pasted together with the handwritten discussion. Only include essential outputs meaning, no need to print the dataset contents, whatsoever. First pages would be the answers to all questions together then, next pages would be the step-by-step process with code chunks in R. In printing the Quarto Markdown file for submission, put in the code chunks the following line on top where you see the {r}. It should be like this: {r, eval=FALSE}. This is so that the results will not appear in the HTML. You can then print the Quarto Markdown clearly. Only do this when you are printing the R codes. "],["basic-introduction-to-r.html", "5 Basic Introduction to R 5.1 Session Information 5.2 Preliminaries 5.3 Packages 5.4 Setting up the Working Directory 5.5 Cleaning the Environment (Do This Regularly) 5.6 Quarto Markdown 5.7 Working Directories and File Management 5.8 Mini-Exercise: First Quarto Render (Mandatory per Individual)", " 5 Basic Introduction to R This portion of the book offers an introduction to the basics of R. R offers a wide variety of functionality. Note that this book only offers basic Econometric analysis. It will be useful to have some basic familiarity with R and its syntax but this is not strictly necessary. Each chapter includes both R code and results to make it easier for students to follow along, even without detailed knowledge of R. 5.1 Session Information This version of the book was built using R version 4.4.2. See below for the session information: ## R version 4.5.2 (2025-10-31 ucrt) ## Platform: x86_64-w64-mingw32/x64 ## Running under: Windows 11 x64 (build 26200) ## ## Matrix products: default ## LAPACK version 3.12.1 ## ## locale: ## [1] LC_COLLATE=English_Netherlands.utf8 ## [2] LC_CTYPE=English_Netherlands.utf8 ## [3] LC_MONETARY=English_Netherlands.utf8 ## [4] LC_NUMERIC=C ## [5] LC_TIME=English_Netherlands.utf8 ## ## time zone: Asia/Manila ## tzcode source: internal ## ## attached base packages: ## [1] stats graphics grDevices utils datasets ## [6] methods base ## ## other attached packages: ## [1] AER_1.2-15 survival_3.8-3 sandwich_3.1-1 ## [4] lmtest_0.9-40 zoo_1.8-15 car_3.1-3 ## [7] carData_3.0-5 wooldridge_1.4-4 lubridate_1.9.4 ## [10] forcats_1.0.1 stringr_1.6.0 dplyr_1.1.4 ## [13] purrr_1.2.0 readr_2.1.6 tidyr_1.3.2 ## [16] tibble_3.3.0 ggplot2_4.0.1 tidyverse_2.0.0 ## ## loaded via a namespace (and not attached): ## [1] sass_0.4.10 generics_0.1.4 stringi_1.8.7 ## [4] lattice_0.22-7 hms_1.1.4 digest_0.6.39 ## [7] magrittr_2.0.4 evaluate_1.0.5 grid_4.5.2 ## [10] timechange_0.3.0 RColorBrewer_1.1-3 bookdown_0.46 ## [13] fastmap_1.2.0 jsonlite_2.0.0 Matrix_1.7-4 ## [16] Formula_1.2-5 mgcv_1.9-3 scales_1.4.0 ## [19] jquerylib_0.1.4 abind_1.4-8 mnormt_2.1.1 ## [22] cli_3.6.5 rlang_1.1.6 splines_4.5.2 ## [25] cachem_1.1.0 withr_3.0.2 yaml_2.3.12 ## [28] tools_4.5.2 parallel_4.5.2 tzdb_0.5.0 ## [31] vctrs_0.6.5 R6_2.6.1 lifecycle_1.0.4 ## [34] psych_2.5.6 pkgconfig_2.0.3 bslib_0.9.0 ## [37] pillar_1.11.1 gtable_0.3.6 glue_1.8.0 ## [40] xfun_0.55 tidyselect_1.2.1 rstudioapi_0.17.1 ## [43] knitr_1.51 farver_2.1.2 htmltools_0.5.9 ## [46] nlme_3.1-168 rmarkdown_2.30 labeling_0.4.3 ## [49] compiler_4.5.2 S7_0.2.1 5.2 Preliminaries The first step is to gain access to R, which is free and available on the R website: http://cran.r-project.org/. Simply go to the R website, select the appropriate location and operating system, and follow the instructions to download the base distribution of R. RStudio offers a user friendly environment to run R and is recommended. Once R is opened, we can begin to run commands. R commands can be run directly from the console, from the R script editor or from a text editor separate from R. 5.2.1 Understanding the RStudio Screen Console (Bottom Left) This is where you see the output from your code Error messages Warnings You can type R commands directly in the console after the &gt; and press Enter to run them. Try it! Type 3*9 Use the console for: Quick calculations Testing commands Seeing results Important! Anything types only in the console is not saved. Script/Editor Area (Top Left) The Script area is where you write and save your code or Quarto documents. This is where R scripts and qmd files are written. Code here is saved, reproducible, and reusable. To run code from the script: Select a line and press Ctrl+Enter (Windows) or Cmd+Enter (Mac) or just place the cursor beside the line to run it. Run a whole chunk in Quarto Rule: Always write code in the script/editor, not in the console. Environment (Top Right) The Environment shows all objects currently in memory such as data frames, variables, functions, models. Try this: type jmn&lt;-1101 in the console. After running this, jmn will appear in the Environment. If something does NOT appear, it does not exist in your session. Files, Plots, Packages, Help, Viewer (Bottom Right) This panel has multiple tabs: Files Shows files in your working directory. Your working directory can be seen just below the Console, the one after ~ Use this to navigate project files Plots Displays graphs created by code Packages Shows list of installed packages. Checkboxes allow loading/unloading packages but you still need to add in the code library() in scripts for reproducibility. Help Displays help pages Viewer Shows rendered HTML outputs Used when previewing Quarto documents R offers detailed help files for each function. To access help, run: ?sum All lines proceeded by a # are comments and will not run*. For example: # This is a comment. R will not recognize this as a command. 5.3 Packages Each package of interest must be installed and loaded before it can be used. The packages will not be immediately available when R is opened. A package only has to be installed once on a computer, but the package will have to be loaded every time R is restarted. We can install a package individually as we need them. For example, to install tidyverse and psych, we would do: install.packages(&quot;tidyverse&quot;) install.packages(&quot;psych&quot;) In the tidyverse package, the ggplot2 is usually included; if you do not see the package in the Packages list at the lower right, you can do this: if(!(&quot;ggplot2&quot; %in% installed.packages()[,&quot;Package&quot;])) install.packages(&quot;ggplot2&quot;) Now that we have our packages successfully installed, we can go ahead and load them into R. Here we will load the tidyverse package as an example. We can use of all the functions available in that package once it is loaded into R. We load packages by using a library() function. The input is the name of the package, not in quotes. library(tidyverse) We can look up all of the functions within a package by using a help() function. For example, let’s look at the functions available in the tidyverse package. help(package = tidyverse) Note that the package argument is necessary to look up all of the functions. We can also detach a package if we no longer want it loaded. This is sometimes useful if two packages do not play well together. Here we will use a detach() function. detach(package:tidyverse) For simplicity, we will assume that the reader has restarted R at the beginning of each tutorial. 5.4 Setting up the Working Directory MOST IMPORTANT! If your working directory is incorrect, R will not find your files and your code will fail even if it is correct. 5.4.1 What is a Working Directory? The working directory is the folder where R looks for data files, saves outputs, where .qmd files should be placed. You will only use one main local folder as your working directory (technically two: one in your laptop, one in your university account computer) 5.4.2 Required Folder Structure Create a folder: DLSU_COURSE_SECTION This folder is your local working directory. On university computers: create this folder inside your university account storage. On your personal computer: create this folder anywhere convenient for you. All qmd files must be placed inside this folder! 5.4.3 Setting the Working Directory in RStudio This method works on both Windows and Mac and avoids typing errors. Open RStudio Go to Session&gt;Set Working Directory&gt;Choose Working Directory Select the folder DLSU_COURSE_Section Click Open Once set, R will treat this folder as the default location for all files; meaning, all files in the course should be placed in this folder. Download files then place in this folder. 5.4.4 Setting the Working Directory Using Code This is a hassle as you will manually code working directory using setwd() Windows: # Use double backslashes \\\\ or forward slashes / setwd(&quot;C:\\\\Users\\\\YourUsername\\\\Documents\\\\DLSU_COURSE_Section&quot;) setwd(&quot;C:/Users/YourUsername/Documents/DLSU_COURSE_Section&quot;) Mac: # Use forward slashes / setwd(&quot;/Users/YourUsername/Documents/DLSU_LBOMETR_Section&quot;) How to get the path? I forgot! Just kidding. I just right-click on the file and copy as path name. I am not sure how it is with Mac, I think same, right click but press option so the path name will show up. To confirm your working directory: getwd() If the printed path is NOT DLSU_COURSE_Section, fix it. 5.5 Cleaning the Environment (Do This Regularly) You should clean your environment at the start of a new lecture, when switching datasets, or when something behaves unexpectedly. # Remove all objects from the environment rm(list = ls()) # Free up memory (makes processor faster) gc() The Environment panel should be empty. You are always starting with a clean session. 5.6 Quarto Markdown In this course, Quarto Markdown (.qmd files). Quarto Markdown is a tool for creating documents, reports and presentations using Markdown and executable code. 5.6.1 Installing Quarto in R Before starting, install the quarto R package if not already installed: install.packages(&quot;quarto&quot;) 5.6.2 Starting a Quarto Document To begin creating a Quarto document, follow these steps: Open RStudio. Go to File &gt; New File &gt; Quarto Document. Choose the document type (e.g., HTML, PDF, Word, etc.) and specify whether the document will include code. For ease, we will use the html document type. Use the visual editor for ease of editing. 5.6.3 YAML Header Use Here is the YAML header for all quarto documents that you will submit along with line-by-line explanations. title: “TITLE of the FILE” This contains the title of the document. It appears at the top and in the browser tab. author: “put your name here” Replace with your name. Identifies the document author. date: today Automatically inserts the current date. format: html: toc: true theme: united embed-resources: true Format specifies the output type (html in this case), adds a table of contents, sets a bootstrap html theme (you can use other themes but this is preferred), embeds all resources in the html file. knitr: opts_chunk: warning: false message: false Controls code chunk behavior, meaning, it hides warnings and hides messages when rendering. editor: visual Uses the visual editor in Rstudio for the output Marks the end of the YAML header. As a whole, the YAML header looks like this: 5.6.4 Quarto Key Features Code Chunks Code chunks allow you to include and run code inside your document. To make code chunks, long way (there is shortcut later). Type ```{r} then press Enter. If you want to run the specific code chunk, you can also press the green play button at the right side of the chunk. If you want to run previous code chunks, press the button that has an arrow down with a green line below, just beside the green play button. jem&lt;-15+50+143 Inline Code Embed R code in text using backticks and r . Try typing this: The total number of pairs is “r 15+12”. Replace the quotation marks with backticks. When you render the file, what should come out in place of the backticks thing would be the number 27. 5.6.5 Quarto Markdown Shortcuts Action Windows Shortcut Mac Shortcut Insert a new code chunk Ctrl+Alt+I Cmd+Option+I Run current code chunk Ctrl+Shift+Enter Cmd+Shift+Enter Run all code chunks Ctrl+Alt+R Cmd+Alt+R Run current line/selection Ctrl+Enter Cmd+Enter Knit/Render document Ctrl+Shift+K Cmd+Shift+K Comment/uncomment lines Ctrl+Shift+C Cmd+Shift+C Insert pipe (%&gt;%) Ctrl+Shift+M Cmd+Shift+M Headings /H Number of Heading (if in Visual mode) Prefix line with #, ##, etc. manually (in Source mode) /H Number of Heading (if in Visual mode) Prefix line with #, ##, etc. manually (in Source mode) Bold Ctrl+B Cmd+B Italic Ctrl+I Cmd+I Inline code Surround with backticks (’) manually Surround with backticks (’) manually 5.6.6 Rendering a Quarto Document Rendering is the process of turning your .qmd file into an html. No rendering done, no submission. In this course, you will primarily render to HTML. 5.6.6.1 Render Using RStudio Button (DO THIS) Open your qmd file Make sure it is saved in the working directory Click Render button at the top of the editor. It looks like a blue arrow with the word Render. RStudio will run all code chunks, convert the document to HTML then open the result in the Viewer pane. If the render is successful, an html file will appear in your Files tab. You can double-click on the html file in your Files so that it appears in a bigger window which you can use to print as PDF. 5.7 Working Directories and File Management You will always work locally first. Each student must: Edit .qmd files only inside their local DLSU_COURSE_Section folder. Render HTML files locally to check for errors Never edit files directly inside Google Drive via a browser 5.7.1 File Naming Convention (Mandatory) To avoid confusion, all .qmd files must follow this format: initials_topic.qmd Example: jmn_descriptives.qmd jmn_visualization.qmd/ 5.8 Mini-Exercise: First Quarto Render (Mandatory per Individual) This exercise checks that you can write, run, and render a Quarto document correctly. Instructions: Create a new Quarto HTML document in RStudio. Keep the YAML header exactly as shown earlier in this chapter. In the body of the document, do the following steps: a. Add a Section Header Below the YAML header, add the following Markdown header: About Me as Heading 1 Under this header, write 2-3 sentences introducing yourself (name, program, year, hobbies) b. Add Inline Code for your age In the same section, add a sentence that includes inline R code. Sentence: I am “r 2026-type your year of birth” years old. Do NOT calculate your age manually. The age must be computed by R. The number should appear as plain text in the sentence. c. Add a Code Chunk Insert a new R code chunk. Inside the chunk: Add a comment saying this is just a test Create an object named after your initials Add the alphabetical positions of your initials Print the result Example: #This is just a test jmn&lt;-10+13+14 print(jmn) ## [1] 37 d. Render the document This exercise will help you in future lectures! e. Create a new Quarto document Clean the environment and free up the memory. Don’t forget your YAML header before the code chunk including the cleaning and freeing the memory! "],["data-management---cross-sectional-data.html", "6 Data Management - Cross-Sectional Data 6.1 Where to Get Data? 6.2 Preliminaries 6.3 Data Cleaning 6.4 Closing", " 6 Data Management - Cross-Sectional Data 6.1 Where to Get Data? Before we proceed to Data Management, let us first find where we can get data for the Data Story Archive. Note that the data you collect should still ensure that you are following the Code of Ethics and analyze Ethical Considerations. Please view the necessary documents from the Office of the Vice Chancellor for Research and Innovation (https://www.dlsu.edu.ph/research/research-manual/) A list of links you can search and get data from: Note: I will not include the best links as they are pretty straightforward and these are governmental databases like the ones from World Bank, IMF, UN, Philippine Statistics Authority, and Bangko Sentral ng Pilipinas. The list here is a general list but use with proper discretion. Name Link Notes Awesome Public Datasets https://github.com/awesomedata/awesome-public-datasets This repository is filled with public datasets, mostly from International contexts. Google Dataset Search https://datasetsearch.research.google.com/ You can download publicly available datasets from searching through Google. Though, sometimes the datasets come from ‘Statista.com’. You can check the sources from the search. 6.2 Preliminaries 6.2.1 Packages We will mostly use the tidyverse package, in particular, the dplyr package and the tidyr package; double-check in your Packages list whether you have these two packages; if not, you can simply install them. 6.2.2 Clean Everything Do this step every time you use other data or when we do the other chapters. # Remove all objects in the global environment rm(list = ls()) # Perform garbage collection to free up memory gc() ## used (Mb) gc trigger (Mb) max used (Mb) ## Ncells 3112258 166.3 5173005 276.3 5173005 276.3 ## Vcells 6108317 46.7 12255594 93.6 9035397 69.0 6.2.3 Importing the Datasets Before we can manage the data, we must first import it into R. There are two ways to do this: Writing code (preferred for replicability) Clicking in RStudio We start with the most common file types. 6.2.3.1 Importing a CSV file cp_csv&lt;-read.csv(&quot;CP_1.csv&quot;) The file must be in the working directory. If it is not, then, simply putting the file name inside the quotation marks will not work. Meaning, you have to input the entire path where the file is. Another, if you notice, the CSV file name is simple and easy to share. The object, in this case, cp_csv is also in small letters and does not have spaces but rather an underscore replacing the space. Always do this when naming objects. a. Small letters b. No spaces c. Place underscore instead. To check the information and what the dataset looks like: head(cp_csv) ## Country ID HH1_Num_People HH2a_Sex HH2b_Age ## 1 Bulgaria BG1535216 3 Female 56 ## 2 Netherlands NL5130211 1 Female 20 ## 3 Netherlands NL5063519 2 Male 63 ## 4 Slovenia SI1042916 3 Male 63 ## 5 Bulgaria BG1396625 2 Male 89 ## 6 Slovakia SK1184115 2 Female 44 ## HH2d_EmploymentSituation ## 1 Unemployed less than 12 months ## 2 In education (at school, university, etc.) / student ## 3 Retired ## 4 At work as employee or employer/self-employed ## 5 Retired ## 6 At work as employee or employer/self-employed ## Q1_PaidJob Q2_Empoyment ## 1 Yes &lt;NA&gt; ## 2 Yes &lt;NA&gt; ## 3 Yes &lt;NA&gt; ## 4 &lt;NA&gt; Employed ## 5 Yes &lt;NA&gt; ## 6 &lt;NA&gt; Self-employed without employees ## Q3_Contract ## 1 &lt;NA&gt; ## 2 &lt;NA&gt; ## 3 &lt;NA&gt; ## 4 On an unlimited permanent contract ## 5 &lt;NA&gt; ## 6 On an unlimited permanent contract ## Q4_Occupation Q7_HoursWeekWork ## 1 &lt;NA&gt; NA ## 2 &lt;NA&gt; NA ## 3 &lt;NA&gt; NA ## 4 Technician or junior professional 35 ## 5 &lt;NA&gt; NA ## 6 Technician or junior professional 43 ## Q7a_AdditionalJob Q7b_HoursWeekWeekAddJob Q7c_Work ## 1 &lt;NA&gt; NA No ## 2 &lt;NA&gt; NA No ## 3 &lt;NA&gt; NA No ## 4 No NA &lt;NA&gt; ## 5 &lt;NA&gt; NA No ## 6 No NA &lt;NA&gt; ## Q8_HoursWeekWorkPref Q9_HoursWeekWorkPartner ## 1 40 40 ## 2 10 NA ## 3 0 NA ## 4 30 NA ## 5 0 NA ## 6 43 NA ## Q10_HoursWeekWorkPartnerPref Q17_Rooms ## 1 4 3 ## 2 NA 1 ## 3 0 6 ## 4 30 3 ## 5 0 1 ## 6 NA 2 ## Q18_Tenancy Q19a_ShortageSpace ## 1 Own without mortgage 0 ## 2 Other 0 ## 3 Own with mortgage 0 ## 4 Tenant, paying rent to private landlord 0 ## 5 Own without mortgage 0 ## 6 Own without mortgage 1 ## Q19b_Rot Q19c_Leaks Q19d_NoFlusingToilet Q19e_NoBathShower ## 1 0 0 0 0 ## 2 0 0 0 0 ## 3 0 0 0 0 ## 4 0 0 0 0 ## 5 0 0 0 0 ## 6 0 0 0 0 ## Q19f_NoOutside Q20_LeaveAccomodation_NoAff ## 1 0 Very unlikely ## 2 0 Quite unlikely ## 3 0 Very unlikely ## 4 0 Quite unlikely ## 5 0 Very unlikely ## 6 0 Very likely ## Q24_Trust Q25a_TensionClass ## 1 6 Some tension ## 2 6 Some tension ## 3 8 Some tension ## 4 6 Some tension ## 5 1 - you can&#39;t be too careful Some tension ## 6 5 Some tension ## Q25b_TensionWork Q25c_TensionSex Q25d_TensionsAge ## 1 No tension No tension 3 ## 2 Some tension No tension 3 ## 3 Some tension No tension 3 ## 4 Some tension No tension 3 ## 5 Don”t know Some tension 2 ## 6 Some tension Some tension 2 ## Q25e_TensionRace Q25f_TensionReligion ## 1 Some tension No tension ## 2 Some tension Some tension ## 3 A lot of tension A lot of tension ## 4 Some tension Some tension ## 5 Don”t know Don”t know ## 6 Some tension Some tension ## Q25g_TensionSexOrient Q37a_HoursWeekChildren ## 1 No tension 2 ## 2 A lot of tension NA ## 3 Some tension NA ## 4 No tension 5 ## 5 Don”t know NA ## 6 Some tension NA ## Q37b_HoursWeekHousework Q37c_HoursWeekElderly ## 1 14 NA ## 2 7 NA ## 3 14 3 ## 4 NA NA ## 5 NA NA ## 6 20 NA ## Q48_Education Q49_Area ## 1 1250 A medium to large town ## 2 2956 A city or city suburb ## 3 2963 A medium to large town ## 4 3420 A medium to large town ## 5 1253 A city or city suburb ## 6 3530 A medium to large town ## Q50a_NeighbourhoodNoise Q50b_NeighbourhoodAir ## 1 No problems No problems ## 2 No problems No problems ## 3 No problems No problems ## 4 No problems No problems ## 5 No problems No problems ## 6 No problems No problems ## Q50c_NeighbourhoodWater Q50d_NeighbourhoodCrime ## 1 Major problems No problems ## 2 No problems No problems ## 3 No problems No problems ## 4 No problems No problems ## 5 No problems Moderate problems ## 6 No problems No problems ## Q50e_NeighbourhoodLitter Q50f_NeighbourhoodTraffic ## 1 No problems No problems ## 2 No problems No problems ## 3 No problems No problems ## 4 No problems Moderate problems ## 5 Moderate problems Don&#39;t know ## 6 No problems Moderate problems ## Q51a_AccServicesPost Q51b_AccServicesBank ## 1 Service not used Easily ## 2 Easily With some difficulty ## 3 Easily Easily ## 4 Easily Easily ## 5 Service not used Service not used ## 6 Very easily Very easily ## Q53c_QualityPublicTransport Income_PPP ## 1 6 NA ## 2 7 273.4731 ## 3 7 3190.5196 ## 4 6 4171.6329 ## 5 3 478.3407 ## 6 5 759.6961 str(cp_csv) ## &#39;data.frame&#39;: 4036 obs. of 49 variables: ## $ Country : chr &quot;Bulgaria&quot; &quot;Netherlands&quot; &quot;Netherlands&quot; &quot;Slovenia&quot; ... ## $ ID : chr &quot;BG1535216&quot; &quot;NL5130211&quot; &quot;NL5063519&quot; &quot;SI1042916&quot; ... ## $ HH1_Num_People : int 3 1 2 3 2 2 2 1 4 2 ... ## $ HH2a_Sex : chr &quot;Female&quot; &quot;Female&quot; &quot;Male&quot; &quot;Male&quot; ... ## $ HH2b_Age : int 56 20 63 63 89 44 52 43 47 81 ... ## $ HH2d_EmploymentSituation : chr &quot;Unemployed less than 12 months&quot; &quot;In education (at school, university, etc.) / student&quot; &quot;Retired&quot; &quot;At work as employee or employer/self-employed&quot; ... ## $ Q1_PaidJob : chr &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; NA ... ## $ Q2_Empoyment : chr NA NA NA &quot;Employed&quot; ... ## $ Q3_Contract : chr NA NA NA &quot;On an unlimited permanent contract&quot; ... ## $ Q4_Occupation : chr NA NA NA &quot;Technician or junior professional&quot; ... ## $ Q7_HoursWeekWork : int NA NA NA 35 NA 43 NA NA 17 NA ... ## $ Q7a_AdditionalJob : chr NA NA NA &quot;No&quot; ... ## $ Q7b_HoursWeekWeekAddJob : int NA NA NA NA NA NA NA NA NA NA ... ## $ Q7c_Work : chr &quot;No&quot; &quot;No&quot; &quot;No&quot; NA ... ## $ Q8_HoursWeekWorkPref : int 40 10 0 30 0 43 40 40 35 0 ... ## $ Q9_HoursWeekWorkPartner : int 40 NA NA NA NA NA 41 NA 35 NA ... ## $ Q10_HoursWeekWorkPartnerPref: int 4 NA 0 30 0 NA 30 NA 35 0 ... ## $ Q17_Rooms : int 3 1 6 3 1 2 NA 1 5 3 ... ## $ Q18_Tenancy : chr &quot;Own without mortgage&quot; &quot;Other&quot; &quot;Own with mortgage&quot; &quot;Tenant, paying rent to private landlord&quot; ... ## $ Q19a_ShortageSpace : int 0 0 0 0 0 1 0 0 0 0 ... ## $ Q19b_Rot : int 0 0 0 0 0 0 0 1 1 0 ... ## $ Q19c_Leaks : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Q19d_NoFlusingToilet : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Q19e_NoBathShower : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Q19f_NoOutside : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Q20_LeaveAccomodation_NoAff : chr &quot;Very unlikely&quot; &quot;Quite unlikely&quot; &quot;Very unlikely&quot; &quot;Quite unlikely&quot; ... ## $ Q24_Trust : chr &quot;6&quot; &quot;6&quot; &quot;8&quot; &quot;6&quot; ... ## $ Q25a_TensionClass : chr &quot;Some tension&quot; &quot;Some tension&quot; &quot;Some tension&quot; &quot;Some tension&quot; ... ## $ Q25b_TensionWork : chr &quot;No tension&quot; &quot;Some tension&quot; &quot;Some tension&quot; &quot;Some tension&quot; ... ## $ Q25c_TensionSex : chr &quot;No tension&quot; &quot;No tension&quot; &quot;No tension&quot; &quot;No tension&quot; ... ## $ Q25d_TensionsAge : int 3 3 3 3 2 2 98 3 3 1 ... ## $ Q25e_TensionRace : chr &quot;Some tension&quot; &quot;Some tension&quot; &quot;A lot of tension&quot; &quot;Some tension&quot; ... ## $ Q25f_TensionReligion : chr &quot;No tension&quot; &quot;Some tension&quot; &quot;A lot of tension&quot; &quot;Some tension&quot; ... ## $ Q25g_TensionSexOrient : chr &quot;No tension&quot; &quot;A lot of tension&quot; &quot;Some tension&quot; &quot;No tension&quot; ... ## $ Q37a_HoursWeekChildren : int 2 NA NA 5 NA NA 4 NA NA NA ... ## $ Q37b_HoursWeekHousework : int 14 7 14 NA NA 20 4 8 4 6 ... ## $ Q37c_HoursWeekElderly : int NA NA 3 NA NA NA NA NA NA NA ... ## $ Q48_Education : chr &quot;1250&quot; &quot;2956&quot; &quot;2963&quot; &quot;3420&quot; ... ## $ Q49_Area : chr &quot;A medium to large town&quot; &quot;A city or city suburb&quot; &quot;A medium to large town&quot; &quot;A medium to large town&quot; ... ## $ Q50a_NeighbourhoodNoise : chr &quot;No problems&quot; &quot;No problems&quot; &quot;No problems&quot; &quot;No problems&quot; ... ## $ Q50b_NeighbourhoodAir : chr &quot;No problems&quot; &quot;No problems&quot; &quot;No problems&quot; &quot;No problems&quot; ... ## $ Q50c_NeighbourhoodWater : chr &quot;Major problems&quot; &quot;No problems&quot; &quot;No problems&quot; &quot;No problems&quot; ... ## $ Q50d_NeighbourhoodCrime : chr &quot;No problems&quot; &quot;No problems&quot; &quot;No problems&quot; &quot;No problems&quot; ... ## $ Q50e_NeighbourhoodLitter : chr &quot;No problems&quot; &quot;No problems&quot; &quot;No problems&quot; &quot;No problems&quot; ... ## $ Q50f_NeighbourhoodTraffic : chr &quot;No problems&quot; &quot;No problems&quot; &quot;No problems&quot; &quot;Moderate problems&quot; ... ## $ Q51a_AccServicesPost : chr &quot;Service not used&quot; &quot;Easily&quot; &quot;Easily&quot; &quot;Easily&quot; ... ## $ Q51b_AccServicesBank : chr &quot;Easily&quot; &quot;With some difficulty&quot; &quot;Easily&quot; &quot;Easily&quot; ... ## $ Q53c_QualityPublicTransport : chr &quot;6&quot; &quot;7&quot; &quot;7&quot; &quot;6&quot; ... ## $ Income_PPP : num NA 273 3191 4172 478 ... The head code shows the first 6 rows and all the columns (variables) in the dataset. Meanwhile the str code shows the information of the dataset, such as what format type each column/variable is, how many observations, etc. B. Using RStudio Go to Environment Click Import Dataset Choose From Text (base) or From Test (readr) Click Browse and select your .csv file Click Import RStudio will load the dataset and generate R code in Console. Copy the R code to your script for reproducibility. 6.2.3.2 Importing an Excel File First, install and load the package: install.packages(&quot;readxl&quot;) library(readxl) cp_xlsx&lt;-read_excel(&quot;CP_1.xlsx&quot;) If you only want a particular sheet to be imported, cp_xlsx&lt;-read_excel(&quot;CP_1.xlsx&quot;, sheet= 1) head(cp_xlsx) ## # A tibble: 6 × 49 ## Country ID HH1_Num_People HH2a_Sex HH2b_Age ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Bulgaria BG1535216 3 Female 56 ## 2 Netherlands NL5130211 1 Female 20 ## 3 Netherlands NL5063519 2 Male 63 ## 4 Slovenia SI1042916 3 Male 63 ## 5 Bulgaria BG1396625 2 Male 89 ## 6 Slovakia SK1184115 2 Female 44 ## # ℹ 44 more variables: HH2d_EmploymentSituation &lt;chr&gt;, ## # Q1_PaidJob &lt;chr&gt;, Q2_Empoyment &lt;chr&gt;, Q3_Contract &lt;chr&gt;, ## # Q4_Occupation &lt;chr&gt;, Q7_HoursWeekWork &lt;dbl&gt;, ## # Q7a_AdditionalJob &lt;chr&gt;, Q7b_HoursWeekWeekAddJob &lt;dbl&gt;, ## # Q7c_Work &lt;chr&gt;, Q8_HoursWeekWorkPref &lt;dbl&gt;, ## # Q9_HoursWeekWorkPartner &lt;dbl&gt;, ## # Q10_HoursWeekWorkPartnerPref &lt;dbl&gt;, Q17_Rooms &lt;dbl&gt;, … B. Using RStudio Go to Environment Click Import Dataset Choose From Excel Click Browse and select your .xlsx file Choose the sheet Click Import Check that you also have the variables as the header. 6.2.3.3 Importing RData These files are native to R and can contain multiple objects. load(&quot;Dataset_CP.RData&quot;) head(Dataset_CP) ## # A tibble: 6 × 49 ## Country ID HH1_Num_People HH2a_Sex HH2b_Age ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 Bulgaria BG1535216 3 Female 56 ## 2 Netherlands NL5130211 1 Female 20 ## 3 Netherlands NL5063519 2 Male 63 ## 4 Slovenia SI1042916 3 Male 63 ## 5 Bulgaria BG1396625 2 Male 89 ## 6 Slovakia SK1184115 2 Female 44 ## # ℹ 44 more variables: HH2d_EmploymentSituation &lt;fct&gt;, ## # Q1_PaidJob &lt;fct&gt;, Q2_Empoyment &lt;fct&gt;, Q3_Contract &lt;fct&gt;, ## # Q4_Occupation &lt;fct&gt;, Q7_HoursWeekWork &lt;dbl&gt;, ## # Q7a_AdditionalJob &lt;fct&gt;, Q7b_HoursWeekWeekAddJob &lt;dbl&gt;, ## # Q7c_Work &lt;fct&gt;, Q8_HoursWeekWorkPref &lt;dbl&gt;, ## # Q9_HoursWeekWorkPartner &lt;dbl&gt;, ## # Q10_HoursWeekWorkPartnerPref &lt;dbl&gt;, Q17_Rooms &lt;dbl&gt;, … B. Using RStudio Environment panel Click Import Dataset Choose From RData Select the .RData file Click Import 6.3 Data Cleaning As you can see, there are 49 columns. Let’s simplify and work with fewer variables relevant for analysis. We can do this using the select() function in dplyr. We will save them into a new data frame, ch2_p1.1. library(dplyr) ch2_p1.1&lt;-select(cp_csv, Country, ID, HH2a_Sex, HH2b_Age, Q1_PaidJob, Q7_HoursWeekWork, Q17_Rooms, Q49_Area, Income_PPP) select(df, var1, var2,...) keeps only the listed columns and removes the rest. 6.3.1 Renaming the Variables We will edit the names to much easier conventions. First, let us say that we just want to change them to lowercase names. names(ch2_p1.1)&lt;-tolower(names(ch2_p1.1)) Inspect: head(ch2_p1.1) ## country id hh2a_sex hh2b_age q1_paidjob ## 1 Bulgaria BG1535216 Female 56 Yes ## 2 Netherlands NL5130211 Female 20 Yes ## 3 Netherlands NL5063519 Male 63 Yes ## 4 Slovenia SI1042916 Male 63 &lt;NA&gt; ## 5 Bulgaria BG1396625 Male 89 Yes ## 6 Slovakia SK1184115 Female 44 &lt;NA&gt; ## q7_hoursweekwork q17_rooms q49_area ## 1 NA 3 A medium to large town ## 2 NA 1 A city or city suburb ## 3 NA 6 A medium to large town ## 4 35 3 A medium to large town ## 5 NA 1 A city or city suburb ## 6 43 2 A medium to large town ## income_ppp ## 1 NA ## 2 273.4731 ## 3 3190.5196 ## 4 4171.6329 ## 5 478.3407 ## 6 759.6961 Inspect again on your own. Let us rename specific columns: ch2_p1.1&lt;-ch2_p1.1 %&gt;% rename( sex = hh2a_sex, age = hh2b_age, paid_job = q1_paidjob, hours_work = q7_hoursweekwork, rooms = q17_rooms, area = q49_area, income = income_ppp ) Inspect on your own. names() simply gets or sets variable names. tolower is to simply change to small letters rename(new_name = old_name) changes a column’s name without touching data %&gt;% is the pipe operator: it passes the dataset from one function to the next. 6.3.2 Sorting Variables Let’s say, we want to arrange income. We will create a different data for this. We use arrange or desc in dplyr package #Sort dataset by income, ascending (default) ch2_p1sort&lt;-arrange(ch2_p1.1, income) #Sort dataset by income, descending ch2_p1sort_desc&lt;-arrange(ch2_p1.1, desc(income)) Inspect on your own. 6.3.2.1 Making our own data frame ch2_p2&lt;-data.frame( ch2_p2 = as.factor(c(&quot;$10,000&quot;, &quot;$20,500&quot;, &quot;$15,250&quot;, &quot;$30,000&quot;, &quot;$50,750&quot;)) ) c() stands for combine. We’re creating a vector of values. as.factor() converts the vector into a factor, that is a categorical variable not numbers. data.frame(...) creates a dataset in R. head(ch2_p2) ## ch2_p2 ## 1 $10,000 ## 2 $20,500 ## 3 $15,250 ## 4 $30,000 ## 5 $50,750 Let’s sort this: ch2_p2sort&lt;-arrange(ch2_p2) head(ch2_p2) ## ch2_p2 ## 1 $10,000 ## 2 $20,500 ## 3 $15,250 ## 4 $30,000 ## 5 $50,750 It did not work. The problem is, ch2_p2 is not numeric. We can check: class(ch2_p2$ch2_p2) ## [1] &quot;factor&quot; We need to make it into a numeric value but we have a , and $. We need to remove them. We use the str_replace function in the stringr package. library(stringr) ch2_p2$ch2_p2&lt;-str_replace( ch2_p2$ch2_p2, #column we want to edit pattern = &#39;,&#39;, #what to find replacement = &#39;&#39; #what to replace it with ) head(ch2_p2) ## ch2_p2 ## 1 $10000 ## 2 $20500 ## 3 $15250 ## 4 $30000 ## 5 $50750 Now, let us remove the dollar sign; usually, simply doing the same thing we did with the comma works, but, there are some symbols that are used as “special character”. To “force” R to replace the presence of ‘$’, we add two backslashes before the dollar sign. ch2_p2$ch2_p2&lt;-str_replace( ch2_p2$ch2_p2, pattern = &#39;\\\\$&#39;, replacement = &#39;&#39; ) Inspect on your own. Now, sort ch2_p2 on your own. We can see that it was arranged, however, take a look at the way ch2_p2 was encoded; it is not numeric. So, we need to change this. class(ch2_p2$ch2_p2) ## [1] &quot;character&quot; Change to numeric through as.numeric() ch2_p2$ch2_p2&lt;-as.numeric(ch2_p2$ch2_p2) Inspect on your own. 6.3.3 Pipe Operator %&gt;% allows functions to be chained; it can be read as “then” - it tells R to do whatever comes after it to the stuff that comes before it. ch2_p1.1.2 &lt;- ch2_p1.1 %&gt;% filter(age &gt; 60) %&gt;% arrange(desc(income)) Inspect on your own. 6.3.4 Adding columns We will be using the pipe operator and the mutate to add a new column to ch2_p1.1 ch2_p1.1 &lt;- ch2_p1.1 %&gt;% mutate( #adds a new column to the dataset room_group = case_when( #checks each row&#39;s value and assigned a category rooms == 1 ~ &quot;one_room&quot;, rooms == 2 ~ &quot;two_rooms&quot;, rooms == 3 ~ &quot;three_rooms&quot;, rooms == 4 ~ &quot;four_rooms&quot;, rooms == 5 ~ &quot;five_rooms&quot;, rooms == 6 ~ &quot;six_rooms&quot;, rooms == 7 ~ &quot;seven_rooms&quot;, rooms == 8 ~ &quot;eight_rooms&quot;, rooms == 9 ~ &quot;nine_rooms&quot;, rooms == 10 ~ &quot;ten_rooms&quot;, rooms == 11 ~ &quot;eleven_rooms&quot;, rooms == 12 ~ &quot;twelve_rooms&quot;, TRUE ~ &quot;other&quot; # anything outside 1–12 or missing ) ) 6.3.5 Transforming Values Now, you can see that paid_job is a character that is “yes/no”. We need to change that to numeric value. This is particularly useful when we use dummy variables later on. We will not use case_when as it is not necessary; rather, we will use ifelse: ch2_p1.1&lt;-ch2_p1.1 %&gt;% mutate( paid_job = ifelse(paid_job == &quot;Yes&quot;, 1, 0) ) 6.3.6 Categorizing into groups We use the case_when(): ch2_p1.1 &lt;- ch2_p1.1 %&gt;% mutate( age_group = case_when( age &gt;= 18 &amp; age &lt;= 29 ~ &quot;young&quot;, # 18–29 years age &gt;= 30 &amp; age &lt;= 59 ~ &quot;adult&quot;, # 30–59 years age &gt;= 60 ~ &quot;older_adult&quot;, # 60+ years TRUE ~ &quot;other&quot; # NA ) ) mutate() adding new column The conditions are important. 6.3.7 Summarizing Let us get the average of income by age group, which we’ll call ave_income, by using the group_by() and summarise() functions in dplyr ch2_p1.1ave&lt;-ch2_p1.1 %&gt;% group_by(age_group) %&gt;% #group by age group, THEN summarise(ave_income=mean(income)) #calculate the mean of income for each age group head(ch2_p1.1ave) ## # A tibble: 3 × 2 ## age_group ave_income ## &lt;chr&gt; &lt;dbl&gt; ## 1 adult NA ## 2 older_adult NA ## 3 young NA How did this happen? It is because there are NAs in income. Let’s check for NAs in our dataset colSums(is.na(ch2_p1.1)) ## country id sex age paid_job ## 0 0 0 0 1768 ## hours_work rooms area income room_group ## 2304 25 0 978 0 ## age_group ## 0 There are 978 NAs in income, that is why the average income is NA. For the sake of, let’s remove the NA. ch2_p1.1ave &lt;- ch2_p1.1 %&gt;% group_by(age_group) %&gt;% summarise(ave_income = mean(income, na.rm = TRUE)) head(ch2_p1.1ave) ## # A tibble: 3 × 2 ## age_group ave_income ## &lt;chr&gt; &lt;dbl&gt; ## 1 adult 2145. ## 2 older_adult 1566. ## 3 young 1809. The difference is the inclusion of na.rm = TRUE which means we remove the missing values when calculating the mean. So, even though we know income is numeric, the presence of NA causes the output when calculating mean, sd, etc. become NA too. 6.3.8 Merging datasets We have two main datasets, ch2_p1.1 and ch2_p1.1ave. By doing this, we could compare side-by-side each observation compared to the average per age group. We will join the datasets by age_group variable, since that is consistent across both datasets. We name the new file as ch2_p1merged: ch2_p1merged&lt;-merge(x=ch2_p1.1, y=ch2_p1.1ave, by=&quot;age_group&quot;) head(ch2_p1merged) ## age_group country id sex age paid_job ## 1 adult Bulgaria BG1535216 Female 56 1 ## 2 adult Hungary HU1042815 Male 39 0 ## 3 adult Poland PL1229721 Female 47 NA ## 4 adult Germany DE1045717 Female 56 1 ## 5 adult Finland FI1024310 Male 58 NA ## 6 adult Slovakia SK1184115 Female 44 NA ## hours_work rooms area income ## 1 NA 3 A medium to large town NA ## 2 NA 2 A village/small town 987.0490 ## 3 40 4 A village/small town 1722.0737 ## 4 NA 3 A city or city suburb NA ## 5 32 5 A medium to large town 3958.9271 ## 6 43 2 A medium to large town 759.6961 ## room_group ave_income ## 1 three_rooms 2145.059 ## 2 two_rooms 2145.059 ## 3 four_rooms 2145.059 ## 4 three_rooms 2145.059 ## 5 five_rooms 2145.059 ## 6 two_rooms 2145.059 6.3.9 Splitting datasets Say I want to save different datasets based on the age_group column. adult_data&lt;-ch2_p1.1 %&gt;% filter(age_group==&quot;adult&quot;) filter() keeps rows that meet the condition. You can save it as a .csv file: write.csv(adult_data, &quot;adult_data.csv&quot;, row.names = FALSE) Can you do the others? (young or older_adult) 6.3.10 Reshaping Datasets Let’s say you want to reshape income and hours_work into a long format so every row is a variable measurement. We need both tidyr and dplyr packages. 6.3.10.1 Long Format library(tidyr) library(dplyr) ch2_long&lt;-ch2_p1.1 %&gt;% pivot_longer( cols = c(hours_work, income), # columns to stack names_to = &quot;variable&quot;, #new column for variable names values_to = &quot;value&quot; #new column for their values ) head(ch2_long) ## # A tibble: 6 × 11 ## country id sex age paid_job rooms area room_group ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Bulgaria BG15… Fema… 56 1 3 A me… three_roo… ## 2 Bulgaria BG15… Fema… 56 1 3 A me… three_roo… ## 3 Netherla… NL51… Fema… 20 1 1 A ci… one_room ## 4 Netherla… NL51… Fema… 20 1 1 A ci… one_room ## 5 Netherla… NL50… Male 63 1 6 A me… six_rooms ## 6 Netherla… NL50… Male 63 1 6 A me… six_rooms ## # ℹ 3 more variables: age_group &lt;chr&gt;, variable &lt;chr&gt;, ## # value &lt;dbl&gt; pivot_longer() turns columns into rows 6.3.10.2 Wide Format ch2_wide&lt;-ch2_long %&gt;% pivot_wider( names_from = variable, #column to spread to multiple columns values_from = value #value to fill columns ) head(ch2_wide) ## # A tibble: 6 × 11 ## country id sex age paid_job rooms area room_group ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Bulgaria BG15… Fema… 56 1 3 A me… three_roo… ## 2 Netherla… NL51… Fema… 20 1 1 A ci… one_room ## 3 Netherla… NL50… Male 63 1 6 A me… six_rooms ## 4 Slovenia SI10… Male 63 NA 3 A me… three_roo… ## 5 Bulgaria BG13… Male 89 1 1 A ci… one_room ## 6 Slovakia SK11… Fema… 44 NA 2 A me… two_rooms ## # ℹ 3 more variables: age_group &lt;chr&gt;, hours_work &lt;dbl&gt;, ## # income &lt;dbl&gt; Say you want to analyze repeated categories or measures. We can pivot the room_group and age_group to long: ch2_long2&lt;-ch2_p1.1 %&gt;% pivot_longer( cols = c(room_group, age_group), names_to = &quot;category&quot;, values_to = &quot;group&quot; ) head(ch2_long2) ## # A tibble: 6 × 11 ## country id sex age paid_job hours_work rooms area ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 Bulgaria BG15… Fema… 56 1 NA 3 A me… ## 2 Bulgaria BG15… Fema… 56 1 NA 3 A me… ## 3 Netherla… NL51… Fema… 20 1 NA 1 A ci… ## 4 Netherla… NL51… Fema… 20 1 NA 1 A ci… ## 5 Netherla… NL50… Male 63 1 NA 6 A me… ## 6 Netherla… NL50… Male 63 1 NA 6 A me… ## # ℹ 3 more variables: income &lt;dbl&gt;, category &lt;chr&gt;, ## # group &lt;chr&gt; 6.3.11 Missing Values We already did it before but just to remind: #Count missing values per column colSums(is.na(ch2_p1.1)) ## country id sex age paid_job ## 0 0 0 0 1768 ## hours_work rooms area income room_group ## 2304 25 0 978 0 ## age_group ## 0 When there are missing values, it reduces sample size, can bias estimates if the missingness is not random and may introduce endogeneity if the missing data is correlated with the outcome or other variables. 6.3.11.1 Types of Missing Data MCAR - Missing Completely at Random Missingness is unrelated to anything in the data MAR - Missing at Random Missingness depends on observed variables but not on the missing value itself. Example: younger respondents are less likely to report income. MNAR - Missing Not at Random Missingness depends on the value itself Example: high income people refuse to report income If you have MCAR, safe to remove this. If MAR or MNAR, removing these can cause bias. If a variable with missing values affects the outcome and you remove it or impute incorrectly, you can create endogeneity. 6.3.11.2 Handling Missing Values 6.3.11.2.1 a. Remove missing observations #Keep only rows without missing income ch2_p1.2&lt;-ch2_p1.1 %&gt;% filter(!is.na(income)) !is.na() means you are keeping rows WITHOUT missing income Again, double-check because removing the missing values may bias your results 6.3.11.2.2 b. Replace missing values (imputation of mean/median) #replace missing income with the mean ch2_p1.3&lt;-ch2_p1.1 %&gt;% mutate(income=ifelse(is.na(income), mean(income, na.rm=TRUE), income)) ifelse(condition, value_if_true, value_if_false) in this case, you are telling R to search income’s NA values, if there are NA values, calculate the mean income without the NAs. then, change the NAs with the calculated mean income, otherwise, keep the income as is. There are advanced methods but will not be discussed. You have to be careful and always check for missing data. 6.4 Closing Quiz questions will be uploaded in Animospace. You have 15 minutes to answer. Clean the environment and free the memory. "],["data-management---time-series-and-panel-data.html", "7 Data Management - Time Series and Panel Data 7.1 Time Series Data 7.2 Panel Data 7.3 Closing", " 7 Data Management - Time Series and Panel Data 7.1 Time Series Data Time series analysis requires correct date formats, proper chronological ordering, etc. For this, we will use Chapter3 Practice found in the Modules. 7.1.1 Preliminaries Always remember the first steps: Set Working Directory and Clean the Global Environment. rm(list=ls()) gc() ## used (Mb) gc trigger (Mb) max used (Mb) ## Ncells 3165374 169.1 5173005 276.3 5173005 276.3 ## Vcells 6228023 47.6 12255594 93.6 10624508 81.1 To make sure that you are using the correct directory and that you have all the files you need, use list.files() function. list.files() Now, we load the following packages: dplyr, lubridate, and zoo. Make sure you have all 3 installed; if not, install them. library(dplyr) library(lubridate) library(zoo) 7.1.2 Create a Simple Date Dataset date_date&lt;-data.frame( ID = 1:5, dob = c(&quot;15-05-1990&quot;, &quot;20-08-1985&quot;, &quot;01-12-2000&quot;, &quot;10-03-1995&quot;, &quot;25-07-2010&quot;), stringsAsFactors = FALSE ) print(date_date) ## ID dob ## 1 1 15-05-1990 ## 2 2 20-08-1985 ## 3 3 01-12-2000 ## 4 4 10-03-1995 ## 5 5 25-07-2010 str(date_date) ## &#39;data.frame&#39;: 5 obs. of 2 variables: ## $ ID : int 1 2 3 4 5 ## $ dob: chr &quot;15-05-1990&quot; &quot;20-08-1985&quot; &quot;01-12-2000&quot; &quot;10-03-1995&quot; ... I won’t repeat what the code has except for new ones like ID which creates a numeric sequence for the ID column. date_of_birth = c(\"15-05-1990\",...) are dates stored as character strings. stringsAsFactors = FALSE because R sometimes turns text into factors (categories) so by making it FALSE, it ensures dates remain character strings not categories As seen, the dob is in character format: 7.1.3 Converting Character Dates to Date Format date_date$dob&lt;- as.Date( date_date$dob, format = &quot;%d-%m-%Y&quot; ) class(date_date$dob) ## [1] &quot;Date&quot; So, we converted character strings into Date objects. With this, we can calculate age, lags, etc. 7.1.4 Calculate Age: Say you want to calculate the age: Before, we added columns through mutate; this time, we add columns through creating a new object. date_date$age &lt;- as.numeric(floor((Sys.Date()-date_date$dob)/365.25)) as.Date() converts strings to dates while floor() simply rounds down. 7.1.4.1 Custom Reference Date: ref_date&lt;-as.Date(&quot;2020-12-20&quot;) #uses YYYY-MM-DD as ISO standard that R recognizes immediately date_date$age2&lt;- as.numeric( floor((ref_date-date_date$dob)/365.25) ) 7.1.5 Using Time Series Data We load Daily Bitcoin Data ch3_p1&lt;-read.csv(&quot;Ch3Practice.csv&quot;) head(ch3_p1) ## ds y ## 1 2015-06-13 232.402 ## 2 2015-06-14 233.543 ## 3 2015-06-15 236.823 ## 4 2015-06-16 250.895 ## 5 2015-06-17 249.284 ## 6 2015-06-18 249.007 We need to understand our data; str(ch3_p1) ## &#39;data.frame&#39;: 1825 obs. of 2 variables: ## $ ds: chr &quot;2015-06-13&quot; &quot;2015-06-14&quot; &quot;2015-06-15&quot; &quot;2015-06-16&quot; ... ## $ y : num 232 234 237 251 249 ... 7.1.6 Convert to Date Format As you can see, our ds is what our date column is, however, it is in the character format. We need to convert it to the Date class. We also need to do this to a copy of the raw data for further modifications. ch3_p1.1&lt;-ch3_p1 head(ch3_p1.1$ds) ## [1] &quot;2015-06-13&quot; &quot;2015-06-14&quot; &quot;2015-06-15&quot; &quot;2015-06-16&quot; ## [5] &quot;2015-06-17&quot; &quot;2015-06-18&quot; class(ch3_p1.1$ds) ## [1] &quot;character&quot; ch3_p1.1$ds &lt;- as.Date(ch3_p1.1$ds, format = &quot;%Y-%m-%d&quot;) class(ch3_p1.1$ds) ## [1] &quot;Date&quot; 7.1.7 Aggregate Data We now have daily data. Say we want to create weekly data, we use the cut function to group dates by week, month, quarter and year. Since we know for sure that the date column is in date format, no need to check, however, it is always useful to check the class of date. 7.1.7.1 Aggregate by Week Add new columns for each aggregation. ch3_p1.1$week&lt;-cut(ch3_p1.1$ds, breaks = &quot;week&quot;) The cut is used to divide the date into intervals while the breaks specifies that the date be divided into weekly intervals. Check creation of the week column head(ch3_p1.1) ## ds y week ## 1 2015-06-13 232.402 2015-06-08 ## 2 2015-06-14 233.543 2015-06-08 ## 3 2015-06-15 236.823 2015-06-15 ## 4 2015-06-16 250.895 2015-06-15 ## 5 2015-06-17 249.284 2015-06-15 ## 6 2015-06-18 249.007 2015-06-15 Since we have the y column which is actually Bitcoin Price, we need to aggregate that weekly using the aggregate function week_y&lt;-aggregate(y ~ week, #refers to column for bitcoin data=ch3_p1.1, FUN = mean #specifies that the mean function should be applied to the numeric column within each week ) You will notice that this creates a separate data frame. We will merge week_y with ch3_p1.1 ch3_p1.1&lt;-merge(ch3_p1.1, week_y, by = &quot;week&quot;, #ensures the merge aligns based on the week suffixes = c(&quot;&quot;,&quot;_weekly&quot;)) #adds _weekly to the column name to distinguish them from the original columns We will slightly do the same thing when aggregating by month, quarter and year. I will do the initial steps, but please do the succeeding steps on your own. 7.1.7.2 Aggregate by Month # Add a month column ch3_p1.1$month &lt;- format(ch3_p1.1$ds, &quot;%Y-%m&quot;) # Calculate monthly means month_y &lt;- aggregate(y ~ month, data = ch3_p1.1, FUN = mean) Do the next steps as well as inspection on your own. 7.1.7.3 Aggregate by Quarter This is different since we will use the paste0 and the format functions. The format function extracts the year from the date and extracts the quarter from the date. The paste0 combines the year and quarter without a space between them so that it results in which quarter of which year. ch3_p1.1$quarter &lt;- paste0(format(ch3_p1.1$ds, &quot;%Y&quot;), &quot; &quot;, quarters(ch3_p1.1$ds)) 7.1.7.4 Aggregate by Year ch3_p1.1$year&lt;-format(ch3_p1.1$ds, &quot;%Y&quot;) Can you aggregate the Bitcoin values quarterly and yearly on your own with inspection? 7.2 Panel Data We will use a package in R containing different datasets. if(!(&quot;wooldridge&quot; %in% installed.packages()[,&quot;Package&quot;])) install.packages(&quot;wooldridge&quot;) library(wooldridge) data(&quot;wagepan&quot;) Unlike importing CSV, Excel or RData, since the data is found in a package, we call for the data through loading the package then data(\"dataset_name\"). The dataset will appear in the environment. str(wagepan) ## &#39;data.frame&#39;: 4360 obs. of 44 variables: ## $ nr : int 13 13 13 13 13 13 13 13 17 17 ... ## $ year : int 1980 1981 1982 1983 1984 1985 1986 1987 1980 1981 ... ## $ agric : int 0 0 0 0 0 0 0 0 0 0 ... ## $ black : int 0 0 0 0 0 0 0 0 0 0 ... ## $ bus : int 1 0 1 1 0 1 1 1 0 0 ... ## $ construc: int 0 0 0 0 0 0 0 0 0 0 ... ## $ ent : int 0 0 0 0 0 0 0 0 0 0 ... ## $ exper : int 1 2 3 4 5 6 7 8 4 5 ... ## $ fin : int 0 0 0 0 0 0 0 0 0 0 ... ## $ hisp : int 0 0 0 0 0 0 0 0 0 0 ... ## $ poorhlth: int 0 0 0 0 0 0 0 0 0 0 ... ## $ hours : int 2672 2320 2940 2960 3071 2864 2994 2640 2484 2804 ... ## $ manuf : int 0 0 0 0 0 0 0 0 0 0 ... ## $ married : int 0 0 0 0 0 0 0 0 0 0 ... ## $ min : int 0 0 0 0 0 0 0 0 0 0 ... ## $ nrthcen : int 0 0 0 0 0 0 0 0 0 0 ... ## $ nrtheast: int 1 1 1 1 1 1 1 1 1 1 ... ## $ occ1 : int 0 0 0 0 0 0 0 0 0 0 ... ## $ occ2 : int 0 0 0 0 0 1 1 1 1 1 ... ## $ occ3 : int 0 0 0 0 0 0 0 0 0 0 ... ## $ occ4 : int 0 0 0 0 0 0 0 0 0 0 ... ## $ occ5 : int 0 0 0 0 1 0 0 0 0 0 ... ## $ occ6 : int 0 0 0 0 0 0 0 0 0 0 ... ## $ occ7 : int 0 0 0 0 0 0 0 0 0 0 ... ## $ occ8 : int 0 0 0 0 0 0 0 0 0 0 ... ## $ occ9 : int 1 1 1 1 0 0 0 0 0 0 ... ## $ per : int 0 1 0 0 1 0 0 0 0 0 ... ## $ pro : int 0 0 0 0 0 0 0 0 0 0 ... ## $ pub : int 0 0 0 0 0 0 0 0 0 0 ... ## $ rur : int 0 0 0 0 0 0 0 0 0 0 ... ## $ south : int 0 0 0 0 0 0 0 0 0 0 ... ## $ educ : int 14 14 14 14 14 14 14 14 13 13 ... ## $ tra : int 0 0 0 0 0 0 0 0 0 0 ... ## $ trad : int 0 0 0 0 0 0 0 0 1 1 ... ## $ union : int 0 1 0 0 0 0 0 0 0 0 ... ## $ lwage : num 1.2 1.85 1.34 1.43 1.57 ... ## $ d81 : int 0 1 0 0 0 0 0 0 0 1 ... ## $ d82 : int 0 0 1 0 0 0 0 0 0 0 ... ## $ d83 : int 0 0 0 1 0 0 0 0 0 0 ... ## $ d84 : int 0 0 0 0 1 0 0 0 0 0 ... ## $ d85 : int 0 0 0 0 0 1 0 0 0 0 ... ## $ d86 : int 0 0 0 0 0 0 1 0 0 0 ... ## $ d87 : int 0 0 0 0 0 0 0 1 0 0 ... ## $ expersq : int 1 4 9 16 25 36 49 64 16 25 ... ## - attr(*, &quot;time.stamp&quot;)= chr &quot;25 Jun 2011 23:03&quot; In practice, it is best to leave the raw dataset untouched. Create a copy of the dataset and that is where you do modifications. ch3_p2&lt;-wagepan str(ch3_p2) ## &#39;data.frame&#39;: 4360 obs. of 44 variables: ## $ nr : int 13 13 13 13 13 13 13 13 17 17 ... ## $ year : int 1980 1981 1982 1983 1984 1985 1986 1987 1980 1981 ... ## $ agric : int 0 0 0 0 0 0 0 0 0 0 ... ## $ black : int 0 0 0 0 0 0 0 0 0 0 ... ## $ bus : int 1 0 1 1 0 1 1 1 0 0 ... ## $ construc: int 0 0 0 0 0 0 0 0 0 0 ... ## $ ent : int 0 0 0 0 0 0 0 0 0 0 ... ## $ exper : int 1 2 3 4 5 6 7 8 4 5 ... ## $ fin : int 0 0 0 0 0 0 0 0 0 0 ... ## $ hisp : int 0 0 0 0 0 0 0 0 0 0 ... ## $ poorhlth: int 0 0 0 0 0 0 0 0 0 0 ... ## $ hours : int 2672 2320 2940 2960 3071 2864 2994 2640 2484 2804 ... ## $ manuf : int 0 0 0 0 0 0 0 0 0 0 ... ## $ married : int 0 0 0 0 0 0 0 0 0 0 ... ## $ min : int 0 0 0 0 0 0 0 0 0 0 ... ## $ nrthcen : int 0 0 0 0 0 0 0 0 0 0 ... ## $ nrtheast: int 1 1 1 1 1 1 1 1 1 1 ... ## $ occ1 : int 0 0 0 0 0 0 0 0 0 0 ... ## $ occ2 : int 0 0 0 0 0 1 1 1 1 1 ... ## $ occ3 : int 0 0 0 0 0 0 0 0 0 0 ... ## $ occ4 : int 0 0 0 0 0 0 0 0 0 0 ... ## $ occ5 : int 0 0 0 0 1 0 0 0 0 0 ... ## $ occ6 : int 0 0 0 0 0 0 0 0 0 0 ... ## $ occ7 : int 0 0 0 0 0 0 0 0 0 0 ... ## $ occ8 : int 0 0 0 0 0 0 0 0 0 0 ... ## $ occ9 : int 1 1 1 1 0 0 0 0 0 0 ... ## $ per : int 0 1 0 0 1 0 0 0 0 0 ... ## $ pro : int 0 0 0 0 0 0 0 0 0 0 ... ## $ pub : int 0 0 0 0 0 0 0 0 0 0 ... ## $ rur : int 0 0 0 0 0 0 0 0 0 0 ... ## $ south : int 0 0 0 0 0 0 0 0 0 0 ... ## $ educ : int 14 14 14 14 14 14 14 14 13 13 ... ## $ tra : int 0 0 0 0 0 0 0 0 0 0 ... ## $ trad : int 0 0 0 0 0 0 0 0 1 1 ... ## $ union : int 0 1 0 0 0 0 0 0 0 0 ... ## $ lwage : num 1.2 1.85 1.34 1.43 1.57 ... ## $ d81 : int 0 1 0 0 0 0 0 0 0 1 ... ## $ d82 : int 0 0 1 0 0 0 0 0 0 0 ... ## $ d83 : int 0 0 0 1 0 0 0 0 0 0 ... ## $ d84 : int 0 0 0 0 1 0 0 0 0 0 ... ## $ d85 : int 0 0 0 0 0 1 0 0 0 0 ... ## $ d86 : int 0 0 0 0 0 0 1 0 0 0 ... ## $ d87 : int 0 0 0 0 0 0 0 1 0 0 ... ## $ expersq : int 1 4 9 16 25 36 49 64 16 25 ... ## - attr(*, &quot;time.stamp&quot;)= chr &quot;25 Jun 2011 23:03&quot; 7.2.1 Sorting Panel Data We need to sort panel data so that they are ordered by individual (id), then time. This will help you when you do lags and differences. ch3_p2&lt;-ch3_p2 %&gt;% arrange(nr, year) head(ch3_p2) ## nr year agric black bus construc ent exper fin hisp ## 1 13 1980 0 0 1 0 0 1 0 0 ## 2 13 1981 0 0 0 0 0 2 0 0 ## 3 13 1982 0 0 1 0 0 3 0 0 ## 4 13 1983 0 0 1 0 0 4 0 0 ## 5 13 1984 0 0 0 0 0 5 0 0 ## 6 13 1985 0 0 1 0 0 6 0 0 ## poorhlth hours manuf married min nrthcen nrtheast occ1 ## 1 0 2672 0 0 0 0 1 0 ## 2 0 2320 0 0 0 0 1 0 ## 3 0 2940 0 0 0 0 1 0 ## 4 0 2960 0 0 0 0 1 0 ## 5 0 3071 0 0 0 0 1 0 ## 6 0 2864 0 0 0 0 1 0 ## occ2 occ3 occ4 occ5 occ6 occ7 occ8 occ9 per pro pub rur ## 1 0 0 0 0 0 0 0 1 0 0 0 0 ## 2 0 0 0 0 0 0 0 1 1 0 0 0 ## 3 0 0 0 0 0 0 0 1 0 0 0 0 ## 4 0 0 0 0 0 0 0 1 0 0 0 0 ## 5 0 0 0 1 0 0 0 0 1 0 0 0 ## 6 1 0 0 0 0 0 0 0 0 0 0 0 ## south educ tra trad union lwage d81 d82 d83 d84 d85 d86 ## 1 0 14 0 0 0 1.197540 0 0 0 0 0 0 ## 2 0 14 0 0 1 1.853060 1 0 0 0 0 0 ## 3 0 14 0 0 0 1.344462 0 1 0 0 0 0 ## 4 0 14 0 0 0 1.433213 0 0 1 0 0 0 ## 5 0 14 0 0 0 1.568125 0 0 0 1 0 0 ## 6 0 14 0 0 0 1.699891 0 0 0 0 1 0 ## d87 expersq ## 1 0 1 ## 2 0 4 ## 3 0 9 ## 4 0 16 ## 5 0 25 ## 6 0 36 7.2.2 Checking Panel Balance When using panel data, checking for a balanced panel is necessary to know if every individual is observed every time period. An unbalanced panel means some individuals are missing years. This matters because many econometric assumptions rely on balance; such as selection bias in the sense that when you do not detect imbalance, you might miss events like firms exiting the market or respondents dropping out. table(table(ch3_p2$nr)) ## ## 8 ## 545 From the result, we know that each of the 545 individuals were observed 8 times. For unbalanced panel, you will see an output with multiple numbers as these indicate the missing periods and individuals. The table(table(id)) tells you whether each individual has the same amount of time in the panel. 7.2.3 Creating Lagged Variables We know that economic processes do not happen simultaneously, that is why it is important to use lags. Lags enforce causality, not correlation. In panel data, lags are within-individual; So, the question you are trying to answer here is: “How did this individual’s past outcome affect this person’s current outcome?” When using the Fixed Effects which rely on within-unit variation over time, lag adjustments are needed because without them, fixed effects would not work. ch3_p2&lt;-ch3_p2 %&gt;% group_by(nr) %&gt;% mutate(lwage_lag = lag(lwage)) head(ch3_p2) ## # A tibble: 6 × 45 ## # Groups: nr [1] ## nr year agric black bus construc ent exper fin ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 13 1980 0 0 1 0 0 1 0 ## 2 13 1981 0 0 0 0 0 2 0 ## 3 13 1982 0 0 1 0 0 3 0 ## 4 13 1983 0 0 1 0 0 4 0 ## 5 13 1984 0 0 0 0 0 5 0 ## 6 13 1985 0 0 1 0 0 6 0 ## # ℹ 36 more variables: hisp &lt;int&gt;, poorhlth &lt;int&gt;, ## # hours &lt;int&gt;, manuf &lt;int&gt;, married &lt;int&gt;, min &lt;int&gt;, ## # nrthcen &lt;int&gt;, nrtheast &lt;int&gt;, occ1 &lt;int&gt;, occ2 &lt;int&gt;, ## # occ3 &lt;int&gt;, occ4 &lt;int&gt;, occ5 &lt;int&gt;, occ6 &lt;int&gt;, ## # occ7 &lt;int&gt;, occ8 &lt;int&gt;, occ9 &lt;int&gt;, per &lt;int&gt;, ## # pro &lt;int&gt;, pub &lt;int&gt;, rur &lt;int&gt;, south &lt;int&gt;, ## # educ &lt;int&gt;, tra &lt;int&gt;, trad &lt;int&gt;, union &lt;int&gt;, … It is important to do group_by because each individual has their own time line and the lags stay within individuals. Therefore, what this does is treating each individual as separate time series. 7.2.4 Missing Data in Panels colSums(is.na(ch3_p2)) ## nr year agric black bus construc ## 0 0 0 0 0 0 ## ent exper fin hisp poorhlth hours ## 0 0 0 0 0 0 ## manuf married min nrthcen nrtheast occ1 ## 0 0 0 0 0 0 ## occ2 occ3 occ4 occ5 occ6 occ7 ## 0 0 0 0 0 0 ## occ8 occ9 per pro pub rur ## 0 0 0 0 0 0 ## south educ tra trad union lwage ## 0 0 0 0 0 0 ## d81 d82 d83 d84 d85 d86 ## 0 0 0 0 0 0 ## d87 expersq lwage_lag ## 0 0 545 ch3_p2.1&lt;-ch3_p2 %&gt;% filter(!is.na(lwage_lag)) 7.2.5 Creating Growth Rates ch3_p2 &lt;- ch3_p2 %&gt;% group_by(nr) %&gt;% mutate( wg = lwage - lag(lwage) ) head(ch3_p2$wg, 10) #due to number of cols, chose the one we just created and specified 10 observations ## [1] NA 0.65551984 -0.50859833 0.08875167 ## [5] 0.13491178 0.13176584 -2.42015356 2.38945049 ## [9] NA -0.15756428 What we did was create within-individual wage growth over time. Remember our discussion on lag? It will answer: “How much did this individual’s wage change from the previous period?” Furthermore, we are doing dynamic analysis here. We also note that the first observation per person has no lag. 7.3 Closing Quiz questions will be uploaded in Animospace. You have 15 minutes to answer. Clean the environment and free the memory AFTER the quiz. "],["visualizations-using-ggplot2.html", "8 Visualizations using ggplot2 8.1 Preliminaries 8.2 Pre-plotting Check 8.3 Visualizing Data using geom 8.4 Visualizing Time Series Data 8.5 Closing 8.6 Visualizations using Base R 8.7 Closing", " 8 Visualizations using ggplot2 For this lecture, we will be using ggplot2 package. Please make sure that you have it installed. The package works best with data in the ‘long’ format so it helps to modify the dataset to this format rather than a wide format. 8.1 Preliminaries 8.1.1 Load the dataset The dataset can be downloaded from the Modules. We will use Ch4PracticeA for this portion of the discussion. Unlike previous datasets that we loaded, we are now loading an Excel file. We will need the readxl package for this. Also, it is important to check if there are additional sheets and which sheet you will need. There are actually two sheets in the Excel file, but we will only use the first sheet named base. rm(list=ls()) gc() ## used (Mb) gc trigger (Mb) max used (Mb) ## Ncells 3170270 169.4 5173005 276.3 5173005 276.3 ## Vcells 6239997 47.7 12255594 93.6 10624508 81.1 library(readxl) ch4_p1 &lt;- read_excel(&quot;Ch4PracticeA.xlsx&quot;, sheet = &quot;base&quot;) I will not delve deeply in the description. Please read up on it. The description can be found in the last sheet of the Excel file. Next, we load the following package: tidyverse and ggplot2. library(tidyverse) library(ggplot2) 8.2 Pre-plotting Check In some cases, ggplot2 does not run. This can be brought by package mismatch, so, we need to double-check that everything is working properly. user &lt;- file.path(Sys.getenv(&quot;USERPROFILE&quot;), &quot;R&quot;, &quot;win-library&quot;, &quot;4.5&quot;) dir.create(user, recursive = TRUE, showWarnings = FALSE) .libPaths(c(user, .libPaths())) ok &lt;- tryCatch({ library(ggplot2) ggplot(data.frame(x=1:5, y=rnorm(5)), aes(x,y)) + geom_point() TRUE }, error = function(e) FALSE) if (!ok) { install.packages(&quot;rlang&quot;, lib = user_lib) ok &lt;- tryCatch({ library(ggplot2) ggplot(data.frame(x=1:5, y=rnorm(5)), aes(x,y)) + geom_point() TRUE }, error = function(e) FALSE) } if (!ok) plot(1:5, rnorm(5)) If it is still not working, please refer to Visualization using Base R 8.2.1 Template There is a basic template that can be used for different types of plots: &lt;DATA&gt; %&gt;% ggplot(aes(&lt;MAPPINGS&gt;))+ &lt;GEOM_FUNCTION&gt;() ggplot is a function that expects a data frame to be the first argument. This allows for us to change from specifying the data = argument within the ggplot function and instead pipe the data into the function. Use the ggplot() function… ch4_p1 %&gt;% ggplot() Now, we define the mapping (using the aesthetic (aes) function), by selecting the variables to be plotted and specifying how to present them in the graph, e.g. as x/y positions or characteristics such as size, shape, color, etc. ch4_p1 %&gt;% ggplot(aes(x=poverty_index, y=health_expenditures)) The next step is to add geom which will make the graphical representations of the data. These include: geom_point() for scatter plots, dot plots, etc. geom_boxplot() for boxplots geom_line() for trend lines, time series, etc. geom_bar() for bar plots and pie charts 8.3 Visualizing Data using geom 8.3.1 Scatterplots Let us use the geom_point() first then we will do the others after. Also, scatterplots are useful when you want to display the relationship between two continuous variables. Can you give me an example of when to use scatterplots? ch4_p1 %&gt;% ggplot(aes(x=poverty_index, y=health_expenditures)) + geom_point() This visualization is so unclear; this is due to the number of observations being more than 9,000. Let’s just use the first 500 rows as this is for our practice and for visualization purposes. fch4_p1 &lt;- head(ch4_p1, 500) View(fch4_p1) This is more manageable; let’s try the scatterplot again, this time using the fch4_p1 dataset. fch4_p1%&gt;% ggplot(aes(x=poverty_index, y=health_expenditures)) + geom_point() This is more visible; There are some points that overlap with each other. Let us incorporate some strategies to try and ensure that there will be no overplotting issues. The first strategy is changing the transparency of the points. To control the transparency of points, we add the alpha argument. The range of transparency is from 0 to 1, with lower values corresponding to more transparency. The default value is 1. Let’s try to change the alpha to 0.5. fch4_p1 %&gt;% ggplot(aes(x=poverty_index, y=health_expenditures)) + geom_point(alpha=0.5) Some of the points are gray while the others are much darker, then we can see (slightly) the difference. Another method that we can do is jittering the points on the plot to see the locations where there are overplotting points. Jittering adds randomness into the position of the points. To do this, we add geom_jitter() rather than geom_point(). Also, we need to edit the width and height. You can experiment but if you want less spread, pick values between 0.1 and 0.4. fch4_p1 %&gt;% ggplot(aes(x=poverty_index, y=health_expenditures)) + geom_jitter(alpha = 0.5, width= 0.3, height= 0.3) Let us add color to geom_jitter() fch4_p1 %&gt;% ggplot(aes(x=poverty_index, y=health_expenditures)) + geom_jitter(alpha = 0.5, color = &quot;darkgreen&quot;, width= 0.3, height= 0.3) If you want to have different colors depending on a certain variable, we need to use a vector as an input in the argument color. Here though, we map features of the data to a certain color. When we map a variable in our data to the color of the points, ggplot2 will provide a different color corresponding to the different values of the variable. We will continue to specify the value of alpha, width, and height outside of the aes function because we are using the same value for every point. fch4_p1$enrolled&lt;-as.factor(fch4_p1$enrolled) fch4_p1 %&gt;% ggplot(aes(x=poverty_index, y=health_expenditures))+ geom_jitter(aes(color=enrolled), alpha=0.5, width=0.3, height=0.3) We can add a regression line to the plot. The line helps us summarize and predict the relationship between our two variables. To add, it is geom_smooth(method=\"lm\",...) you can edit the design and you can remove the confidence interval (gray area) with placing inside the parenthesis se=FALSE. fch4_p1$enrolled&lt;-as.factor(fch4_p1$enrolled) fch4_p1 %&gt;% ggplot(aes(x=poverty_index, y=health_expenditures))+ geom_jitter(aes(color=enrolled), alpha=0.5, width=0.3, height=0.3)+ geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;, linewidth = 1) ## `geom_smooth()` using formula = &#39;y ~ x&#39; 8.3.2 Boxplots Here is how to make a box plot that is useful when summarizing the distribution of a continuous variable, highlighting the median, quartiles, and potential outliers. To interpret a boxplot, take note of the following things: Horizontal Line in the Box: this is the median wherein it represents the 50% of the data. The Box Itself (Q1 to Q3): The box represents the middle 50% of the data, at the bottom of the box, that is the Q1 (25th percentile) while the top of box is the Q3 (75th percentile). A taller box means more variability is present in the middle of the data Whiskers extend from the box to represent the range of the data excluding outliers so it usually end at the smallest and largest data points. It shows the spread of data outside the middle 50%. Longer whiskers point to greater spread in that direction Outliers are points outside the whiskers. 8.3.2.1 Skewness This describes the asymmetry of a data distribution, it can be right-skewed or left-skewed. Boxplots are a great way to visualize skewness. Right-skewed The right tail is longer, meaning, median is closer to the bottom and has a longer top whisker. Most data values are smaller with few large values ex. income, price Left-skewed The left tail is longer, meaning, median is closer to the top and has a longer bottom whisker. Most data values are large with few small values test scores *The labs argument is to add labels in the plot. fch4_p1 %&gt;% ggplot(aes(x = as.factor(treatment_locality), y = health_expenditures)) + geom_boxplot(fill = &quot;green&quot;) + labs(title = &quot;Box Plot: Health Expenditures by Treatment Locality&quot;, x = &quot;Treatment Locality&quot;, y = &quot;Health Expenditures&quot;) + theme_minimal() You can also add some points in the box plot by adding geom_jitter fch4_p1 %&gt;% ggplot(aes(x = as.factor(treatment_locality), y = health_expenditures)) + geom_boxplot(fill=&quot;green&quot;) + geom_jitter(alpha = 0.5, color = &quot;tomato&quot;, width = 0.2, height = 0.2) + labs(title = &quot;Box Plot: Health Expenditures by Treatment Locality&quot;, x = &quot;Treatment Locality&quot;, y = &quot;Health Expenditures&quot;) + theme_minimal() 8.3.3 Bar plots Barplots are also useful for visualizing categorical data. By default, geom_bar accepts a variable for x, and plots the number of instances each value of x (in this case, treatment_locality) appears in the dataset. fch4_p1 %&gt;% ggplot(aes(x = as.factor(treatment_locality))) + geom_bar(fill = &quot;lightgreen&quot;) + labs(title = &quot;Bar Plot: Count of Localities by Treatment Locality&quot;, x = &quot;Treatment Locality&quot;, y = &quot;Count&quot;) + theme_minimal() Let us change the fill to be locality_identifier. Note, we will have a lot of colors here but this is done for visualization purposes and practice. fch4_p1 %&gt;% ggplot(aes(x = as.factor(treatment_locality), fill = as.factor(locality_identifier))) + geom_bar() + labs(title = &quot;Bar Plot: Count of Localities by Treatment Locality and Locality&quot;, x = &quot;Treatment Locality&quot;, y = &quot;Count&quot;, fill = &quot;Locality&quot;) + theme_minimal() This creates a stacked bar chart. These are generally more difficult to read than side-by-side bars. We can separate the portions of the stacked bar that correspond to each locality and put them side-by-side by using the position argument for geom_bar() and setting it to “dodge”. fch4_p1 %&gt;% ggplot(aes(x = as.factor(treatment_locality), fill = as.factor(locality_identifier))) + geom_bar(position = &quot;dodge&quot;) + labs(title = &quot;Bar Plot: Count of Localities by Treatment Locality and Locality&quot;, x = &quot;Treatment Locality&quot;, y = &quot;Count&quot;, fill = &quot;Locality&quot;) + theme_minimal() What if you want to visualize a numeric value for each category? geom_bar() only visualizes the number of observations for a categorical x-variable. We use geom_col() which allows us to visualize numeric values. I will create 4 groups for locality_group so that we have better-looking bars. fch4_p1 &lt;- fch4_p1 %&gt;% mutate(locality_group = cut(locality_identifier, breaks = c(20, 40, 60, 80, 100), labels = c(&quot;20-40&quot;, &quot;40-60&quot;, &quot;60-80&quot;, &quot;80-100&quot;), include.lowest = TRUE)) table(fch4_p1$locality_group) ## ## 20-40 40-60 60-80 80-100 ## 197 42 66 51 fch4_p1 %&gt;% ggplot(aes(x = as.factor(locality_group), y=health_expenditures, fill = as.factor(locality_group))) + geom_col(position = &quot;dodge&quot;) + labs(title = &quot;Bar Plot: Count of Localities by Treatment Locality and Locality&quot;, x = &quot;Locality Group&quot;, y = &quot;Health Expenditures&quot;, fill = &quot;Locality&quot;) + theme_minimal() Want to have height arrangement? Since locality_group appears multiple times, we have to calculate for the mean expenditures per locality_group or total expenditures per locality_group using the .fun=mean or .fun=sum then, from largest to smallest height, we say .desc = TRUE. The fct_reorder is used because we have the calculations per group and the bars are reordered based on the calculations. fch4_p1 %&gt;% mutate(locality_group = fct_reorder(locality_group, health_expenditures, .fun = sum, .desc = TRUE)) %&gt;% ggplot(aes(x = locality_group, y = health_expenditures, fill = locality_group)) + geom_col() + labs( title = &quot;Health Expenditures by Locality Group&quot;, x = &quot;Locality Group&quot;, y = &quot;Health Expenditures&quot;, fill = &quot;Locality Group&quot; ) + theme_minimal() # + coord_flip() if names are too long 8.3.4 Faceting ggplot2 has a special technique called faceting that allows the user to split one plot into multiple plots based on a factor included in the dataset. fch4_p1 %&gt;% ggplot(aes(x = poverty_index, y = health_expenditures)) + geom_point(alpha = 0.6, color = &quot;darkblue&quot;) + facet_wrap(~ locality_identifier, ncol = 4) + labs(title = &quot;Faceted Scatter Plot by Locality Identifier&quot;, x = &quot;Poverty Index&quot;, y = &quot;Health Expenditures&quot;) + theme_minimal() It doesn’t look that nice; let’s use the locality_group we created before. fch4_p1 %&gt;% ggplot(aes(x = poverty_index, y = health_expenditures)) + geom_point(alpha = 0.6, color = &quot;darkgreen&quot;) + facet_wrap(~ locality_group, ncol = 2) + labs(title = &quot;Faceted Scatter Plot by Locality Identifier&quot;, x = &quot;Poverty Index&quot;, y = &quot;Health Expenditures&quot;) + theme_minimal() 8.3.5 Pie Chart Pie charts are used to illustrate proportions or parts of a whole (limited to a small number of categories). Before we do the pie chart, we need to count how many observations there are in the variable we want to analyze. treatment_counts &lt;- fch4_p1 %&gt;% count(treatment_locality) ggplot(treatment_counts, aes(x = &quot;&quot;, y = n, fill = as.factor(treatment_locality))) + geom_bar(stat = &quot;identity&quot;, width = 1) + coord_polar(theta = &quot;y&quot;) + #this makes it a pie chart labs(title = &quot;Pie Chart: Proportion of Treatment Localities&quot;, fill = &quot;Treatment Locality&quot;) + theme_void() #another way to have a nice background If you want to put labels to each slice and let’s say you want the percentage for it, this is the code: treatment_counts &lt;- fch4_p1 %&gt;% count(treatment_locality) %&gt;% #for count mutate(percentage = n / sum(n) * 100, #calculate percentage label = paste0(treatment_locality, &quot;\\n&quot;, round(percentage, 1), &quot;%&quot;)) #add label for percentage ggplot(treatment_counts, aes(x = &quot;&quot;, y = n, fill = as.factor(treatment_locality))) + geom_bar(stat = &quot;identity&quot;, width = 1) + coord_polar(theta = &quot;y&quot;) + geom_text(aes(label = label), position = position_stack(vjust = 0.5), size = 4) + labs(title = &quot;Pie Chart: Proportion of Treatment Localities&quot;, fill = &quot;Treatment Locality&quot;) + theme_void() 8.3.6 Histogram Though it looks similar to a bar plot, a histogram is different since it displays the distribution of a continuous variable. It groups the data into intervals called bins and shows the frequency of data points within each bin. fch4_p1 %&gt;% ggplot(aes(x=poverty_index))+ geom_histogram(binwidth = 5, fill=&quot;green&quot;, color=&quot;black&quot;)+ theme_minimal() Here is a cheat sheet for ggplot2 from the ones who developed the package: ggplot2 Cheat Sheet 8.4 Visualizing Time Series Data When visualizing time series data, it is important to ensure that the time variable is formatted as Date. For this portion of the lecture, we use Ch4PracticeB.xlsx which is found in the Modules. Make sure to clean the environment, load the file and rename the columns since they are quite long. I will not show the codes for this portion anymore as I am sure you already know how. ## used (Mb) gc trigger (Mb) max used (Mb) ## Ncells 3264866 174.4 5173005 276.3 5173005 276.3 ## Vcells 6393979 48.8 12255594 93.6 10624508 81.1 ## # A tibble: 6 × 12 ## year nominal_gdp_current nominal_gdp_constant ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1986 692852 4298952 ## 2 1987 777283 4486464 ## 3 1988 910280 4786920 ## 4 1989 1054529 5082939 ## 5 1990 1227882 5239629 ## 6 1991 1422958 5216764 ## # ℹ 9 more variables: gdp_growth_current &lt;dbl&gt;, ## # gdp_growth_constant &lt;dbl&gt;, total_debt &lt;dbl&gt;, ## # debt_to_gdp &lt;dbl&gt;, interest_payments &lt;dbl&gt;, ## # amortization_payments &lt;dbl&gt;, ## # phil_us_exchange_rate &lt;dbl&gt;, inflation &lt;dbl&gt;, ## # fdi_net &lt;dbl&gt; Ensure the time variable is formatted as Date I will not show the code for this since this has been done in previous lectures. Template To make a time series visualization, this is the template: ggplot(data, aes(x = time_variable, y = value_variable)) + geom_line(color = &quot;blue&quot;) + #adds a trend line labs(title = &quot;Time Series Plot&quot;, x = &quot;Time&quot;, y = &quot;Value&quot;) + theme_minimal() Time Series Plot You might wonder why mine does not have a warning. Usually, there will be warnings that will come out. To remove it, simply add inside the {r}, warning=FALSE like: {r,warning=FALSE} options(scipen=999) #not scientific notation ch4_p2 %&gt;% ggplot(aes(x=year, y=nominal_gdp_current))+ geom_line(color=&quot;darkgreen&quot;)+ labs(title = &quot;Time Series Plot of Nominal GDP (Current)&quot;, x = &quot;Year&quot;, y = &quot;Nominal GDP (Current)&quot;) + theme_minimal() We can add points for clarity and change size of line to be thicker. options(scipen=999) ch4_p2 %&gt;% ggplot(aes(x=year, y=nominal_gdp_current))+ geom_line(color=&quot;pink&quot;, size=1)+ geom_point(color= &quot;purple&quot;)+ labs(title = &quot;Time Series Plot of Nominal GDP (Current)&quot;, x = &quot;Year&quot;, y = &quot;Nominal GDP (Current)&quot;) 8.4.1 Save Graphs using ggplot2 Every time you plot, you create an object so it should be like p&lt;-ggplot(...) so you can save the plots. ggsave(&quot;my_plot.png&quot;, plot = p, width = 6, height = 4, dpi = 300) width and height are in inches while dpi is the resolution, 300 is good for printing. 8.5 Closing Quiz will be done for 15 minutes Clean the environment BEFORE the quiz. You will have a different dataset for the quiz. 8.6 Visualizations using Base R library(readxl) ch4_p1 &lt;- read_excel(&quot;Ch4PracticeA.xlsx&quot;, sheet = &quot;base&quot;) fch4_p1 &lt;- head(ch4_p1, 500) 8.6.1 Scatterplots plot(fch4_p1$poverty_index, fch4_p1$health_expenditures, xlab= &quot;Poverty Index&quot;, ylab= &quot;Health Expenditures&quot;, main=&quot;Scatterplot of Poverty Index vs Health Expenditures&quot;) Transparency In ggplot2, we use alpha but in Base R, we use rgb() plot(fch4_p1$poverty_index, fch4_p1$health_expenditures, col = rgb(0,0,0,0.5), pch=16, xlab= &quot;Poverty Index&quot;, ylab= &quot;Health Expenditures&quot;, main=&quot;Scatterplot of Poverty Index vs Health Expenditures&quot;) rgb for black is r=0, g=0, b=0 then the last is alpha 50% transparency. pch = 16 is solid filled circle, if it is 1, it is a hollow circle. Jitter plot(jitter(fch4_p1$poverty_index, amount = 0.3), jitter(fch4_p1$health_expenditures, amount=0.3), col=rgb(0,0.5,0, 0.5), pch=16, xlab= &quot;Poverty Index&quot;, ylab= &quot;Health Expenditures&quot;, main=&quot;Scatterplot of Poverty Index vs Health Expenditures&quot;) The amount is 0.3 which is the width and height equivalent of the jitter using ggplot2. We changed the color to green so g=0.5. Color by group (aes(color=enrolled)) colors&lt;-as.factor(fch4_p1$enrolled) plot(jitter(fch4_p1$poverty_index, amount = 0.3), jitter(fch4_p1$health_expenditures, amount=0.3), col= colors, pch=16, xlab= &quot;Poverty Index&quot;, ylab= &quot;Health Expenditures&quot;, main=&quot;Scatterplot of Poverty Index vs Health Expenditures&quot;) legend(&quot;topright&quot;, legend = levels(colors), col=1:length(levels(colors)), pch=16, title = &quot;Enrolled&quot;) We need to convert enrolled into categorical variable so that each value becomes a group. Base R automatically decides on the colors to each factor. In the legend, we manually tell R to provide it, and we set legend = levels(colors), as factor levels being their legend, col=1:length(levels(colors)) assigns legend colors then pch=16 for matching the plot symbols. Adding Regression Line You can add a regression line by adding abline(lm(y~x),...) colors&lt;-as.factor(fch4_p1$enrolled) plot(jitter(fch4_p1$poverty_index, amount = 0.3), jitter(fch4_p1$health_expenditures, amount=0.3), abline(lm(fch4_p1$health_expenditures ~ fch4_p1$poverty_index), col = &quot;red&quot;, lwd=2), col= colors, pch=16, xlab= &quot;Poverty Index&quot;, ylab= &quot;Health Expenditures&quot;, main=&quot;Scatterplot of Poverty Index vs Health Expenditures&quot;) legend(&quot;topright&quot;, legend = levels(colors), col=1:length(levels(colors)), pch=16, title = &quot;Enrolled&quot;) 8.6.2 Boxplots boxplot(health_expenditures ~ as.factor(treatment_locality), data=fch4_p1) If you want to change color, just add col = Adding jittered points boxplot(health_expenditures ~ as.factor(treatment_locality), data=fch4_p1, col=&quot;green&quot;, outline=FALSE) stripchart(health_expenditures ~ as.factor(treatment_locality), data=fch4_p1, vertical = TRUE, method = &quot;jitter&quot;, jitter = 0.2, pch=16, col = rgb(1,0.2,0.2,0.5), #similar to tomato color with transparency add = TRUE) We need to draw the boxplot before we add the jitter. outline = FALSE prevents double-plotting, stripchart() overlays the individual observations on the boxplot with jittering to avoid overlap, vertical = TRUE aligns points with the boxplots. 8.6.3 Bar Plots counts&lt;-table(fch4_p1$treatment_locality) barplot(counts, col=&quot;green&quot;) We need to count the values while barplot visualizes the counts Stacked Bar Plots counts_matrix &lt;- with(fch4_p1, table(treatment_locality, locality_identifier)) In the stacked bar plots, rows are the treatment_locality while columns are locality_identifier barplot(t(counts_matrix), col = rainbow(ncol(counts_matrix)), #color per row legend=TRUE) Side-by-Side Bars barplot(t(counts_matrix), beside = TRUE, # side-by-side col = rainbow(ncol(counts_matrix)), legend.text = TRUE) Reordering bars/when specified numeric value Unlike ggplot2, in base R, we need to calculate first. fch4_p1 &lt;- fch4_p1 %&gt;% mutate(locality_group = cut(locality_identifier, breaks = c(20, 40, 60, 80, 100), labels = c(&quot;20-40&quot;, &quot;40-60&quot;, &quot;60-80&quot;, &quot;80-100&quot;), include.lowest = TRUE)) table(fch4_p1$locality_group) ## ## 20-40 40-60 60-80 80-100 ## 197 42 66 51 agg &lt;- aggregate(health_expenditures ~ locality_group, data = fch4_p1, sum) agg &lt;- agg[order(agg$health_expenditures, decreasing = TRUE), ] barplot( height = agg$health_expenditures, names.arg = agg$locality_group, col = rainbow(nrow(agg)), main = &quot;Health Expenditures by Locality Group&quot;, xlab = &quot;Locality Group&quot;, ylab = &quot;Health Expenditures&quot;, las = 2 # rotate axis labels for readability ) 8.6.4 Faceting par(mfrow = c(2, 2)) for (g in levels(fch4_p1$locality_group)) { subset_data &lt;- subset(fch4_p1, locality_group == g) plot(subset_data$poverty_index, subset_data$health_expenditures, main = paste(&quot;Locality Group:&quot;, g), xlab = &quot;Poverty Index&quot;, ylab = &quot;Health Expenditures&quot;, pch = 16, col = rgb(0, 0.5, 0, 0.6)) } par(mfrow = c(1, 1)) You will notice that unlike ggplot2, Base R skips the missing. To avoid this, you have to create a separate category for missing. par(mfrow=c(nrows, ncols)) splits the plotting area then we created a function that loops each locality_group to have a different plot. Then, we reset the plotting layout with par(mfrow=c(1,1)) for succeeding plots. 8.6.5 Pie Chart tcounts &lt;- table(fch4_p1$treatment_locality) pie(tcounts, col = rainbow(length(tcounts)), main = &quot;Pie Chart: Proportion of Treatment Localities&quot;) Again, table() counts instances per category, while rainbow assigns a color to each category Adding Percentages per slice pslice &lt;- round(100 * tcounts / sum(tcounts), 1) #We create labels that includes the name of slice with the percent of each slice labels &lt;- paste(names(tcounts), &quot;\\n&quot;, pslice, &quot;%&quot;, sep=&quot;&quot;) pie(tcounts, labels = labels, col = rainbow(length(tcounts)), main = &quot;Pie Chart: Proportion of Treatment Localities&quot;) 8.6.6 Histogram hist(fch4_p1$poverty_index, col = &quot;green&quot;, border = &quot;black&quot;, xlab = &quot;Poverty Index&quot;, main = &quot;Histogram of Poverty Index&quot;) R automatically chooses breaks. 8.6.7 Time Series options(scipen=999) plot(ch4_p2$year, ch4_p2$nominal_gdp_current, type = &quot;l&quot;, col = &quot;darkgreen&quot;, xlab = &quot;Year&quot;, ylab = &quot;Nominal GDP (Current)&quot;, main = &quot;Time Series Plot of Nominal GDP (Current)&quot;) To make the line, the type is small letter L, not 1. To add points: options(scipen=999) plot(ch4_p2$year, ch4_p2$nominal_gdp_current, type = &quot;l&quot;, col = &quot;pink&quot;, lwd = 2, xlab = &quot;Year&quot;, ylab = &quot;Nominal GDP (Current)&quot;, main = &quot;Time Series Plot of Nominal GDP (Current)&quot;) points(ch4_p2$year, ch4_p2$nominal_gdp_current, col = &quot;purple&quot;, pch = 16) 8.6.8 Save Graphs using Base R When creating plots, you can also create an object, p&lt;-plot png(&quot;my_plot.png&quot;, width = 800, height = 600) # open PNG device print(p) # draw the plot dev.off() # close device and save file You can also place the plotting code between png() and dev.off() 8.7 Closing Quiz will be done for 15 minutes Clean the environment BEFORE the quiz. You will have a different dataset for the quiz. "],["advanced-visualizations.html", "9 Advanced Visualizations 9.1 Preliminaries 9.2 Animated Visualizations 9.3 Animated Time Series 9.4 Animated Faceted Time Series 9.5 Maps 9.6 Static Maps 9.7 Philippine Regional Map 9.8 Animated Map 9.9 Practical: Advanced Visualizations", " 9 Advanced Visualizations 9.1 Preliminaries 9.1.1 Install Packages For this lecture, you need to install a lot of packages. Please do this before our lecture as it will take a long time to install them. Furthermore, there might be issues in installing, such as needing to install other packages. Please let the professor know if you encounter any issues. # Install necessary packages if (!require(gganimate)) install.packages(&quot;gganimate&quot;) ## Loading required package: gganimate if (!require(ggplot2)) install.packages(&quot;ggplot2&quot;) if (!require(dplyr)) install.packages(&quot;dplyr&quot;) if (!require(sf)) install.packages(&quot;sf&quot;) ## Loading required package: sf ## Linking to GEOS 3.13.1, GDAL 3.11.4, PROJ 9.7.0; sf_use_s2() ## is TRUE if (!require(rnaturalearth)) install.packages(&quot;rnaturalearth&quot;) ## Loading required package: rnaturalearth if (!require(readr)) install.packages(&quot;readr&quot;) if (!require(tidyr)) install.packages(&quot;tidyr&quot;) if (!require(gifski)) install.packages(&quot;gifski&quot;) ## Loading required package: gifski if (!require(WDI)) install.packages(&quot;WDI&quot;) ## Loading required package: WDI if (!require(leaflet)) install.packages(&quot;leaflet&quot;) ## Loading required package: leaflet if (!require(rnaturalearthdata)) install.packages(&quot;rnaturalearthdata&quot;) ## Loading required package: rnaturalearthdata ## ## Attaching package: &#39;rnaturalearthdata&#39; ## The following object is masked from &#39;package:rnaturalearth&#39;: ## ## countries110 #Load packages library(gganimate) library(ggplot2) library(dplyr) library(sf) library(rnaturalearth) library(rnaturalearthdata) library(readr) library(tidyr) library(gifski) library(WDI) library(leaflet) 9.2 Animated Visualizations gganimate includes animation to ggplot2; It adds some classes to the plot object in order to customise how it should change with time. transition_*() defines how the data should be spread out and how it relates to itself across time. view_*() defines how the positional scales should change along the animation. shadow_*() defines how data from other points in time should be presented in the given point in time. enter_*()/exit_*() defines how new data should appear and how old data should disappear during the course of the animation. ease_aes() defines how different aesthetics should be eased during transitions For this lecture, we will use the quotas dataset and fetch some World Bank Development Indicators from the WDI package. 9.2.1 Animated Bar Chart At the start, you need to do the same steps as that of when doing the visualizations without animations. rm(list = ls()) gc() ## used (Mb) gc trigger (Mb) max used (Mb) ## Ncells 3486903 186.3 5173005 276.3 5173005 276.3 ## Vcells 6874057 52.5 14786712 112.9 10624508 81.1 # Load the dataset quotas &lt;- read.csv(&quot;quotas.csv&quot;) ch5.1&lt;-quotas %&gt;% ggplot(aes(x=factor(AC_type_noST)))+ geom_bar(fill=&quot;darkgreen&quot;, color=&quot;black&quot;)+ labs(title = &quot;Animated Bar Chart: Assembly Constituencies&quot;, x=&quot;Reservation Status&quot;, y=&quot;Count&quot;)+ theme_minimal()+ transition_states(AC_type_noST, transition_length = 2, state_length = 1) The transition_states animates transitions between different categorical states. The AC_type_noST is the categorical variable that defines different animation states. The transition_length controls how long the transition between states lasts, measured in animation frames. While the state_length defines how long each state remains static before transitioning to the next one. This is very important; it is different for Quarto where the gif is automatically rendered; however, later, we will find out how to save in Quarto. anim_file &lt;- &quot;bar_chart.gif&quot; animate(ch5.1, duration = 4, fps = 10, renderer = gifski_renderer(anim_file), preview = TRUE) # Preview the animation before rendering # Save animation as a GIF knitr::include_graphics(anim_file) The animate function generates the animation of the ggplot object ch5.1. duration = 4: Specifies that the animation should run for 4 seconds in total. fps = 10: Defines the frame rate, meaning the animation will show 10 frames per second. renderer = gifski_renderer(anim_file): Uses the gifski_renderer to save the animation as a GIF file named bar_chart.gif. preview = TRUE: Allows you to view the animation immediately in the RStudio Viewer before saving it as a file. 9.2.2 Animated Scatter Plot ch5.2&lt;-quotas %&gt;% ggplot(aes(x=Plit71, y=P_W71))+ geom_point(aes(color=factor(AC_type_noST)))+ labs(title=&quot;Animated Scatter Plot: Literacy vs. Employment&quot;, x=&quot;Literacy Rate (1971)&quot;, y=&quot;Employment Rate (1971)&quot;, color=&quot;Reservation Status&quot;)+ theme_minimal()+ transition_reveal(Plit71) Here, the transition_reveal animates the points to be revealed over literacy rates anim_file2&lt;-&quot;scatter_plot.gif&quot; animate(ch5.2, duration=20, fps=10, renderer=gifski_renderer(anim_file2), preview=TRUE) knitr::include_graphics(anim_file2) To slow down the animation, increase the duration and decrease fps. 9.2.3 Animated Faceted Scatter Plot 9.2.3.1 Fetching WDI Data For this portion, we will make use of the WDI database. To search which indicator you wish to work with, type WDIsearch(\"keyword\") ch5&lt;-WDI( country = c(&quot;USA&quot;,&quot;CHN&quot;, &quot;IND&quot;, &quot;BRA&quot;,&quot;NLD&quot;, &quot;JPN&quot;), indicator = c(&quot;NY.GDP.PCAP.CD&quot;, &quot;SP.DYN.LE00.IN&quot;), start = 2000, end = 2020, extra = TRUE ) This code chunk pulls data from the World Bank WDI database of 6 countries and two indicators (GDP per capita (current US$) and Life Expectancy at Birth (years). It fetches data from 2000 to 2020 and includes extra metadata such as region names and income levels. #Cleaning the dataset ch5 &lt;- ch5 %&gt;% rename(GDPpc = NY.GDP.PCAP.CD, Life_Exp = SP.DYN.LE00.IN) ch5&lt;-ch5 %&gt;% filter(!is.na(GDPpc), !is.na(Life_Exp)) head(ch5) ## country iso2c iso3c year status lastupdated GDPpc ## 1 Brazil BR BRA 2000 2026-01-28 3766.548 ## 2 Brazil BR BRA 2001 2026-01-28 3176.290 ## 3 Brazil BR BRA 2002 2026-01-28 2855.940 ## 4 Brazil BR BRA 2003 2026-01-28 3090.607 ## 5 Brazil BR BRA 2004 2026-01-28 3663.823 ## 6 Brazil BR BRA 2005 2026-01-28 4827.782 ## Life_Exp region capital longitude ## 1 69.584 Latin America &amp; Caribbean Brasilia -47.9292 ## 2 69.980 Latin America &amp; Caribbean Brasilia -47.9292 ## 3 70.396 Latin America &amp; Caribbean Brasilia -47.9292 ## 4 70.884 Latin America &amp; Caribbean Brasilia -47.9292 ## 5 71.361 Latin America &amp; Caribbean Brasilia -47.9292 ## 6 71.832 Latin America &amp; Caribbean Brasilia -47.9292 ## latitude income lending ## 1 -15.7801 Upper middle income IBRD ## 2 -15.7801 Upper middle income IBRD ## 3 -15.7801 Upper middle income IBRD ## 4 -15.7801 Upper middle income IBRD ## 5 -15.7801 Upper middle income IBRD ## 6 -15.7801 Upper middle income IBRD ch5.3&lt;-ch5 %&gt;% ggplot(aes(x=GDPpc, y=Life_Exp))+ geom_point(aes(color=region), alpha=0.7, size=3)+ labs(title=&quot;Faceted Scatter Plot: GDP vs. Life Expectancy&quot;, subtitle = &quot;Year: 2000-2020&quot;, x=&quot;GDP per Capita (USD)&quot;, y=&quot;Life Expectancy (Years)&quot;, color=&quot;Region&quot;)+ theme_minimal()+ facet_wrap(~country, ncol=3)+ scale_x_log10()+ #log scale for better visualization transition_states(year, transition_length = 2, state_length = 1) There are additional things here like the size=3 which changes the size of points. the scale_x_log10() was added because it applies a logarithmic scale to the x-axis since GDP per capital values vary widely, and a log scale makes comparisons clearer. The transition_states has year since each frame would represent a different year. anim_file3&lt;-&quot;faceted_scatter_plot.gif&quot; animate(ch5.3, duration=10, fps=15, renderer=gifski_renderer(anim_file3), preview=TRUE) knitr::include_graphics(anim_file3) This does not really provide any information. Let us try visualizing in a different way. 9.3 Animated Time Series Let us just choose USA for this. ch5_1&lt;-ch5 %&gt;% filter(iso3c==&quot;USA&quot;) ch5.4&lt;-ch5_1 %&gt;% ggplot(aes(x=year, y=GDPpc))+ geom_line(color=&quot;green&quot;, size = 1.2)+ geom_point(color=&quot;purple&quot;, size=2)+ labs(title = &quot;US GDP per Capita Growth over Time&quot;, subtitle = &quot;Year:2000-2020&quot;, x=&quot;Year&quot;, y=&quot;GDP per Capita (Current US$)&quot;)+ theme_minimal()+ transition_reveal(year) anim_file4&lt;-&quot;us_timeseries.gif&quot; animate(ch5.4, duration = 10, fps = 15, renderer = gifski_renderer(anim_file4), preview = TRUE) knitr::include_graphics(anim_file4) 9.4 Animated Faceted Time Series ch5.5&lt;-ch5 %&gt;% ggplot(aes(x=year, y=GDPpc, group=country))+ geom_line(aes(color=country), size=1.2)+ labs(title = &quot;Faceted Time Series: GDP Growth Over Time&quot;, subtitle = &quot;Year: 2000-2020&quot;, x=&quot;Year&quot;, y=&quot;GDP (Current US$)&quot;, color=&quot;Country&quot;)+ theme_minimal()+ facet_wrap(~country, scales = &quot;free_y&quot;)+ #allows free-scaling in the y-axis transition_reveal(year) anim_file5&lt;-&quot;timeseries.gif&quot; animate(ch5.5, duration = 10, fps = 15, renderer = gifski_renderer(anim_file5), preview = TRUE) knitr::include_graphics(anim_file5) 9.4.1 Saving in Quarto For Data Presentation, you need to save interactive visualizations so that you can place them in your presentations. # Create an animated plot p &lt;- ggplot(DATA, aes(MAPPINGS)) + geom_function + transition_states(gear, transition_length = 2, state_length = 1) # Save as GIF anim_save(&quot;animation.gif&quot;, p) # Save as MP4 anim_save(&quot;animation.mp4&quot;, p) 9.5 Maps 9.5.1 Where to get shapefiles? The rnaturalearth provides some shapefiles you can use. A suggested site to find shapefiles of different countries: https://gadm.org/index.html . You can also locate shapefiles from the government sites. We will make use of the packages like WDI, rnaturalearth,sf, gganimate, and leaflet. Do note however, that gganimate and leaflet does not work for PDF, thus, it can only be used for your Data Story Presentation. 9.6 Static Maps 9.6.1 Fetching GDP Data from WDI rm(list=ls()) gc() ## used (Mb) gc trigger (Mb) max used (Mb) ## Ncells 3571300 190.8 6548841 349.8 5173005 276.3 ## Vcells 7735268 59.1 14786712 112.9 11551640 88.2 ch5_2&lt;-WDI(country = &quot;all&quot;, indicator = &quot;NY.GDP.MKTP.CD&quot;, start = 2000, end = 2022, extra=TRUE) #clean the dataset ch5_2&lt;-ch5_2 %&gt;% rename(gdp=NY.GDP.MKTP.CD, iso_a3=iso2c) %&gt;% drop_na(gdp) #dropping missing values We are retrieving GDP data for all countries from 2000 to 2022 and we are also renaming columns to match with map data found in the rnaturalearth. We also remove missing GDP values ch5map&lt;-ne_countries(scale = &quot;medium&quot;, returnclass = &quot;sf&quot;) This fetches the world map with country boundaries in sf (simple features) format to visualize the GDP data. We fetch medium-scale country boundaries in spatial data format and the returnclass=\"sf\" ensures it can be used with ggplot2 #Merge wgdp&lt;-ch5map %&gt;% left_join(ch5_2, by=&quot;iso_a3&quot;) We need to match the GDP data with the corresponding country for visualization. You can opt to retrieve the world map to find out how to merge both. ggplot(data = wgdp) + geom_sf(aes(fill = gdp_md)) + scale_fill_viridis_c(option = &quot;plasma&quot;, trans = &quot;log&quot;, na.value = &quot;grey&quot;) + theme_minimal() + labs(title = &quot;GDP by Country&quot;, subtitle = &quot;Data from WDI&quot;, fill = &quot;GDP (log scale)&quot;) We use the merged map and GDP data and geom_sf(aes(fill=gdp_md)) filles countries based on GDP. The scale_fill_viridis_c uses “plasma” which is best used for maps. The log scale transformation is used to better differentiate large economies and small economies and grey fill for missing data. 9.6.1.1 Animated World Map for GDP Now we create an animated version of the map. You need to check that your time variable does not have any NA. It would be better to choose the years that are available consecutively so we will filter the data to only include from 2009-2019. unique(wgdp$gdp_year) ## [1] 2019 2014 2018 2017 2016 2013 2010 2009 2012 2006 2015 ## [12] 2003 2007 2011 # Filter for years 2009-2019 wgdp_filtered &lt;- wgdp %&gt;% filter(gdp_year %in% 2009:2019) ch5.6 &lt;- ggplot(data = wgdp_filtered) + geom_sf(aes(fill = gdp_md)) + scale_fill_viridis_c(option = &quot;plasma&quot;, trans = &quot;log&quot;, na.value = &quot;grey50&quot;) + theme_minimal() + labs(title = &quot;World GDP by Country: {closest_state}&quot;, subtitle = &quot;Data from World Development Indicators&quot;, fill = &quot;GDP (log scale)&quot;) + transition_states(gdp_year) + ease_aes(&#39;linear&#39;) anim_file6&lt;-&quot;worldgdp.gif&quot; animate(ch5.6, duration = 10, fps = 15, renderer = gifski_renderer(anim_file6), preview = TRUE) knitr::include_graphics(anim_file6) 9.7 Philippine Regional Map We will use Ch6.xlsx containing Regional GDP and use the Philippine Regions shapefile. rm(list=ls()) gc() ## used (Mb) gc trigger (Mb) max used (Mb) ## Ncells 3642159 194.6 6548841 349.8 6009573 321.0 ## Vcells 8148717 62.2 14786712 112.9 14770706 112.7 library(sf) layers&lt;-st_layers(&quot;PH_Adm1_Regions.shp&quot;) print(layers) ## Driver: ESRI Shapefile ## Available layers: ## layer_name geometry_type features fields ## 1 PH_Adm1_Regions.shp Polygon 17 7 ## 2 Regions.shp Polygon 17 19 ## crs_name ## 1 WGS 84 / UTM zone 51N ## 2 WGS 84 sp_df = sf::st_read( dsn= &#39;PH_Adm1_Regions.shp&#39;, layer=&quot;Regions.shp&quot;) ## Reading layer `Regions.shp&#39; from data source ## `C:\\Users\\jemma\\OneDrive\\Documents\\DLSU\\LBOMETR_Book\\PH_Adm1_Regions.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 17 features and 19 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 114.2779 ymin: 4.587294 xmax: 126.605 ymax: 21.12189 ## Geodetic CRS: WGS 84 The code loads the sf package then st_layers checks all available layers in the shapefile because some shapefiles contains multiple layers like administrative boundaries, water bodies, etc. so we display the available layers in the console. The st_read(dsn \"PH_Adm1_Regions.shp\") specifies the data source and the layer = \"Regions.shp\" specifies the layer name to read (the one we will use and need). Let us check if the shapefile was loaded correctly. print(sp_df) #check structure ## Simple feature collection with 17 features and 19 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 114.2779 ymin: 4.587294 xmax: 126.605 ymax: 21.12189 ## Geodetic CRS: WGS 84 ## First 10 features: ## psgc_code name corr_code ## 1 100000000 Region I (Ilocos Region) 10000000 ## 2 200000000 Region II (Cagayan Valley) 20000000 ## 3 300000000 Region III (Central Luzon) 30000000 ## 4 400000000 Region IV-A (CALABARZON) 40000000 ## 5 500000000 Region V (Bicol Region) 50000000 ## 6 600000000 Region VI (Western Visayas) 60000000 ## 7 700000000 Region VII (Central Visayas) 70000000 ## 8 800000000 Region VIII (Eastern Visayas) 80000000 ## 9 900000000 Region IX (Zamboanga Peninsula) 90000000 ## 10 1000000000 Region X (Northern Mindanao) 100000000 ## geo_level city_class inc_class urb_rur pop_2015 pop_2020 ## 1 Reg &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 Reg &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 Reg &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 Reg &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 Reg &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 Reg &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 7 Reg &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 8 Reg &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 9 Reg &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 10 Reg &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## status adm1_pcode adm1_en ## 1 &lt;NA&gt; PH01 Region I (Ilocos Region) ## 2 &lt;NA&gt; PH02 Region II (Cagayan Valley) ## 3 &lt;NA&gt; PH03 Region III (Central Luzon) ## 4 &lt;NA&gt; PH04 Region IV-A (Calabarzon) ## 5 &lt;NA&gt; PH05 Region V (Bicol Region) ## 6 &lt;NA&gt; PH06 Region VI (Western Visayas) ## 7 &lt;NA&gt; PH07 Region VII (Central Visayas) ## 8 &lt;NA&gt; PH08 Region VIII (Eastern Visayas) ## 9 &lt;NA&gt; PH09 Region IX (Zamboanga Peninsula) ## 10 &lt;NA&gt; PH10 Region X (Northern Mindanao) ## adm1_alt adm0_pcode adm0_en ## 1 Ilocos Region PH Philippines (the) ## 2 Cagayan Valley PH Philippines (the) ## 3 Central Luzon PH Philippines (the) ## 4 Calabarzon PH Philippines (the) ## 5 Bicol Region PH Philippines (the) ## 6 Western Visayas PH Philippines (the) ## 7 Central Visayas PH Philippines (the) ## 8 Eastern Visayas PH Philippines (the) ## 9 Zamboanga Peninsula PH Philippines (the) ## 10 Northern Mindanao PH Philippines (the) ## date shape_len shape_area shape_sqkm ## 1 2022-11-09 14.99505 1.043983 12307.35 ## 2 2022-11-09 19.13905 2.241812 26387.73 ## 3 2022-11-09 15.94956 1.793513 21304.16 ## 4 2022-11-09 27.62549 1.326720 15846.63 ## 5 2022-11-09 44.92324 1.446324 17338.38 ## 6 2022-11-09 27.77415 1.657591 20047.63 ## 7 2022-11-09 29.11311 1.178431 14293.66 ## 8 2022-11-09 42.02116 1.726804 20835.68 ## 9 2022-11-09 23.18144 1.196677 14596.05 ## 10 2022-11-09 15.00295 1.435115 17489.29 ## geometry ## 1 MULTIPOLYGON (((120.9714 18... ## 2 MULTIPOLYGON (((121.9488 21... ## 3 MULTIPOLYGON (((122.2342 16... ## 4 MULTIPOLYGON (((122.3079 14... ## 5 MULTIPOLYGON (((122.9882 11... ## 6 MULTIPOLYGON (((121.4341 12... ## 7 MULTIPOLYGON (((124.093 11.... ## 8 MULTIPOLYGON (((124.3678 12... ## 9 MULTIPOLYGON (((123.4129 8.... ## 10 MULTIPOLYGON (((124.6987 9.... We then check available attributes and also check the region names so that we can merge the shapefile with our GDP data. colnames(sp_df) ## [1] &quot;psgc_code&quot; &quot;name&quot; &quot;corr_code&quot; &quot;geo_level&quot; ## [5] &quot;city_class&quot; &quot;inc_class&quot; &quot;urb_rur&quot; &quot;pop_2015&quot; ## [9] &quot;pop_2020&quot; &quot;status&quot; &quot;adm1_pcode&quot; &quot;adm1_en&quot; ## [13] &quot;adm1_alt&quot; &quot;adm0_pcode&quot; &quot;adm0_en&quot; &quot;date&quot; ## [17] &quot;shape_len&quot; &quot;shape_area&quot; &quot;shape_sqkm&quot; &quot;geometry&quot; head(sp_df) ## Simple feature collection with 6 features and 19 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 119.7497 ymin: 9.444193 xmax: 124.4249 ymax: 21.12189 ## Geodetic CRS: WGS 84 ## psgc_code name corr_code geo_level ## 1 100000000 Region I (Ilocos Region) 10000000 Reg ## 2 200000000 Region II (Cagayan Valley) 20000000 Reg ## 3 300000000 Region III (Central Luzon) 30000000 Reg ## 4 400000000 Region IV-A (CALABARZON) 40000000 Reg ## 5 500000000 Region V (Bicol Region) 50000000 Reg ## 6 600000000 Region VI (Western Visayas) 60000000 Reg ## city_class inc_class urb_rur pop_2015 pop_2020 status ## 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## adm1_pcode adm1_en adm1_alt ## 1 PH01 Region I (Ilocos Region) Ilocos Region ## 2 PH02 Region II (Cagayan Valley) Cagayan Valley ## 3 PH03 Region III (Central Luzon) Central Luzon ## 4 PH04 Region IV-A (Calabarzon) Calabarzon ## 5 PH05 Region V (Bicol Region) Bicol Region ## 6 PH06 Region VI (Western Visayas) Western Visayas ## adm0_pcode adm0_en date shape_len ## 1 PH Philippines (the) 2022-11-09 14.99505 ## 2 PH Philippines (the) 2022-11-09 19.13905 ## 3 PH Philippines (the) 2022-11-09 15.94956 ## 4 PH Philippines (the) 2022-11-09 27.62549 ## 5 PH Philippines (the) 2022-11-09 44.92324 ## 6 PH Philippines (the) 2022-11-09 27.77415 ## shape_area shape_sqkm geometry ## 1 1.043983 12307.35 MULTIPOLYGON (((120.9714 18... ## 2 2.241812 26387.73 MULTIPOLYGON (((121.9488 21... ## 3 1.793513 21304.16 MULTIPOLYGON (((122.2342 16... ## 4 1.326720 15846.63 MULTIPOLYGON (((122.3079 14... ## 5 1.446324 17338.38 MULTIPOLYGON (((122.9882 11... ## 6 1.657591 20047.63 MULTIPOLYGON (((121.4341 12... library(dplyr) unique(sp_df$name) ## [1] &quot;Region I (Ilocos Region)&quot; ## [2] &quot;Region II (Cagayan Valley)&quot; ## [3] &quot;Region III (Central Luzon)&quot; ## [4] &quot;Region IV-A (CALABARZON)&quot; ## [5] &quot;Region V (Bicol Region)&quot; ## [6] &quot;Region VI (Western Visayas)&quot; ## [7] &quot;Region VII (Central Visayas)&quot; ## [8] &quot;Region VIII (Eastern Visayas)&quot; ## [9] &quot;Region IX (Zamboanga Peninsula)&quot; ## [10] &quot;Region X (Northern Mindanao)&quot; ## [11] &quot;Region XI (Davao Region)&quot; ## [12] &quot;Region XII (SOCCSKSARGEN)&quot; ## [13] &quot;National Capital Region (NCR)&quot; ## [14] &quot;Cordillera Administrative Region (CAR)&quot; ## [15] &quot;Region XIII (Caraga)&quot; ## [16] &quot;MIMAROPA Region&quot; ## [17] &quot;Bangsamoro Autonomous Region In Muslim Mindanao (BARMM)&quot; sp_df &lt;- sp_df %&gt;% rename(region = name) Now, we load our Regional GDP package; library(readxl) rgdp&lt;-read_excel(&quot;Ch5PracticeB.xlsx&quot;, sheet = &quot;Sheet1&quot;) ## New names: ## • `` -&gt; `...1` head(rgdp) ## # A tibble: 6 × 26 ## ...1 region `2000` `2001` `2002` `2003` `2004` `2005` ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 NCR National C… 2.42e9 2.48e9 2.51e9 2.62e9 2.84e9 2.99e9 ## 2 CAR Cordillera… 1.44e8 1.49e8 1.56e8 1.64e8 1.73e8 1.76e8 ## 3 I Ilocos Reg… 2.40e8 2.45e8 2.53e8 2.64e8 2.78e8 2.91e8 ## 4 II Cagayan Va… 1.58e8 1.64e8 1.66e8 1.71e8 1.86e8 1.83e8 ## 5 III Central Lu… 7.07e8 7.49e8 7.92e8 8.26e8 8.52e8 8.86e8 ## 6 IVA CALABARZON 1.05e9 1.07e9 1.12e9 1.18e9 1.24e9 1.31e9 ## # ℹ 18 more variables: `2006` &lt;dbl&gt;, `2007` &lt;dbl&gt;, ## # `2008` &lt;dbl&gt;, `2009` &lt;dbl&gt;, `2010` &lt;dbl&gt;, `2011` &lt;dbl&gt;, ## # `2012` &lt;dbl&gt;, `2013` &lt;dbl&gt;, `2014` &lt;dbl&gt;, `2015` &lt;dbl&gt;, ## # `2016` &lt;dbl&gt;, `2017` &lt;dbl&gt;, `2018` &lt;dbl&gt;, `2019` &lt;dbl&gt;, ## # `2020` &lt;dbl&gt;, `2021` &lt;dbl&gt;, `2022` &lt;dbl&gt;, `2023` &lt;dbl&gt; We need to convert the columns to have the same format first so that we can edit all of them later on when we transform to long format since different formats cannot be combined when modifying to long format. colnames(rgdp)&lt;-as.character(colnames(rgdp)) rgdp&lt;-rgdp %&gt;% select(-&quot;...1&quot;) Now, we can modify library(tidyverse) rgdpl&lt;-rgdp %&gt;% pivot_longer(cols = -region, names_to = &quot;Year&quot;, values_to = &quot;GDP&quot;) %&gt;% mutate(Year=as.numeric(Year), GDP=as.numeric(GDP)) head(rgdpl) ## # A tibble: 6 × 3 ## region Year GDP ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 National Capital Region 2000 2416391870. ## 2 National Capital Region 2001 2483504980. ## 3 National Capital Region 2002 2507171644. ## 4 National Capital Region 2003 2624052475. ## 5 National Capital Region 2004 2841837836. ## 6 National Capital Region 2005 2988546218. unique(rgdpl$region) ## [1] &quot;National Capital Region&quot; ## [2] &quot;Cordillera Administrative Region&quot; ## [3] &quot;Ilocos Region&quot; ## [4] &quot;Cagayan Valley&quot; ## [5] &quot;Central Luzon&quot; ## [6] &quot;CALABARZON&quot; ## [7] &quot;MIMAROPA Region&quot; ## [8] &quot;Bicol Region&quot; ## [9] &quot;Western Visayas&quot; ## [10] &quot;Central Visayas&quot; ## [11] &quot;Eastern Visayas&quot; ## [12] &quot;Zamboanga Peninsula&quot; ## [13] &quot;Northern Mindanao&quot; ## [14] &quot;Davao Region&quot; ## [15] &quot;SOCCSKSARGEN&quot; ## [16] &quot;Caraga&quot; ## [17] &quot;BARMM&quot; Since we know that the names are different in the shapefile, we rename the regions so that they match through mapping. region_mapping &lt;- c( &quot;Ilocos Region&quot; = &quot;Region I (Ilocos Region)&quot;, &quot;Cagayan Valley&quot; = &quot;Region II (Cagayan Valley)&quot;, &quot;Central Luzon&quot; = &quot;Region III (Central Luzon)&quot;, &quot;CALABARZON&quot; = &quot;Region IV-A (CALABARZON)&quot;, &quot;Bicol Region&quot; = &quot;Region V (Bicol Region)&quot;, &quot;Western Visayas&quot; = &quot;Region VI (Western Visayas)&quot;, &quot;Central Visayas&quot; = &quot;Region VII (Central Visayas)&quot;, &quot;Eastern Visayas&quot; = &quot;Region VIII (Eastern Visayas)&quot;, &quot;Zamboanga Peninsula&quot; = &quot;Region IX (Zamboanga Peninsula)&quot;, &quot;Northern Mindanao&quot; = &quot;Region X (Northern Mindanao)&quot;, &quot;Davao Region&quot; = &quot;Region XI (Davao Region)&quot;, &quot;SOCCSKSARGEN&quot; = &quot;Region XII (SOCCSKSARGEN)&quot;, &quot;National Capital Region&quot; = &quot;National Capital Region (NCR)&quot;, &quot;Cordillera Administrative Region&quot; = &quot;Cordillera Administrative Region (CAR)&quot;, &quot;Caraga&quot; = &quot;Region XIII (Caraga)&quot;, &quot;BARMM&quot; = &quot;Bangsamoro Autonomous Region In Muslim Mindanao (BARMM)&quot;, &quot;MIMAROPA Region&quot; = &quot;MIMAROPA Region&quot; ) rgdpl$region &lt;- dplyr::recode(rgdpl$region, !!!region_mapping) setdiff(sp_df$region, rgdpl$region) # Should be (0) ## character(0) Now, we can merge. rgdp_map&lt;-sp_df %&gt;% left_join(rgdpl, by = &quot;region&quot;) setdiff(rgdpl$region, sp_df$region) ## character(0) #remove unnecessary columns and mutate GDP to billions rgdp_map &lt;- rgdp_map %&gt;% select(region, Year, GDP, geometry) %&gt;% mutate(GDP = GDP / 1e9) 9.7.0.1 Choropleth Map A Choropleth Map is a type of thematic map where regions are color-coded based on a statistical value (e.g., GDP, population, unemployment rate, etc.). It is commonly used in geographic data visualization to show spatial variations across different regions. For the static map, let us choose the latest year, 2023; rgdp2023&lt;-rgdp_map %&gt;% filter(Year==2023) library(scales) ## ## Attaching package: &#39;scales&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## discard ## The following object is masked from &#39;package:readr&#39;: ## ## col_factor # Modify the Choropleth Map ggplot(rgdp2023) + geom_sf(aes(fill = GDP), color = &quot;white&quot;) + # Fill regions based on GDP scale_fill_viridis_c( option = &quot;magma&quot;, name = &quot;GDP (2023)&quot;, labels = label_number(accuracy = 0.1, suffix = &quot;B&quot;, big.mark = &quot;,&quot;) # Fix rounding issue ) + labs(title = &quot;Regional GDP of the Philippines (2023)&quot;) + theme_minimal() #Saving the images #ggsave(&quot;rgdp2023.jpg&quot;, plot = last_plot(), width = 10, height = 8, dpi = 300) #or plot = nameofplot, width and height are in inches and for high quality image, dpi = 300 9.7.0.2 Bubble Map A Bubble Map is a type of thematic map that represents data values using circles (bubbles) placed over specific locations. The size of the bubble corresponds to the magnitude of the variable being visualized (e.g., GDP, population, sales, etc.). We use 2023 again. The first step that we need to do is calculate the centroids using st_centroid from the sf package because this calculates the geometric center of each polygon region in the shapefile. These will be used as our bubble locations and to ensure that there will be no overlapping with borders. phr_centroids&lt;-st_centroid(rgdp2023) ## Warning: st_centroid assumes attributes are constant over ## geometries print(phr_centroids) ## Simple feature collection with 17 features and 3 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: 119.8902 ymin: 6.580654 xmax: 125.8263 ymax: 17.35385 ## Geodetic CRS: WGS 84 ## First 10 features: ## region Year GDP ## 1 Region I (Ilocos Region) 2023 0.7014687 ## 2 Region II (Cagayan Valley) 2023 0.4470732 ## 3 Region III (Central Luzon) 2023 2.3184283 ## 4 Region IV-A (CALABARZON) 2023 3.0952557 ## 5 Region V (Bicol Region) 2023 0.6103399 ## 6 Region VI (Western Visayas) 2023 1.0242699 ## 7 Region VII (Central Visayas) 2023 1.3811721 ## 8 Region VIII (Eastern Visayas) 2023 0.5235611 ## 9 Region IX (Zamboanga Peninsula) 2023 0.4473210 ## 10 Region X (Northern Mindanao) 2023 0.9847503 ## geometry ## 1 POINT (120.4827 16.90283) ## 2 POINT (121.7316 17.20396) ## 3 POINT (120.8223 15.3919) ## 4 POINT (121.568 14.16225) ## 5 POINT (123.4734 13.27216) ## 6 POINT (122.652 10.84319) ## 7 POINT (123.6147 9.921139) ## 8 POINT (124.9534 11.53574) ## 9 POINT (122.8477 7.825902) ## 10 POINT (124.6742 8.187355) We now create our Bubble Map ggplot()+ geom_sf(data=rgdp2023, fill = &quot;gray&quot;, color=&quot;white&quot;)+#base map geom_point(data = phr_centroids, aes(x = st_coordinates(geometry)[,1], y = st_coordinates(geometry)[,2], size = GDP, color = GDP), alpha = 0.7) + scale_size(range = c(2, 15), name = &quot;GDP (2023)&quot;) + # Adjust bubble size scale_color_viridis_c(option = &quot;magma&quot;, name = &quot;GDP (2023)&quot;, labels = label_number(accuracy = 0.1, suffix = &quot;B&quot;, big.mark = &quot;,&quot;)) + labs(title = &quot;Regional GDP of the Philippines (2023)&quot;, subtitle = &quot;Bubble Size Represents GDP&quot;, x=&quot;Coordinates&quot;, y=&quot;Coordinates&quot;)+ theme_minimal() As you can see, the base map is drawn using geom_sf() then the bubbles represent the GDP values. The bubble size and color represent GDP magnitude. You will notice that ggplot() is empty, this creates an empty ggplot canvas where the map layers will be added using the geom_sf. In the aes there’s x = st_coordinates(geometry)[,1] same for y. These extract the longitude and latitude of each centroid. The scale_size() range sets the bubble sizes between 2 and 15 units but you can edit this. Now, scale_viridis_c has more things, the name sets the legend title for GDP colors. the labels ensures decimal precision and adds suffix B, plus, adds comma separators since we do not want the legend to contain scientific notations. 9.8 Animated Map 9.8.0.1 Animated Philippine Regional GDP Choropleth Map We will now make an animated version of the Regional GDP Choropleth Map since we have data from 2000 to 2023. Let’s convert to billions like what we did for the 2023. It takes really long to render (it depends on the memory you have in your laptops) so, I will place the best strategy, that is creating static maps and combining them to form a video. Notice that when cleaning the environment, I chose to keep rgdp_map since we will still use this. rm(list = setdiff(ls(), &quot;rgdp_map&quot;)) gc() ## used (Mb) gc trigger (Mb) max used (Mb) ## Ncells 3699371 197.6 6548841 349.8 6548841 349.8 ## Vcells 34287981 261.6 155411355 1185.7 202265969 1543.2 #for mp4 if (!require(av)) install.packages(&quot;av&quot;) ## Loading required package: av library(av) We need to create a directory within our working directory where the images will be saved. dir.create(&quot;ch_gdp_frames&quot;, showWarnings = FALSE) We now generate static maps; this requires us to create a function/loop. When we use the av, the default is a black setting so it’s better to have a white background set for the plot and the entire canvas. for(year in 2000:2023){ #filter data for the specific year data_year&lt;-rgdp_map %&gt;% filter(Year==year) #Generate the plot ip&lt;-ggplot(data_year) + geom_sf(aes(fill = GDP), color = &quot;white&quot;) + scale_fill_viridis_c( option = &quot;magma&quot;, name = &quot;GDP&quot;, labels = scales::label_number(accuracy = 0.1, suffix = &quot;B&quot;, big.mark = &quot;,&quot;) ) + labs( title = paste(&quot;Regional GDP of the Philippines&quot;), subtitle = paste(&quot;Year:&quot;, year), fill = &quot;GDP (Billion $)&quot; ) + theme_minimal() + theme( plot.title = element_text(size = 16, face = &quot;bold&quot;), plot.subtitle = element_text(size = 14), panel.background = element_rect(fill = &quot;white&quot;, color = NA), # Set white background plot.background = element_rect(fill = &quot;white&quot;, color = NA) # Set white background ) # Save the plot as an image ggsave(filename = paste0(&quot;ch_gdp_frames/gdp_&quot;, year, &quot;.png&quot;), plot = ip, width = 8, height = 6, dpi = 300) } You need to edit some parts, such as the 2000:2023 with whatever years you have; you need to filter data depending on how you called the time column. Then edit the fill and other things. We now combine our static maps into MP4. If you want the transition for each year to be seen clearly, the framerate is set at 1 to have 1 frame per second and the frame is repeated twice. the vfilter makes the video HD file_list &lt;- list.files(&quot;ch_gdp_frames&quot;, pattern = &quot;gdp_.*\\\\.png&quot;, full.names = TRUE) rep_files &lt;- rep(file_list, each = 2) # Repeat each frame twice av::av_encode_video( input = rep_files, output = &quot;ch_rgdp_time.mp4&quot;, framerate = 1, vfilter = &quot;scale=1280:720&quot; ) 9.9 Practical: Advanced Visualizations For the practical, you need to search and clean the dataset on your own. Please search in PSA OpenStat: Number of Registered Live Births by Sex and by Usual Residence of Mother (Region, Province and Highly Urbanized City), Philippines: January - December 2013-2022. Only gather regional data since the shapefile that is given for this practical is for Regions only. Please save each animation using anim_save Using PSA OpenStat (2013-2022) data, create an animated bar chart showing the total number of live births per year. Differentiate Male vs. Female births using fill Create an animated bar chart that compares the number of live births per region over time (2013-2022). Use transition_states(year, wrap = FALSE) to animate regional birth counts changing over time. Determine the region with the highest live births in each frame. Select 3-5 regions and visualize their live birth trends (2013-2022) using an animated time series plot. Using PSA OpenStat (2013-2022) data and a Philippines regional shapefile, create a static choropleth map of the recent number of live births per region. Color the regions based on birth count Using PSA OpenStat (2013-2022) data and a Philippines regional shapefile, create an animated choropleth map showing how the number of live births per region changes over time. Color the regions based on birth count How do birth trends differ across Philippine regions? Which regions have experienced the highest increase or decrease in live births from 2013 to 2022? How does using gganimate with sf enhance your ability to analyze spatial(and time) data trends? "],["descriptive-statistics.html", "10 Descriptive Statistics 10.1 Preliminaries 10.2 Frequency Table 10.3 Histogram 10.4 Summary Table 10.5 Control Variables 10.6 Confounding Variables 10.7 Difference between Control Variables and Confounding Variables 10.8 Descriptive Statistics of Control and Confounding Variables 10.9 Descriptive Statistics of Categorical/Character Variables 10.10 Importance of Descriptive Statistics: 10.11 Closing", " 10 Descriptive Statistics For this section, we will make use of Ch6PRactice.RData. Since the file is in RData format, we will only need to load it in R. 10.1 Preliminaries 10.1.1 Install Packages We install and load the psych , stargazer, kableExtra and gtsummary packages and load tidyverse package. if(!(&quot;psych&quot; %in% installed.packages()[,&quot;Package&quot;])) install.packages(&quot;psych&quot;) if(!(&quot;gtsummary&quot; %in% installed.packages()[,&quot;Package&quot;])) install.packages(&quot;gtsummary&quot;) if(!(&quot;stargazer&quot; %in% installed.packages()[,&quot;Package&quot;])) install.packages(&quot;stargazer&quot;) if(!(&quot;kableExtra&quot; %in% installed.packages()[,&quot;Package&quot;])) install.packages(&quot;kableExtra&quot;) library(psych) ## ## Attaching package: &#39;psych&#39; ## The following objects are masked from &#39;package:scales&#39;: ## ## alpha, rescale ## The following object is masked from &#39;package:car&#39;: ## ## logit ## The following objects are masked from &#39;package:ggplot2&#39;: ## ## %+%, alpha library(tidyverse) library(gtsummary) library(stargazer) ## ## Please cite as: ## Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables. ## R package version 5.2.3. https://CRAN.R-project.org/package=stargazer library(kableExtra) ## ## Attaching package: &#39;kableExtra&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## group_rows 10.1.2 Load the dataset rm(list=ls()) gc() ## used (Mb) gc trigger (Mb) max used (Mb) ## Ncells 3733295 199.4 6548841 349.8 6548841 349.8 ## Vcells 34340189 262.0 124329084 948.6 202265969 1543.2 load(&quot;Ch6Practice.RData&quot;) We subset the data: # Create a subset of the dataset with only female respondents Dataset_CP_f &lt;- Dataset_CP %&gt;% filter(HH2a_Sex == &quot;Female&quot;) # Create a subset of the dataset with only male respondents Dataset_CP_m &lt;- Dataset_CP %&gt;% filter(HH2a_Sex == &quot;Male&quot;) str(Dataset_CP_m) ## tibble [1,723 × 49] (S3: tbl_df/tbl/data.frame) ## $ Country : Factor w/ 34 levels &quot;Austria&quot;,&quot;Belgium&quot;,..: 20 25 3 8 12 16 7 21 6 6 ... ## $ ID : chr [1:1723] &quot;NL5063519&quot; &quot;SI1042916&quot; &quot;BG1396625&quot; &quot;EE1261010&quot; ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Unique respondent ID&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;A60&quot; ## ..- attr(*, &quot;display_width&quot;)= int 9 ## $ HH1_Num_People : num [1:1723] 2 3 2 2 2 2 1 2 2 1 ... ## $ HH2a_Sex : Factor w/ 2 levels &quot;Male&quot;,&quot;Female&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ HH2b_Age : num [1:1723] 63 63 89 52 81 54 59 76 70 53 ... ## $ HH2d_EmploymentSituation : Factor w/ 10 levels &quot;At work as employee or employer/self-employed&quot;,..: 7 1 7 7 7 6 1 7 7 1 ... ## $ Q1_PaidJob : Factor w/ 4 levels &quot;Yes&quot;,&quot;No&quot;,&quot;Don&#39;t Know&quot;,..: 1 NA 1 1 1 1 NA 1 1 NA ... ## $ Q2_Empoyment : Factor w/ 6 levels &quot;Self-employed without employees&quot;,..: NA 3 NA NA NA NA 3 NA NA 3 ... ## $ Q3_Contract : Factor w/ 9 levels &quot;On an unlimited permanent contract&quot;,..: NA 1 NA NA NA NA 1 NA NA 1 ... ## $ Q4_Occupation : Factor w/ 13 levels &quot;Armed forces&quot;,..: NA 4 NA NA NA NA 6 NA NA 6 ... ## $ Q7_HoursWeekWork : num [1:1723] NA 35 NA NA NA NA 37 NA NA 25 ... ## $ Q7a_AdditionalJob : Factor w/ 4 levels &quot;Yes&quot;,&quot;No&quot;,&quot;Don&#39;t know&quot;,..: NA 2 NA NA NA NA 2 NA NA 2 ... ## $ Q7b_HoursWeekWeekAddJob : num [1:1723] NA NA NA NA NA NA NA NA NA NA ... ## $ Q7c_Work : Factor w/ 4 levels &quot;Yes&quot;,&quot;No&quot;,&quot;Don&#39;t Know&quot;,..: 2 NA 2 1 2 2 NA 2 2 NA ... ## $ Q8_HoursWeekWorkPref : num [1:1723] 0 30 0 40 0 20 30 NA 40 38 ... ## $ Q9_HoursWeekWorkPartner : num [1:1723] NA NA NA 41 NA 30 NA NA NA NA ... ## $ Q10_HoursWeekWorkPartnerPref: num [1:1723] 0 30 0 30 0 40 NA NA 35 NA ... ## $ Q17_Rooms : num [1:1723] 6 3 1 NA 3 4 2 3 3 1 ... ## $ Q18_Tenancy : Factor w/ 8 levels &quot;Own without mortgage&quot;,..: 2 3 1 1 5 1 1 1 4 4 ... ## $ Q19a_ShortageSpace : num [1:1723] 0 0 0 0 0 0 1 0 0 1 ... ## $ Q19b_Rot : num [1:1723] 0 0 0 0 0 0 0 1 0 0 ... ## $ Q19c_Leaks : num [1:1723] 0 0 0 0 0 1 0 1 0 0 ... ## $ Q19d_NoFlusingToilet : num [1:1723] 0 0 0 0 0 0 0 0 0 0 ... ## $ Q19e_NoBathShower : num [1:1723] 0 0 0 0 0 0 0 0 0 0 ... ## $ Q19f_NoOutside : num [1:1723] 0 0 0 0 0 1 0 0 0 0 ... ## $ Q20_LeaveAccomodation_NoAff : Factor w/ 6 levels &quot;Very likely&quot;,..: 4 3 4 4 4 4 4 4 4 4 ... ## $ Q24_Trust : Factor w/ 12 levels &quot;1 - you can&#39;t be too careful&quot;,..: 8 6 1 9 5 10 6 2 3 4 ... ## $ Q25a_TensionClass : Factor w/ 5 levels &quot;A lot of tension&quot;,..: 2 2 2 2 1 1 2 1 3 4 ... ## $ Q25b_TensionWork : Factor w/ 5 levels &quot;A lot of tension&quot;,..: 2 2 4 2 1 1 2 1 3 3 ... ## $ Q25c_TensionSex : Factor w/ 5 levels &quot;A lot of tension&quot;,..: 3 3 2 3 1 1 3 2 2 2 ... ## $ Q25d_TensionsAge : hvn_lbl_ [1:1723] 3, 3, 2, 98, 1, 1, 2, 2, ... ## ..@ label : chr &quot;Q25d Old people and young people / How much tension is there in this country?&quot; ## ..@ na_values : num [1:2] 98 99 ## ..@ format.spss: chr &quot;F8.0&quot; ## ..@ labels : Named num [1:5] 1 2 3 98 99 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;A lot of tension&quot; &quot;Some tension&quot; &quot;No tension&quot; &quot;(Don’t know)&quot; ... ## $ Q25e_TensionRace : Factor w/ 5 levels &quot;A lot of tension&quot;,..: 1 2 4 2 1 1 2 2 2 1 ... ## $ Q25f_TensionReligion : Factor w/ 5 levels &quot;A lot of tension&quot;,..: 1 2 4 3 2 1 2 2 2 1 ... ## $ Q25g_TensionSexOrient : Factor w/ 5 levels &quot;A lot of tension&quot;,..: 2 3 4 1 1 1 2 3 3 4 ... ## $ Q37a_HoursWeekChildren : num [1:1723] NA 5 NA 4 NA NA NA NA NA NA ... ## $ Q37b_HoursWeekHousework : num [1:1723] 14 NA NA 4 6 NA 7 12 3 5 ... ## $ Q37c_HoursWeekElderly : num [1:1723] 3 NA NA NA NA NA NA NA NA NA ... ## $ Q48_Education : Factor w/ 472 levels &quot;1010&quot;,&quot;1011&quot;,..: 306 361 44 111 174 226 91 312 78 76 ... ## $ Q49_Area : Factor w/ 6 levels &quot;The open countryside&quot;,..: 3 3 4 2 2 2 4 2 2 4 ... ## $ Q50a_NeighbourhoodNoise : Factor w/ 5 levels &quot;Major problems&quot;,..: 3 3 3 3 3 3 3 3 3 2 ... ## $ Q50b_NeighbourhoodAir : Factor w/ 5 levels &quot;Major problems&quot;,..: 3 3 3 3 3 3 3 3 2 4 ... ## $ Q50c_NeighbourhoodWater : Factor w/ 5 levels &quot;Major problems&quot;,..: 3 3 3 2 3 1 3 3 3 4 ... ## $ Q50d_NeighbourhoodCrime : Factor w/ 5 levels &quot;Major problems&quot;,..: 3 3 2 3 3 3 2 3 3 1 ... ## $ Q50e_NeighbourhoodLitter : Factor w/ 5 levels &quot;Major problems&quot;,..: 3 3 2 2 3 3 3 3 3 1 ... ## $ Q50f_NeighbourhoodTraffic : Factor w/ 5 levels &quot;Major problems&quot;,..: 3 2 4 3 3 3 3 3 3 4 ... ## $ Q51a_AccServicesPost : Factor w/ 7 levels &quot;With great difficulty&quot;,..: 3 3 5 3 3 1 2 2 3 4 ... ## $ Q51b_AccServicesBank : Factor w/ 7 levels &quot;With great difficulty&quot;,..: 3 3 5 3 3 1 4 5 3 4 ... ## $ Q53c_QualityPublicTransport : Factor w/ 12 levels &quot;1 - very poor quality&quot;,..: 7 6 3 8 5 2 4 11 8 7 ... ## $ Income_PPP : num [1:1723] 3191 4172 478 426 887 ... Do Descriptive Statistics for Female Respondents. We compare. 10.2 Frequency Table Run a Frequency Table for the number of hours the respondents spend caring for children and/or grandchildren per week (variable Q37a_HoursWeekChildren). For both sex separately, report the mean and standard deviation. 10.2.1 Male Calculate the descriptive statistics for the variable Q37a_HoursWeekChildren, using the function describe of the package psych: psych::describe(Dataset_CP_m$Q37a_HoursWeekChildren) ## vars n mean sd median trimmed mad min max range ## X1 1 575 16.86 15.53 10 14.36 10.38 1 100 99 ## skew kurtosis se ## X1 1.83 4.65 0.65 Notice how we directly use the describe function on the variable Q37a_HoursWeekChildren (called with the dollar sign operator, $, which works as follows: name_dataset$name_variable - notice that to access a specific variable, some functions can use the pipe operator while others require the dollar sign operator, as in this case). Notice also how we call the function describe: we use the :: operator, which tells R to look for a function within a specific package as follows name_package::name_function You could also simply call the function by only using its name, R will then search for that specific function across all the loaded R packages. However, if more packages have functions with the same name, specifying the name of the package with the :: operator helps assuring the usage of the correct function. There are 575 non-missing observations for Q37a_HoursWeekChildren. psych::describe ignores the missing observations. Report the Frequency Table for the variable Q37a_HoursWeekChildren, using the function tbl_summary of the package gtsummary: Dataset_CP_m %&gt;% select(Q37a_HoursWeekChildren) %&gt;% tbl_summary( statistic = list(all_categorical() ~ &quot;{n} ({p}%)&quot;), # Show count &amp; percentage missing = &quot;no&quot; # Exclude missing values (set to &quot;ifany&quot; to include) ) %&gt;% modify_header(label = &quot;**Hours per Week with Children**&quot;) # Customize header #wjyjwvyeul table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #wjyjwvyeul thead, #wjyjwvyeul tbody, #wjyjwvyeul tfoot, #wjyjwvyeul tr, #wjyjwvyeul td, #wjyjwvyeul th { border-style: none; } #wjyjwvyeul p { margin: 0; padding: 0; } #wjyjwvyeul .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #wjyjwvyeul .gt_caption { padding-top: 4px; padding-bottom: 4px; } #wjyjwvyeul .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #wjyjwvyeul .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #wjyjwvyeul .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #wjyjwvyeul .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #wjyjwvyeul .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #wjyjwvyeul .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #wjyjwvyeul .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #wjyjwvyeul .gt_column_spanner_outer:first-child { padding-left: 0; } #wjyjwvyeul .gt_column_spanner_outer:last-child { padding-right: 0; } #wjyjwvyeul .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #wjyjwvyeul .gt_spanner_row { border-bottom-style: hidden; } #wjyjwvyeul .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #wjyjwvyeul .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #wjyjwvyeul .gt_from_md > :first-child { margin-top: 0; } #wjyjwvyeul .gt_from_md > :last-child { margin-bottom: 0; } #wjyjwvyeul .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #wjyjwvyeul .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #wjyjwvyeul .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #wjyjwvyeul .gt_row_group_first td { border-top-width: 2px; } #wjyjwvyeul .gt_row_group_first th { border-top-width: 2px; } #wjyjwvyeul .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #wjyjwvyeul .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #wjyjwvyeul .gt_first_summary_row.thick { border-top-width: 2px; } #wjyjwvyeul .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #wjyjwvyeul .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #wjyjwvyeul .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #wjyjwvyeul .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #wjyjwvyeul .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #wjyjwvyeul .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #wjyjwvyeul .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #wjyjwvyeul .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #wjyjwvyeul .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #wjyjwvyeul .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #wjyjwvyeul .gt_left { text-align: left; } #wjyjwvyeul .gt_center { text-align: center; } #wjyjwvyeul .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #wjyjwvyeul .gt_font_normal { font-weight: normal; } #wjyjwvyeul .gt_font_bold { font-weight: bold; } #wjyjwvyeul .gt_font_italic { font-style: italic; } #wjyjwvyeul .gt_super { font-size: 65%; } #wjyjwvyeul .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #wjyjwvyeul .gt_asterisk { font-size: 100%; vertical-align: 0; } #wjyjwvyeul .gt_indent_1 { text-indent: 5px; } #wjyjwvyeul .gt_indent_2 { text-indent: 10px; } #wjyjwvyeul .gt_indent_3 { text-indent: 15px; } #wjyjwvyeul .gt_indent_4 { text-indent: 20px; } #wjyjwvyeul .gt_indent_5 { text-indent: 25px; } #wjyjwvyeul .katex-display { display: inline-flex !important; margin-bottom: 0.75em !important; } #wjyjwvyeul div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after { height: 0px !important; } Hours per Week with Children N = 1,7231 Q37a_HoursWeekChildren 10 (5, 24) 1 Median (Q1, Q3) The table shows that there are 1, 723 non-missing observations for males, this shows the entire sample, not just variable-specific number. You need to explain the different Ns from psych::describe and gtsummary The median value of hours per week with children is 10 hours. (5,24) represents the interquartile range. Q1 (5): The first quartile (25th percentile) and the third quartile (75th percentile) are 5 and 24 hours respectively. This simply means that 50% of the responses lie between 5 and 24 hours per week. When interpreting: “The median hours per week spent with children of males is 10 hours, with half of the respondents reporting values between 5 and 24 hours. The total sample size is 1,723.” Moreover, in R there are other ways to access specific elements within a dataset. Datasets are organized in rows (which represent our observations) and columns (which represent our variables). We can select a specific cell of a dataset as follows: name_dataset[row, column], where row is the number(s) of row(s) we are interested in and column is either the number or the name of the column we are interested in. For instance, if we want to select the value of the variable Q37a_HoursWeekChildren for the third observation, we can use the following command Dataset_CP[3,“Q37a_HoursWeekChildren”] If we do not specify a value for the rows (or columns), we select all the rows (or columns) of the specified columns (or rows). For instance, the following command selects all the observations for the variable Q37a_HoursWeekChildren Dataset_CP[ ,“Q37a_HoursWeekChildren”] 10.3 Histogram Plot the histogram for the variable Q37a_HoursWeekChildren ggplot(data = Dataset_CP_m, aes(x = Q37a_HoursWeekChildren)) + geom_histogram(bins = 50, color = &quot;darkred&quot;, fill = &quot;pink&quot;) + theme_minimal()+ labs(title = &quot;Histogram: Hours per week spent taking care of children&quot;, subtitle = &quot;Only male respondents&quot;)+ xlab(&quot;Hours per week spent taking care of children (Question 37a)&quot;) + ylab(&quot;Count&quot;) + theme(plot.title = element_text(color = &quot;darkred&quot;, size = 14, face = &quot;italic&quot;), plot.subtitle = element_text(color = &quot;darkred&quot;, size = 12, face = &quot;italic&quot;)) Depending on the data, different number of bins are more or less appropriate. In this case, we indicate that we want our data to be split in 50 intervals. You could indicate a different number or you could also leave the bins option out of the code and let R select it (R will give a warning (not error) about the number of bins which has been selected). Each type of graph has its own set of options that can be set, use the help function to learn more about them. Next, we call a pre-defined layout of the graph called theme_minimal. There are many pre-defined layouts and you are free to choose whichever you like the most, or even create your own layout (which is partially what we do with the last layer). In the fourth layer we use labs in order to define the title of the graph, we add a subtitle and we specify the name of the axes. There are more options that you can define within labs, use help(labs) to find out what these are. Finally, we edit the appearance of the title and subtitle using theme and introducing the elements plot.title = element_text() and plot.subtitle = element_text(), where we specify their color, size and face. With that, we are able to change the format of the title and subtitle, however, it is not always necessary. If you want to find out more options that you can specify within theme, you can use help(“theme”) or search in Google. 10.4 Summary Table To summarize, we can build a table showing the mean and the standard deviation for the variable Q37a_HoursWeekChildren for the two groups, separately. To do so, we use two features: First, in the menu bar of your Quarto document (on top), we click on “Table” and then select “Insert Table”. A new window will appear and ask us the number of rows and columns our table should include. Once the table has been created, we can fill in its cells with whatever we wish to present in the table. Second, we use inline code, which allows us to embed directly into the text the output of an R code (you can find more information about inline coding here). A chunk of code, as we have seen so far, is separated from the text of our Quarto document and does not allow us to build a readable text combining words with output from R codes which in one linear sentence. So, if we want to say: The number of male respondents in our dataset is equal to Dataset_CP_m %&gt;% summarize(n = n()) ## # A tibble: 1 × 1 ## n ## &lt;int&gt; ## 1 1723 We can clearly see that this is not ideal when using a chunk of code. To better integrate this into our text we can use inline code. To do so, in the menu bar of our Quarto document (on top), we click on “&lt;/&gt;”, then we type r and add a space and then write the line of code we would like to execute in order to include its output into the text. In our previous example, this could be done by writing as an inline code “r Dataset_CP_m %&gt;% summarize(n = n())”, as follows: The number of male respondents in our dataset is equal to 1723 This could be useful, for instance, if we wish to present some properties of the data without us manually copy-pasting the numbers from the console to the text. At the same time, we can use this to populate our table, as we indeed do below. Note: In a Quarto document, you can run the code which is included in a chunk of code with the green play button. This allows you to execute and visualize the output of that specific command. Unfortunately, inline codes do not have the same option. Inline codes will be executed only when the Quarto document is rendered. This means that you cannot directly see the result of inline codes before you render your Quarto document nor you can verify if that code is correct (and if it is not, the rendering will not work). To overcome this issue, you can copy-paste the code of your inline code into the Console and run it from there. This will allow you to verify if the code is correct and what is the output. We have created here a table summarizing some of the descriptive statistics for the male respondents in our dataset. Take a look at this table: Male Code N_total 1723 Dataset_CP_m %&gt;% summarize (n=n()) N_missing 1148 Dataset_CP_m%&gt;% summarize(n_na=sum(is.na(Q37a_HoursWeekChildren))) N_valid 575 Dataset_CP_m%&gt;%summarize(n_valid=sum(!is.na(Q37a_HoursWeekChildren))) Mean 16.8591304 Dataset_CP_m%&gt;%summarize(mean = mean(Q37a_HoursWeekChildren, na.rm=T)) Median 10 Dataset_CP_m%&gt;%summarize(median = median(Q37a_HoursWeekChildren, na.rm=T)) Std. Deviation 15.5294252 Dataset_CP_m%&gt;% summarize(sd=sd(Q37a_HoursWeekChildren, na.rm=T)) Notice how we use the pipe operator on the dataset to compute the descriptive statistics with the summarize function and the n function to calculate the total number of observations; the is.na function to calculate the number of observations with missing values; the ! operator combined with the is.na function to calculate the opposite of the number of observations with missing values, i.e., the number of observations with valid values; the mean function to calculate the mean, with the na.rm option set to TRUE to tell R that there are missing values and it should compute the mean without considering the missing values; the median function to calculate the median, with the na.rm option set to TRUE to tell R that there are missing values and it should compute the median without considering the missing values. the sd function to calculate the standard deviation, with the na.rm option set to TRUE to tell R that there are missing values and it should compute the standard deviation without considering the missing values. 10.4.1 Summary Table using Stargazer We can create a descriptive statistics table for respondents including mean, median, SD, min, and max: We do this by combining psych package with stargazer. We also need to create a data frame since stargazer can’t handle tibbles. out= is so that we can have the tables and we can copy-paste them. desc &lt;- psych::describe(Dataset_CP_m %&gt;% select(Q37a_HoursWeekChildren)) #We don&#39;t need all stats desc_table &lt;- desc %&gt;% select(mean, median, sd, min, max) %&gt;% as.data.frame() desc_table$Variable &lt;- rownames(desc_table) desc_table &lt;- desc_table[, c(&quot;mean&quot;, &quot;median&quot;, &quot;sd&quot;, &quot;min&quot;, &quot;max&quot;)] stargazer(desc_table, type = &quot;text&quot;, summary = FALSE, # Don&#39;t summarize again title = &quot;Descriptive Statistics: Selected Variable (Male Respondents)&quot;, out=&quot;Desc_Stats_HWC.txt&quot;) ## ## Descriptive Statistics: Selected Variable (Male Respondents) ## =================================================== ## mean median sd min max ## --------------------------------------------------- ## Q37a_HoursWeekChildren 16.859 10 15.529 1 100 ## --------------------------------------------------- The output would be as a text but you can also save the table to HTML by changing type = \"text\" to type = \"html\" 10.4.2 Summary Table with Grouping and Stargazer Say you want to see Q37a_HoursWeekChildren by PaidJob. We use describeBy command which is similar to describe() but it works when you want to group by factor. desc_grp&lt;-psych::describeBy(Dataset_CP_m$Q37a_HoursWeekChildren, group = Dataset_CP_m$Q1_PaidJob, mat = TRUE) # mat=TRUE gives a nice combined data frame which we use for stargazer df_grp &lt;- as.data.frame(desc_grp) stargazer(df_grp, type = &quot;text&quot;, summary = FALSE, title = &quot;Hours with Children by Paid Job Status&quot;, out=&quot;HWCbyPJS.txt&quot;) ## ## Hours with Children by Paid Job Status ## ==================================================================================================== ## item group1 vars n mean sd median trimmed mad min max range skew kurtosis se ## ---------------------------------------------------------------------------------------------------- ## X11 1 Yes 1 189 16.566 17.453 10 13.405 10.378 1 100 99 1.738 3.175 1.269 ## X12 2 No 1 14 10.714 10.845 6.500 9.500 6.672 1 35 34 1.063 -0.263 2.898 ## X13 3 Don&#39;t Know 1 0 Inf -Inf -Inf ## X14 4 Refusal 1 1 40 40 40 0 40 40 0 ## ---------------------------------------------------------------------------------------------------- 10.5 Control Variables 10.5.1 What is a Control Variable? A control variable is a variable that is included in an analysis to account for alternative explanations of a relationship between an explanatory variable and an outcome variable. Control variables are chosen based on theory and prior knowledge. Suppose we are interested in the relationship between: Sex Hours spent caring for children per week If other characteristics differ systematically between men and women, and those characteristics also affect caregiving time, then differences we observe may not be attributable to sex alone. Control variables helps us isolate the relationship of interest. There are potential control variables we can use but, for now, we use HH1_Num_People due to care tasks may be shared in larger households. 10.6 Confounding Variables 10.6.1 What is a Confounding Variable? A confounder is a variable that is associated with both: The explanatory variable and The outcome variable When a confounder is omitted, the estimated relationship between the explanatory variable and the outcome may be biased or misleading. In our example, Explanatory Variable: Sex Outcome variable: Hours caring for children Confounder: Hours worked per week (HoursWeekWork) We consider HoursWeekWork as a confounder due to meeting both criteria for confounding: Men and women differ, on average, in hours worked per week. Individuals who work more hours typically have less time available for childcare. As a result, observed differences in caregiving time between men and women may partly reflect differences in labor supply rather than caregiving preferences or norms. Descriptive statistics allow us to detect potential confounding. 10.7 Difference between Control Variables and Confounding Variables Type Variable What it does / Why we care Interpretation Example Control variable HH1_Num_People Extra factor that only affects the outcome. We include it to be fair. We check caregiving hours while making sure household size isn’t confusing the result. Confounding variable Q7_HoursWeekWork A factor that affects both the explanatory and the outcome. Men and women might work different hours, and the more hours someone works, the less time they have for caregiving. 10.8 Descriptive Statistics of Control and Confounding Variables For this portion, I will show you the Descriptive Statistics of our outcome, explanatory, control and confounding variables: a. Create our data frame #Start with males df_m &lt;- Dataset_CP_m %&gt;% select(Q37a_HoursWeekChildren, HH1_Num_People, Q7_HoursWeekWork) %&gt;% na.omit() %&gt;% as.data.frame() # Females df_f &lt;- Dataset_CP_f %&gt;% select(Q37a_HoursWeekChildren, HH1_Num_People, Q7_HoursWeekWork) %&gt;% na.omit() %&gt;% as.data.frame() b. use psych::describe dsc_male &lt;- psych::describe(df_m) dsc_female &lt;- psych::describe(df_f) c. Combine both descriptive statistics results from psych::describe both_dsc &lt;- data.frame( Variable = rownames(dsc_male)[1:3], Mean_Male = dsc_male$mean[1:3], Mean_Female = dsc_female$mean[1:3], Median_Male = dsc_male$median[1:3], Median_Female = dsc_female$median[1:3], SD_Male = dsc_male$sd[1:3], SD_Female = dsc_female$sd[1:3], Min_Male = dsc_male$min[1:3], Min_Female = dsc_female$min[1:3], Max_Male = dsc_male$max[1:3], Max_Female = dsc_female$max[1:3] ) We create a data frame and get the names of the three variables Variable = rownames(dsc_male)[1:3] No need to double this, as it is the same for the female. dsc_male$mean[1:3] gets the mean (median, SD, min, max) for each of the three variables. just copy what was done to the males, for the females. d. Use stargazer for pretty tables stargazer( both_dsc, type = &quot;text&quot;, summary = FALSE, title = &quot;Descriptive Statistics: Outcome, Control, and Confounding by Sex&quot;, out=&quot;DS_OCCbySex.txt&quot; ) ## ## Descriptive Statistics: Outcome, Control, and Confounding by Sex ## ================================================================================================================================== ## Variable Mean_Male Mean_Female Median_Male Median_Female SD_Male SD_Female Min_Male Min_Female Max_Male Max_Female ## ---------------------------------------------------------------------------------------------------------------------------------- ## 1 Q37a_HoursWeekChildren 17.299 31.748 14 25 14.636 31.349 1 1 100 168 ## 2 HH1_Num_People 3.622 3.265 4 3 1.307 1.147 1 1 9 7 ## 3 Q7_HoursWeekWork 44.414 35.646 40 40 11.186 11.087 8 5 90 80 ## ---------------------------------------------------------------------------------------------------------------------------------- e. Optional Use kableExtra for even prettier tables both_dsc %&gt;% kable(&quot;html&quot;, escape = FALSE, align = &quot;c&quot;) %&gt;% add_header_above(c(&quot; &quot; = 1, &quot;Mean&quot; = 2, &quot;Median&quot; = 2, &quot;SD&quot; = 2, &quot;Min&quot; = 2, &quot;Max&quot; = 2)) %&gt;% kable_styling(full_width = FALSE, position = &quot;center&quot;, font_size = 12, bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;)) %&gt;% scroll_box(width = &quot;100%&quot;, height = &quot;300px&quot;) Mean Median SD Min Max Variable Mean_Male Mean_Female Median_Male Median_Female SD_Male SD_Female Min_Male Min_Female Max_Male Max_Female Q37a_HoursWeekChildren 17.298630 31.747826 14 25 14.635916 31.34858 1 1 100 168 HH1_Num_People 3.621918 3.265217 4 3 1.307023 1.14679 1 1 9 7 Q7_HoursWeekWork 44.413699 35.645652 40 40 11.186301 11.08683 8 5 90 80 #saving kableExtra table library(kableExtra) library(htmltools) #install tbl &lt;- both_dsc %&gt;% kable(&quot;html&quot;, escape = FALSE, align = &quot;c&quot;) %&gt;% add_header_above(c(&quot; &quot; = 1, &quot;Mean&quot; = 2, &quot;Median&quot; = 2, &quot;SD&quot; = 2, &quot;Min&quot; = 2, &quot;Max&quot; = 2)) %&gt;% kable_styling(full_width = FALSE, position = &quot;center&quot;) save_html(tbl, &quot;DSOCCbySex.html&quot;) For kable(\"html\", escape = FALSE, align = \"c\") is to have html of the table and the escape is that HTML be shown correctly. The alignment is centered but you can change to “r”, “l”. add_header_above() is to create our multi-column header like the one in the table. The argument where \" \" = 1 is the first column and it is empty but the others span 2 columns because we are differentiating between Male and Female. The table is not stretched across the page but it is centered. You can copy-paste this after rendering your Quarto document. The scroll_box is so that we can scroll when we have a lot of columns. 10.8.0.1 Results: Q37a_HoursWeekChildren Females spend more time on caregiving than males; Women, on average, dedicate around twice as many hours to childcare as men in this sample. HH1_Num_People (Control Variable) Household size can affect caregiving time but doesn’t vary by sex in a way that confuses results. Males have slightly larger households, but the difference is small. When comparing caregiving hours between men and women, we account for household size so that differences aren’t simply because one sex lives in larger households. HoursWeekWork (Confounding Variable) Work hours affect caregiving time and vary by sex. Men work slightly more hours per week but the standard deviation show overlap, meaning many men and women work similar hours Because work hours affect caregiving and differ by sex, they could confuse the relationship between sex and childcare. Including work hours in the analysis helps isolate the true effect of sex on caregiving. 10.9 Descriptive Statistics of Categorical/Character Variables Some variables such as EmploymentSituation are in character format. Therefore, to find out more about the variable, we use gtsummary . You can transform these variables to numeric, though, this is also a good way to know how many respondents there are. Dataset_CP_m %&gt;% select(HH2d_EmploymentSituation) %&gt;% tbl_summary() #cewbzgvqtm table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #cewbzgvqtm thead, #cewbzgvqtm tbody, #cewbzgvqtm tfoot, #cewbzgvqtm tr, #cewbzgvqtm td, #cewbzgvqtm th { border-style: none; } #cewbzgvqtm p { margin: 0; padding: 0; } #cewbzgvqtm .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #cewbzgvqtm .gt_caption { padding-top: 4px; padding-bottom: 4px; } #cewbzgvqtm .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #cewbzgvqtm .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #cewbzgvqtm .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #cewbzgvqtm .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #cewbzgvqtm .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #cewbzgvqtm .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #cewbzgvqtm .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #cewbzgvqtm .gt_column_spanner_outer:first-child { padding-left: 0; } #cewbzgvqtm .gt_column_spanner_outer:last-child { padding-right: 0; } #cewbzgvqtm .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #cewbzgvqtm .gt_spanner_row { border-bottom-style: hidden; } #cewbzgvqtm .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #cewbzgvqtm .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #cewbzgvqtm .gt_from_md > :first-child { margin-top: 0; } #cewbzgvqtm .gt_from_md > :last-child { margin-bottom: 0; } #cewbzgvqtm .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #cewbzgvqtm .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #cewbzgvqtm .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #cewbzgvqtm .gt_row_group_first td { border-top-width: 2px; } #cewbzgvqtm .gt_row_group_first th { border-top-width: 2px; } #cewbzgvqtm .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #cewbzgvqtm .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #cewbzgvqtm .gt_first_summary_row.thick { border-top-width: 2px; } #cewbzgvqtm .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #cewbzgvqtm .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #cewbzgvqtm .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #cewbzgvqtm .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #cewbzgvqtm .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #cewbzgvqtm .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #cewbzgvqtm .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #cewbzgvqtm .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #cewbzgvqtm .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #cewbzgvqtm .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #cewbzgvqtm .gt_left { text-align: left; } #cewbzgvqtm .gt_center { text-align: center; } #cewbzgvqtm .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #cewbzgvqtm .gt_font_normal { font-weight: normal; } #cewbzgvqtm .gt_font_bold { font-weight: bold; } #cewbzgvqtm .gt_font_italic { font-style: italic; } #cewbzgvqtm .gt_super { font-size: 65%; } #cewbzgvqtm .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #cewbzgvqtm .gt_asterisk { font-size: 100%; vertical-align: 0; } #cewbzgvqtm .gt_indent_1 { text-indent: 5px; } #cewbzgvqtm .gt_indent_2 { text-indent: 10px; } #cewbzgvqtm .gt_indent_3 { text-indent: 15px; } #cewbzgvqtm .gt_indent_4 { text-indent: 20px; } #cewbzgvqtm .gt_indent_5 { text-indent: 25px; } #cewbzgvqtm .katex-display { display: inline-flex !important; margin-bottom: 0.75em !important; } #cewbzgvqtm div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after { height: 0px !important; } Characteristic N = 1,7231 HH2d_EmploymentSituation     At work as employee or employer/self-employed 870 (50%)     Employed, on child-care leave or other leave 6 (0.3%)     At work as relative assisting on family farm or business 13 (0.8%)     Unemployed less than 12 months 66 (3.8%)     Unemployed 12 months or more 90 (5.2%)     Unable to work due to long-term illness or disability 46 (2.7%)     Retired 520 (30%)     Full time homemaker/ responsible for ordinary shopping and looking after the home 5 (0.3%)     In education (at school, university, etc.) / student 90 (5.2%)     Other 17 (1.0%) 1 n (%) 10.10 Importance of Descriptive Statistics: Descriptive statistics serve as tools to summarize and organize data into meaningful insights, making it easier to understand and communicate findings. The following are the key aspects of descriptive statistics: 10.10.0.1 Definition Descriptive statistics involve summarizing, organizing, and simplifying data in a way that is understandable and interpretable. This can include numerical summaries, tables, and graphical representations to describe the distribution, central tendency, and variability of data. 10.10.1 Key Components of Descriptive Statistics Measures of Central Tendency: Mean: The average value, useful for symmetric distributions without outliers. Median: The middle value in the data, efficient even with skewness and outliers. Mode: The most frequent value, helpful for categorical or multimodal distributions. Measures of Variability: Range: Difference between the largest and smallest values. Shows how wide the data can go. Standard Deviation (SD): Shows how close most values are to the average. A small SD → most numbers are similar; a large SD → numbers vary a lot. Interquartile Range (IQR): Shows the spread of the middle 50% of the data. Focuses on the “typical” range, ignoring extreme values. Standard Error (SE): Shows how precise the mean estimate is; smaller SE → mean is more reliable. Shape of the Data Skewness: Measures asymmetry. Positive –&gt; long right tail; Negative –&gt; long left tail Kurtosis: Measures heaviness of tails. High –&gt; more outliers; low –&gt; flatter distribution Data Distribution: Histograms and frequency distributions help identify the shape (e.g., symmetric, skewed, bimodal) and any potential outliers in the dataset. Data Representation: Graphical methods like bar charts, line graphs, pie charts, and boxplots make patterns and trends in the data more visible and easier to interpret. 10.10.2 When to Use Mean or Median The mean is preferred when the data is symmetrically distributed without extreme values. The median is better for skewed distributions or datasets with significant outliers, as it is not affected by extreme values. Example: In income data, where a few individuals earn significantly more than the majority, the median provides a more realistic “typical” income. 10.10.3 How Descriptive Statistics Help in Regression Choosing Variables to Include: By looking at descriptive stats, we can see which variables affect our outcome and which might mix up relationships. Control variables are factors that only affect the outcome. Example: Household size might affect caregiving hours but is not our main focus. Confounding variables affect both the explanatory variable and the outcome. Example: Hours worked per week can differ by sex and affect caregiving, so we need to account for it. Preparing for Regression Descriptive stats let us check for outliers, skewed data, or missing values before modeling. They help us understand the scale of variables and whether transformations might be needed. They show if control and confounding variables need special attention in our regression models. Takeaway: Descriptive statistics are the first step in regression analysis. 10.10.4 Limitations Descriptive statistics only describe the data at hand; they do not infer or predict trends in the population. Without considering measures of variability (e.g., SD or IQR), central tendency measures (mean, median) can be misleading. They cannot fully reveal causal relationships; confounding can still bias conclusions if not properly addressed. 10.11 Closing Do NOT clean your Environment You will use the datasets for your quiz. "],["descriptive-statistics-continuation.html", "11 Descriptive Statistics Continuation 11.1 Time Series Descriptive Statistics 11.2 Panel Data: Within vs Between Variation", " 11 Descriptive Statistics Continuation For this section, we use built-in datasets in R. rm(list=ls()) gc() ## used (Mb) gc trigger (Mb) max used (Mb) ## Ncells 4040357 215.8 6548841 349.8 6548841 349.8 ## Vcells 8900232 68.0 99463268 758.9 202265969 1543.2 11.0.0.0.1 Built-in Datasets The packages you download sometimes have datasets built-in them to serve as practice. 11.0.0.0.2 Install Packages Some of the packages are already installed; but, just in case, they are included in the code chunk. ch7p &lt;- c( &quot;tidyverse&quot;, &quot;psych&quot;, &quot;plm&quot;, &quot;AER&quot;, &quot;lmtest&quot;, &quot;sandwich&quot;, &quot;forecast&quot;, &quot;wooldridge&quot; ) ch7mp &lt;- ch7p[ !(ch7p %in% installed.packages()[,&quot;Package&quot;]) ] if(length(ch7mp)) { install.packages(ch7mp) } We list the packages we need, then, we locate the missing packages then install them. Finally, we load the libraries of the packages we installed. There will be a lot of warning messages when you install. Always double-check in the Packages list whether the packages were installed. 11.0.0.0.3 Load Packages lapply(ch7p, library, character.only = TRUE) The packages are listed; 11.1 Time Series Descriptive Statistics As a continuation, let us do a deep-dive of Descriptive Statistics focusing on time series data. In a time series, before you do any modeling, you need to understand the structure, trends and stationarity of the series. 11.1.0.1 Key Things to Look For: Trend: Is the series increasing/decreasing over time? Seasonality: Are there repeating patterns? Volatility or Variance Changes: Look for periods of high/low variability. Autocorrelation: Are observations correlated with past values? The patterns inform whether simple OLS is valid or if we need to do transformations (e.g., differencing, logs) 11.1.0.1.1 Example: We use the built-in AirPassengers dataset. This dataset is the monthly airline passenger numbers. data(&quot;AirPassengers&quot;) ch7.ts_data&lt;-AirPassengers ch7.ts &lt;- data.frame( Month = time(ch7.ts_data), Air_Passengers = as.numeric(ch7.ts_data) ) The dataset is loaded through the data command. We are using tidyverse, it expects data frames so we have to change the object (now it is in Values) to data frame. You notice that we extracted the time; it is because we are going to plot and detect trend and seasonality. We also have the number of Air Passengers per month so we set that as numeric. 11.1.0.1.2 Inspect data str(ch7.ts) ## &#39;data.frame&#39;: 144 obs. of 2 variables: ## $ Month : Time-Series from 1949 to 1961: 1949 1949 1949 1949 1949 ... ## $ Air_Passengers: num 112 118 132 129 121 135 148 148 136 119 ... 11.1.1 1. Descriptive Statistics using psych 11.1.1.0.1 Use describe function psych::describe(ch7.ts$Air_Passengers) ## vars n mean sd median trimmed mad min max range ## X1 1 144 280.3 119.97 265.5 271.45 133.43 104 622 518 ## skew kurtosis se ## X1 0.57 -0.43 10 We need to look at the data, whether it is skewed, highly volatile, or has unusual values. We look at Skewness aside from the central tendency measures. If skewness is 1: it is highly skewed. In this case, it is moderately skewed. Usually, the rule of thumb for skewness interpretations is between -0.5 and 0.5 (roughly symmetric). 11.1.2 2. Plot Time Series ggplot(ch7.ts, aes(x=Month, y=Air_Passengers))+ geom_line(color=&quot;darkgreen&quot;)+ geom_smooth(method=&quot;loess&quot;, color=&quot;tomato&quot;) + #smooth trend line labs(title = &quot;AirPassengers with Smooth Trend Line&quot;, subtitle = &quot;From 1949 to 1961&quot;) ## Don&#39;t know how to automatically pick scale for object of ## type &lt;ts&gt;. Defaulting to continuous. ## `geom_smooth()` using formula = &#39;y ~ x&#39; Let us improve the graph; ch7.ts&lt;-ch7.ts %&gt;% mutate( year = floor(Month), month = round((Month - year)*12+1), date = make_date(year=year, month = month, day = 1) ) We round off the Month to only have the year then, we convert the decimal to months by multiplying by 12 and add 1 because January counts as month 1. Unlike what we used before, as.Date, we use make_date because unlike character strings, this time, it is numeric. 11.1.2.0.0.1 Improved Plot ggplot(ch7.ts, aes(x=date, y=Air_Passengers))+ geom_line(color=&quot;darkgreen&quot;)+ geom_smooth(method=&quot;loess&quot;, color=&quot;tomato&quot;, se=FALSE) + #smooth trend line scale_x_date(date_labels = &quot;%Y&quot;, date_breaks = &quot;1 year&quot;) + labs(title = &quot;AirPassengers with Trend Line&quot;, subtitle = &quot;From 1949 to 1961&quot;, x = &quot;Year&quot;, y = &quot;Passengers&quot;)+ theme_classic() ## `geom_smooth()` using formula = &#39;y ~ x&#39; #for base R plotting fit &lt;- loess(y ~ x) ord &lt;- order(x) lines(x[ord], fitted(fit)[ord], col = &quot;tomato&quot;, lwd = 2) geom_line shows raw counts of Air Passengers geom_smooth(method = \"loess\") fits a smooth curve to highlight trend, ignoring short-term fluctuations LOESS is local regression wherein what happens is that, it can capture curves, trends and nonlinearity. It can be seen that the trend in Air Passengers accelerates then slows down. You can see peaks/troughs, therefore, LOESS is better over linear regression. scale_x_date is just to show every year, not just some years in x-axis. You can add months here by putting inside the parenthesis, %b but it looks cluttered so I did not put it. 11.1.3 3. Autocorrelation Detect if observations are correlated over time, which violates OLS independence assumption. acf(ch7.ts$Air_Passengers, main=&quot;ACF: Monthly AirPassengers&quot;) pacf(ch7.ts$Air_Passengers, main=&quot;PACF: Monthly AirPassengers&quot;) How to Interpret: Plot Significant? Implication for OLS/modeling ACF Spike outside blue dashed lines Residuals may be autocorrelated so this violates OLS independence assumption PACF Spike outside blue dashed lines Direct lagged effect so we may need to lag regressors in OLS Multiple PACF Lags Multiple spikes beyond bounds OLS residuals are correlated so we have to consider other models like ARIMA, dynamic regression or IV In summary, Spikes in PACF of residuals -&gt; OLS standard errors may be invalid so, need to lag variables or do IV. 11.2 Panel Data: Within vs Between Variation You might wonder why we have the within vs between variation; Note the following: Within: Variation of an individual over time Example: NCR GDP changes from year to year Useful when estimating fixed-effects models because we want to isolate how changes within an individual/entity affect the dependent variable Between: Variation across individuals over time Example: Average GDP differs across NCR, CAR, BARMM Useful when estimating random-effects models or when comparing differences across individuals/entities Note: If you ignore within vs between variation, OLS estimates will be biased. Fixed-effects OLS = controls for unobserved, time-invariant heterogeneity so uses within variation only. What is time-invariant heterogeneity? A constant, hidden characteristic of entities in panel data that do not change over time. 11.2.0.0.1 Example: We use the Grunfeld Dataset which is built-in the plm package that is used for Panel Data modeling data(&quot;Grunfeld&quot;, package=&quot;plm&quot;) 11.2.0.0.2 Inspect data str(Grunfeld) ## &#39;data.frame&#39;: 200 obs. of 5 variables: ## $ firm : int 1 1 1 1 1 1 1 1 1 1 ... ## $ year : int 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 ... ## $ inv : num 318 392 411 258 331 ... ## $ value : num 3078 4662 5387 2792 4313 ... ## $ capital: num 2.8 52.6 156.9 209.2 203.4 ... 11.2.0.1 1. Plot Panel Data 11.2.0.1.1 a. Within variation: investment over time per firm ggplot(Grunfeld, aes(x=year, y=inv, group=firm, color=factor(firm)))+ geom_line()+ labs(title=&quot;Within variation: investment over time by firm&quot;, x=&quot;Year&quot;, y=&quot;Investment&quot;, color = &quot;Firm&quot;)+ theme_classic() Some firms show large changes but most of them show little variation. For these firms, it shows changes within the firm but for the others, not really. 11.2.0.1.2 b. Between variation Grunfeld %&gt;% group_by(firm) %&gt;% summarise(mean_invest = mean(inv)) %&gt;% ggplot(aes(x=firm, y=mean_invest)) + geom_col(fill=&quot;purple&quot;) + labs(title=&quot;Between Variation: Average Investment by Firm&quot;, x=&quot;Firm&quot;, y=&quot;Avg Investment&quot;)+ theme_bw() Shows how firms 1 and 2 have much higher average investment than the others. Unlike before, the between variation here looks at differences in average across firms, ignoring time. Important: Choosing the right model (FE vs RE) depends on whether your research question is about changes within entities or differences across entities. "],["hypothesis-testing.html", "12 Hypothesis Testing 12.1 Writing a Hypothesis for a Study 12.2 Test of Proportions 12.3 T-Tests 12.4 Chi-Square Test", " 12 Hypothesis Testing 12.1 Writing a Hypothesis for a Study 12.1.1 Steps: 12.1.1.1 1. Identify the research question: What relationship or difference are you testing? Example: Does the minimum wage increase affect average worker wages? 12.1.1.2 2. State the null hypothesis Typically, no effect, no difference, status quo \\(H_0\\) : The minimum wage policy has no effect on wages. 12.1.1.3 3. State the alternative hypothesis Typically, effect exists, difference exists \\(H_\\alpha\\) : The minimum wage policy increases wages. 12.1.1.4 4. Decide direction: One-tailed –&gt; effect expected in a specific direction Two-tailed–&gt; effect could be in either direction Tip: Good hypotheses are CMT: Clear, Measurable, and Testable. 12.1.1.5 Difference between one-tailed test and two-tailed test A two-tailed test checks whether a sample mean or proportion is significantly different from a given value in either direction (higher or lower). Null Hypothesis (H0​): There is no difference between the sample statistic and the population value. Alternative Hypothesis (\\(H_\\alpha\\)): The sample statistic is either higher or lower than the population value. A one-tailed test checks if a sample statistic is significantly greater or smaller than a given value in one direction only. Null Hypothesis (H0): There is no difference OR the difference is in the opposite direction. Alternative Hypothesis (\\(H_\\alpha\\)): The sample statistic is either higher or lower, but not both. 12.1.1.6 Deciding between One-Tailed versus Two-Tailed A central bank claims that the country’s average annual inflation rate is 3%. An economist wants to test whether this is incorrect. A government introduces a new minimum wage policy, and policymakers want to check whether it has decreased employment levels. A researcher wants to compare the GDP growth rate of two neighboring countries to see if they are different. An economist wants to determine whether male workers earn more than female workers in the same industry. A researcher wants to test whether developing countries receive less Foreign Direct Investment (FDI) than developed countries. 12.2 Test of Proportions A test of proportion is used when we want to check whether the proportion of a certain event (e.g., unemployment rate, poverty rate) in a sample is different from a known proportion in the population. 12.2.1 One-Sample Test of Proportion We use this test when we want to see if the proportion of a sample is different from the population proportion. Example: Unemployment Rate A government report claims that 8% of the labor force is unemployed. We collect a sample of 500 workers and find that 50 are unemployed. Is the actual unemployment rate significantly different from 8%? prop.test(x = 50, n = 500, p = 0.08, correct = FALSE) ## ## 1-sample proportions test without continuity ## correction ## ## data: 50 out of 500, null probability 0.08 ## X-squared = 2.7174, df = 1, p-value = 0.09926 ## alternative hypothesis: true p is not equal to 0.08 ## 95 percent confidence interval: ## 0.07667756 0.12942191 ## sample estimates: ## p ## 0.1 This employs prop.test then inside the parenthesis, we have x=Number of successes, n=total sample size and p=hypothesized population proportion. The correct=FALSE adjusts the t-statistic. The rule is to set it to TRUE for small sample sizes and FALSE for big sample sizes. If the p-value &lt; 0.05, we reject the null hypothesis, meaning the unemployment rate in our sample is significantly different from 8%. If the p-value &gt; 0.05, we fail to reject the null hypothesis, meaning we do not have enough evidence to say the unemployment rate differs from 8%. If we believe the true unemployment rate is higher than 8% prop.test(x = 50, n = 500, p = 0.08, alternative = &quot;greater&quot;, correct = FALSE) ## ## 1-sample proportions test without continuity ## correction ## ## data: 50 out of 500, null probability 0.08 ## X-squared = 2.7174, df = 1, p-value = 0.04963 ## alternative hypothesis: true p is greater than 0.08 ## 95 percent confidence interval: ## 0.08003919 1.00000000 ## sample estimates: ## p ## 0.1 Since it is one-tailed, alternative = \"greater\" tests the direction. The alternative is changed if it is in the opposite direction. If we believe the true unemployment rate is lower than 8% prop.test(x = 50, n = 500, p = 0.08, alternative = &quot;less&quot;, correct = FALSE) ## ## 1-sample proportions test without continuity ## correction ## ## data: 50 out of 500, null probability 0.08 ## X-squared = 2.7174, df = 1, p-value = 0.9504 ## alternative hypothesis: true p is less than 0.08 ## 95 percent confidence interval: ## 0.0000000 0.1242664 ## sample estimates: ## p ## 0.1 12.2.2 Two-Sample Test of Proportion We use this when comparing proportions between two groups (e.g., unemployment rates in two different regions). 12.2.2.1 Example: Employment Rate Comparison A researcher wants to compare the employment rates between urban and rural areas. In an urban area, 450 out of 500 people are employed. In a rural area, 420 out of 500 people are employed. We test whether the employment rates are significantly different. prop.test(x = c(450, 420), n = c(500, 500), correct = FALSE) ## ## 2-sample test for equality of proportions without ## continuity correction ## ## data: c(450, 420) out of c(500, 500) ## X-squared = 7.9576, df = 1, p-value = 0.004789 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## 0.01847836 0.10152164 ## sample estimates: ## prop 1 prop 2 ## 0.90 0.84 The x= number of successes in the two groups, n=total number of sample sizes in the two groups. Interpreting the Results: If p-value &lt; 0.05: Employment rates differ significantly between urban and rural areas. If p-value &gt; 0.05: There is no significant difference in employment rates. If we believe urban employment rate is higher than rural: prop.test(x = c(450, 420), n = c(500, 500), alternative = &quot;greater&quot;, correct = FALSE) ## ## 2-sample test for equality of proportions without ## continuity correction ## ## data: c(450, 420) out of c(500, 500) ## X-squared = 7.9576, df = 1, p-value = 0.002394 ## alternative hypothesis: greater ## 95 percent confidence interval: ## 0.02515394 1.00000000 ## sample estimates: ## prop 1 prop 2 ## 0.90 0.84 If we believe urban employment rate is lower than rural: prop.test(x = c(450, 420), n = c(500, 500), alternative = &quot;less&quot;, correct = FALSE) ## ## 2-sample test for equality of proportions without ## continuity correction ## ## data: c(450, 420) out of c(500, 500) ## X-squared = 7.9576, df = 1, p-value = 0.9976 ## alternative hypothesis: less ## 95 percent confidence interval: ## -1.00000000 0.09484606 ## sample estimates: ## prop 1 prop 2 ## 0.90 0.84 12.2.3 Real-World Example: We are going to use the wage1 dataset from Wooldridge package but we do not want to do anything to the original dataset (though technically we can) but, better safe than sorry. library(wooldridge) data(wage1) sample&lt;-wage1 Is the proportion of female workers greater than 50%? prop.test(sum(sample$female), nrow(sample), p = 0.5, alternative = &quot;greater&quot;, correct = FALSE) ## ## 1-sample proportions test without continuity ## correction ## ## data: sum(sample$female) out of nrow(sample), null probability 0.5 ## X-squared = 0.92015, df = 1, p-value = 0.8313 ## alternative hypothesis: true p is greater than 0.5 ## 95 percent confidence interval: ## 0.443458 1.000000 ## sample estimates: ## p ## 0.4790875 12.3 T-Tests A t-test is used to compare means (e.g., wages, GDP growth). Types of T-Tests: One-Sample T-Test: Compares a sample mean to a known value. Independent-Samples T-Test: Compares means of two different groups. Paired-Samples T-Test: Compares before and after effects within the same group. 12.3.1 One-Sample T-Test A report states that the average monthly wage in a country is 20,500. We take a sample of 100 workers and test if their wages differ. set.seed(123) wages&lt;-rnorm(100, mean=21000, sd=500) t.test(wages, mu=20500) ## ## One Sample t-test ## ## data: wages ## t = 11.946, df = 99, p-value &lt; 0.00000000000000022 ## alternative hypothesis: true mean is not equal to 20500 ## 95 percent confidence interval: ## 20954.64 21135.76 ## sample estimates: ## mean of x ## 21045.2 The rnorm generates a random sample of 100 wages. Then the t.test performs a one-sample t-test by comparing if the sample mean (21000) is different from the hypothesized mean (20500) Interpreting the results: If p-value &lt; 0.05: The average wage is significantly different from 20,500. If p-value &gt; 0.05: There is no significant difference in wages. 12.3.2 Real-World Example Is the average hourly wage significantly different from $5 per hour? t.test(sample$wage, mu = 5) ## ## One Sample t-test ## ## data: sample$wage ## t = 5.5649, df = 525, p-value = 0.00000004186 ## alternative hypothesis: true mean is not equal to 5 ## 95 percent confidence interval: ## 5.579768 6.212437 ## sample estimates: ## mean of x ## 5.896103 12.3.3 Independent-Samples T-Test We compare male and female wages in the same industry. set.seed(123) male_wage&lt;-rnorm(50, mean=23000, sd=600) female_wage&lt;-rnorm(50, mean=22500, sd=500) t.test(male_wage, female_wage, var.equal=TRUE) ## ## Two Sample t-test ## ## data: male_wage and female_wage ## t = 4.4149, df = 98, p-value = 0.00002603 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 246.3177 648.5583 ## sample estimates: ## mean of x mean of y ## 23020.64 22573.20 This performs a two-sample t-test assuming equal variances. Interpreting the results: If p-value &lt; 0.05, wages differ significantly. If p-value &gt; 0.05, there is no significant difference. 12.3.3.1 Checking variance before running the T-Test Variance measures how spread out the data is around the mean. Low variance → Data points are close to the mean. High variance → Data points are spread out from the mean. var.test(male_wage, female_wage) ## ## F test to compare two variances ## ## data: male_wage and female_wage ## F = 1.5057, num df = 49, denom df = 49, p-value = ## 0.1556 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.8544445 2.6533137 ## sample estimates: ## ratio of variances ## 1.505692 If variance ratio is close to 1, assume equal variances (var.equal = TRUE). 12.3.4 Real-World Example Do men and women earn different wages? #check variance var.test(sample$wage[sample$female == 1], sample$wage[wage1$female == 0]) ## ## F test to compare two variances ## ## data: sample$wage[sample$female == 1] and sample$wage[wage1$female == 0] ## F = 0.36954, num df = 251, denom df = 273, p-value = ## 0.000000000000004813 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.2900223 0.4714405 ## sample estimates: ## ratio of variances ## 0.3695359 Interpreting the variance ratio: Null Hypothesis (H0): The variances of male and female wages are equal Alternative Hypothesis (\\(H_\\alpha\\)): The variances of male and female wages are not equal t.test(sample$wage ~ sample$female, var.equal = FALSE) ## ## Welch Two Sample t-test ## ## data: sample$wage by sample$female ## t = 8.44, df = 456.33, p-value = ## 0.0000000000000004243 ## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0 ## 95 percent confidence interval: ## 1.926971 3.096690 ## sample estimates: ## mean in group 0 mean in group 1 ## 7.099489 4.587659 ~ Means “wage depends on gender” (separates the dependent and independent variable) 12.3.5 Paired-Samples T-Test A new minimum wage policy was introduced. We analyze its impact on worker wages before and after implementation. set.seed(0) wage_before &lt;- rnorm(50, mean = 32000, sd = 400) wage_after &lt;- wage_before + rnorm(50, mean = 2500, sd = 100) t.test(wage_before, wage_after, paired = TRUE) ## ## Paired t-test ## ## data: wage_before and wage_after ## t = -206.45, df = 49, p-value &lt; 0.00000000000000022 ## alternative hypothesis: true mean difference is not equal to 0 ## 95 percent confidence interval: ## -2526.496 -2477.785 ## sample estimates: ## mean difference ## -2502.141 This means that wage_after was simply the wages after the policy were increased by approx. 2500 per worker. The t.test simply performs a paired t-test that is why paired=TRUE Interpreting the Results: If p-value &lt; 0.05: The minimum wage policy significantly increased wages. If p-value &gt; 0.05: The policy had no significant effect. Did wages increase? t.test(wage_before, wage_after, paired = TRUE, alternative = &quot;greater&quot;) ## ## Paired t-test ## ## data: wage_before and wage_after ## t = -206.45, df = 49, p-value = 1 ## alternative hypothesis: true mean difference is greater than 0 ## 95 percent confidence interval: ## -2522.46 Inf ## sample estimates: ## mean difference ## -2502.141 12.4 Chi-Square Test This tests association between two categorical variables. \\(H_0\\) variables are independent \\(H_\\alpha\\) variables are associated This is often used in surveys or panel data with categorical outcomes. Example, Is marital status associated with working in trade? chisq.test(table(sample$married, sample$trade)) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity ## correction ## ## data: table(sample$married, sample$trade) ## X-squared = 9.2022, df = 1, p-value = 0.002417 We perform Chi-square test of independence between employment status and gender. We use table to count for each combination of categories (cross-tabulation). The chi-square test is for seeing if these two variables are independent. The results tell us that marital status and working in trade are associated. This simply tells us that distribution of trade employment differs between married and unmarried individuals. However, chi-square does not show the likelihood, the magnitude of the difference and whether marriage causes industry choice. married may be a confounding variable! because when you ignore it, you might encounter omitted variable bias if marriage influences both trade and wages. You can look at the contingency table to see which group drives the association. table(sample$married, sample$trade) ## ## 0 1 ## 0 131 75 ## 1 244 76 prop.table(table(sample$married, sample$trade), margin=1) ## ## 0 1 ## 0 0.6359223 0.3640777 ## 1 0.7625000 0.2375000 The first table, the first column is for married while the first row is for trade (the number of observations). The next table is the proportions, with each row sums to 1. We are NOT saying that marriage causes people to work in trade. NO; we are only showing a descriptive association. 12.4.1 Steps in Hypothesis Testing State hypotheses Choose test Perform the test Examine p-value and confidence intervals Draw conclusion - reject or fail to reject null hypothesis Make a statement about your data - “From our sample, we have evidence that the minimum wage policy increased wages.” "],["ordinary-least-squares.html", "13 Ordinary Least Squares 13.1 Bias in OLS 13.2 Endogeneity 13.3 General Description and notation of a Linear Regression 13.4 Running and reporting OLS Regression 13.5 Interpretation of Coefficients 13.6 Goodness of Fit (F-Statistics and Adjusted R-squared) 13.7 Assumption Diagnostics 13.8 Real-World Example 13.9 Detecting Endogeneity", " 13 Ordinary Least Squares Ordinary Least Squares is a method for estimating the relationship between variables using a linear regression model: \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where: \\(Y\\) = outcome (dependent variable) \\(X\\) = explanatory variable(s) (independent variable/s) \\(\\beta\\) = coefficients we want to estimate \\(\\epsilon\\) = error term (everything affecting Y that is not X) OLS chooses coefficient estimates that minimize the sum of squared residuals: \\(\\sum (Y_i-\\hat{Y}_i)^2\\) 13.0.1 Key Assumptions Behind OLS The most important assumption for unbiased estimates: \\(E[\\epsilon|X] = 0\\) Meaning: The error term is NOT correlated with the explanatory variables. If this holds: OLS estimates are unbiased. Interpretation of coefficients as causal effects becomes possible 13.1 Bias in OLS Bias occurs when the expected value of an estimator differs from the true parameter: \\(E[\\hat{\\beta}]\\neq\\beta\\) What happens is that your regression overestimates or underestimates the true effect, (doesn’t show the effect!) 13.1.0.1 Why Bias Occurs? Bias occurs when OLS assumptions fail. The most common reason is that there is correlation between X’s and the error term. This leads to…endogeneity 13.2 Endogeneity A variable is endogenous if: \\(Cov(X,\\epsilon) \\neq 0\\) Some part of X is related to unbserved factors affecting Y. This violates the OLS assumption. 13.2.1 Sources of Endogeneity Omitted Variable Bias You leave out an important variable that affects both X and Y. Example: Studying effect of education on wages but you did not include experience. Experience affects both education (X) and wages (Y) So, experience goes into the error term and correlates with X. Reverse Causality Simply, Y affects X Example: Health influences income but income also influences health. Measurement Error 13.2.2 Consequence: OLS is misleading! 13.2.3 How Everything Works OLS requires exogeneity –&gt; FAILS when X correlates with \\(\\epsilon\\) –&gt; endogeneity –&gt; CREATES bias –&gt; estimates cannot be interpreted causally. Without diagnosing forour endogeneity, our significant coefficients may be wrong and causal interpretation fails. :( 13.3 General Description and notation of a Linear Regression In R, the set.seed() function is used to set the random number generator’s seed. Random number generators in computer programming are not truly random; they are pseudo-random, meaning they generate sequences of numbers that appear random but are actually determined by an initial starting point called a seed. If you set the seed to a specific value, you can reproduce the same sequence of random numbers every time you run your program. This is important in many data analysis and simulation tasks, as it allows for the results to be reproducible. Note: we will use the cat() function today. The cat() function is used for printing or concatenating objects to the console. It is primarily used to display text, numbers, or other R objects on the screen or to write them to a file. The name “cat” stands for “concatenate” or “concatenation,” as the function can be used to combine and display multiple objects. rm(list = setdiff(ls(), &quot;sample&quot;)) gc() ## used (Mb) gc trigger (Mb) max used (Mb) ## Ncells 4038120 215.7 6548841 349.8 6548841 349.8 ## Vcells 8846280 67.5 79570615 607.1 202265969 1543.2 # Generate example data set.seed(123) n &lt;- 50 x &lt;- runif(n, 0, 10) y &lt;- 2 * x + rnorm(n, 0, 2) # Create a scatterplot ggplot(data = data.frame(x, y), aes(x = x, y = y)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;blue&quot;) + labs(title = &quot;Scatterplot of Simulated Data&quot;, x = &quot;X&quot;, y = &quot;Y&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; 13.3.1 The notation cat(&quot;Linear Regression Notation:\\n&quot;) ## Linear Regression Notation: cat(&quot;Y = β₀ + β₁*X + ε\\n&quot;) ## Y = β₀ + β₁*X + ε cat(&quot;Y: Dependent variable (response)\\n&quot;) ## Y: Dependent variable (response) cat(&quot;X: Independent variable (predictor)\\n&quot;) ## X: Independent variable (predictor) cat(&quot;β₀: Intercept (constant)\\n&quot;) ## β₀: Intercept (constant) cat(&quot;β₁: Coefficient for X (slope)\\n&quot;) ## β₁: Coefficient for X (slope) cat(&quot;ε: Error (residuals)\\n&quot;) ## ε: Error (residuals) 13.4 Running and reporting OLS Regression In R, the lm() function is used to fit linear regression models. Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables by finding the best-fitting linear equation. The lm() function is a fundamental tool for performing linear regression analysis in R. Here’s how to use the lm() function in R: model &lt;- lm(dependent_variable ~ independent_variable1 + independent_variable2, data = your_data_frame) Let’s break down the components of this function: dependent_variable: This is the variable you want to predict or model. It is the response variable, typically numeric. independent_variable1, independent_variable2, …: These are the variables that you believe are related to the dependent variable. They are the predictor variables, which can be numeric or categorical. data: This argument specifies the data frame in which the variables are located. You should specify the name of your data frame here. The lm() function returns a linear regression model object, which contains information about the estimated coefficients and other statistical details of the model. After fitting the model, you can access various information and summaries, such as: summary(model): This function provides a detailed summary of the regression model, including coefficients, standard errors, t-values, p-values, R-squared, and more. coef(model): This function returns the estimated coefficients of the model. predict(model, newdata): You can use this function to make predictions with the model, where newdata is a data frame containing the values of the independent variables for which you want to make predictions. # Fit an OLS regression model model &lt;- lm(y ~ x) # Display the model summary summary(model) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.5116 -1.1157 -0.1313 1.0985 4.3723 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.2847 0.5442 -0.523 0.603 ## x 2.0764 0.0913 22.743 &lt;0.0000000000000002 ## ## (Intercept) ## x *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.881 on 48 degrees of freedom ## Multiple R-squared: 0.9151, Adjusted R-squared: 0.9133 ## F-statistic: 517.2 on 1 and 48 DF, p-value: &lt; 0.00000000000000022 13.5 Interpretation of Coefficients For numeric independent variables, a positive coefficient means that an increase in the independent variable is associated with an increase in the dependent variable, and a negative coefficient means the opposite. For categorical independent variables (e.g., dummies for categories), the coefficients represent the difference in the dependent variable for that category compared to the reference category. # Extract and interpret coefficients cat(&quot;Intercept (β₀): &quot;, coef(model)[1], &quot;\\n&quot;) ## Intercept (β₀): -0.2847054 cat(&quot;Coefficient for X (β₁): &quot;, coef(model)[2], &quot;\\n&quot;) ## Coefficient for X (β₁): 2.076349 13.6 Goodness of Fit (F-Statistics and Adjusted R-squared) The null hypothesis (H0) for the F-test is that all coefficients in the model are equal to zero, meaning that none of the independent variable/s have any effect on the dependent variable. The small p-value (&lt;0.05) means we can reject the null hypothesis. Note: to extract p-value of F-statistic you have to use the function pf() that gives the area, for the F distribution, to the left of the value x for two given degrees of freedom, df1 and df2. # Adjusted R-squared cat(&quot;Adjusted R-squared: &quot;, summary(model)$adj.r.squared, &quot;\\n&quot;) ## Adjusted R-squared: 0.9133112 # Print the F-statistic and p-value cat(&quot;F-statistic:&quot;, summary(model)$fstatistic[1], &quot;\\n&quot;) ## F-statistic: 517.2401 p_val = pf(summary(model)$fstatistic[1],summary(model)$fstatistic[2],summary(model)$fstatistic[3],lower.tail=FALSE) cat(&quot;p-value:&quot;, p_val, &quot;\\n&quot;) ## p-value: 0.000000000000000000000000002364413 13.7 Assumption Diagnostics In R, when you create a plot of a linear regression model (an OLS model) using the plot() function, it produces a set of diagnostic plots to help you assess the assumptions and goodness of fit of the model. These diagnostic plots are typically used to check if the model is a good fit for the data and whether the assumptions underlying linear regression are met. Here’s an explanation of each output produced by plot(model): Residuals vs. Fitted Values (Partial Regression Plot): This plot shows the relationship between the residuals (the differences between the observed values and the predicted values) and the fitted values (the predicted values from the model). It is used to check for linearity, which is one of the key assumptions of linear regression. Ideally, you want to see a random scattering of points around a horizontal line, with no clear pattern. If you see a pattern, such as a curve or funnel shape, it may indicate a violation of the linearity assumption. Normal Q-Q (Quantile-Quantile) Plot: This plot compares the distribution of the residuals to a theoretical normal distribution. If the residuals are normally distributed, the points should roughly follow a straight line. Deviations from a straight line may indicate departures from normality in the residuals. For example, if the points bend upwards or downwards, it may indicate skewness or heavy-tailed residuals. Scale-Location (Spread-Location) Plot: This plot shows the square root of the absolute residuals against the fitted values. It is also known as a “spread vs. location” plot. It is used to check for homoscedasticity, which means that the variability of the residuals is roughly constant across different levels of the fitted values. Ideally, you want to see a random scattering of points around a horizontal line with roughly equal spread. If the spread changes systematically with the fitted values (e.g., a funnel shape), it may indicate heteroscedasticity, which can lead to unreliable parameter estimates and hypothesis tests. Residuals vs. Leverage Plot (Cook’s Distance Plot): This plot displays the standardized residuals against the leverage values (a measure of how much an observation affects the regression coefficients). It is often used to identify influential data points. Data points that are far from the other points in this plot have high leverage, and points above a certain threshold may be influential. Influential points can have a significant impact on the model’s parameters, and it’s important to examine them for potential outliers or errors. Cook’s Distance Plot: Cook’s distance is a measure of the impact of each observation on the model’s coefficients. This plot shows the Cook’s distance for each observation. Observations with a high Cook’s distance are potential outliers or influential points that can significantly affect the model. It is a useful tool for identifying data points that should be investigated further. Multicollinearity Correlation Matrix: A simple way to start is by looking at the correlation matrix of the predictors. High correlation coefficients (close to -1 or 1) between pairs of predictors indicate potential multicollinearity. This can be done using the cor() function. # Check OLS assumptions par(mfrow = c(2, 2)) plot(model) 13.8 Real-World Example We are going to use the sample dataset sample_sub&lt;-sample %&gt;% select(c(wage,educ,exper,tenure)) head(sample_sub) ## wage educ exper tenure ## 1 3.10 11 2 0 ## 2 3.24 12 22 2 ## 3 3.00 11 2 0 ## 4 6.00 8 44 28 ## 5 5.30 12 7 2 ## 6 8.75 16 9 8 Create a scatterplot # Create a scatterplot ggplot(data = sample_sub, aes(x = sample_sub$educ, y =sample_sub$wage)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;blue&quot;) + labs(title = &quot;Scatterplot of Wage Data&quot;, x = &quot;Education&quot;, y = &quot;Wage&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; # Fit an OLS regression model model.1 &lt;- lm(wage ~ educ, data=sample_sub) # Display the model summary summary(model.1) ## ## Call: ## lm(formula = wage ~ educ, data = sample_sub) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.3396 -2.1501 -0.9674 1.1921 16.6085 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.90485 0.68497 -1.321 0.187 ## educ 0.54136 0.05325 10.167 &lt;0.0000000000000002 ## ## (Intercept) ## educ *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.378 on 524 degrees of freedom ## Multiple R-squared: 0.1648, Adjusted R-squared: 0.1632 ## F-statistic: 103.4 on 1 and 524 DF, p-value: &lt; 0.00000000000000022 13.8.0.1 Coefficients: # Extract and interpret coefficients cat(&quot;Intercept (β₀): &quot;, coef(model.1)[1], &quot;\\n&quot;) ## Intercept (β₀): -0.9048516 cat(&quot;Coefficient for X (β₁): &quot;, coef(model.1)[2], &quot;\\n&quot;) ## Coefficient for X (β₁): 0.5413593 13.8.0.2 Adjusted R-Squared and Goodness of Fit # Adjusted R-squared cat(&quot;Adjusted R-squared: &quot;, summary(model)$adj.r.squared, &quot;\\n&quot;) ## Adjusted R-squared: 0.9133112 # Print the F-statistic and p-value cat(&quot;F-statistic:&quot;, summary(model)$fstatistic[1], &quot;\\n&quot;) ## F-statistic: 517.2401 p_val = pf(summary(model)$fstatistic[1],summary(model)$fstatistic[2],summary(model)$fstatistic[3],lower.tail=FALSE) cat(&quot;p-value:&quot;, p_val, &quot;\\n&quot;) ## p-value: 0.000000000000000000000000002364413 13.8.0.3 Assumption Diagnostics # Check OLS assumptions par(mfrow = c(2, 2)) plot(model.1) 13.9 Detecting Endogeneity There is no automatic test for OVB, usually, researchers rely on existing theories, use different research strategies or compare alternative models. "],["introduction-to-instrumental-variables.html", "14 Introduction to Instrumental Variables", " 14 Introduction to Instrumental Variables When OLS fails due to endogeneity, IV estimation helps bring back causal effects. 14.0.1 Idea: Use a third variable (instrument Z) that: is correlated with X (relevance condition) does NOT affect Y except through X (exclusion restriction) Is uncorrelated with the error term. The instrument isolates variation in X that is not contaminated by omitted variables. Instead of using all variation in X, IV uses only exogenous variation. 14.0.1.1 Simulated Instrument n&lt;-500 #sample size ability&lt;-rnorm(n) instrument&lt;-rnorm(n) educ_iv&lt;-2+instrument+ability+rnorm(n) wage&lt;-5+3*educ_iv+2*ability+rnorm(n) ivd&lt;-data.frame(wage,educ_iv, instrument) We generate ability, our unobserved factor affecting both education and wage (this is what creats endogeneity). We generate our instrument variable Z, must influence education but not directly wage. We build our education_iv which depends on instrument (relevance condition) and ability (source of endogeneity) plus random noise. We also simulate wage. 14.0.1.2 Running IV regression library(AER) ivmodel&lt;-ivreg(wage~educ_iv | instrument, data = ivd) We use the AER package which has functions for instrumental variable regression. We use ivreg to do instrumental variable regression. Inside the parenthesis: outcome ~ endogenous_var | instrument. The vertical bar separates regressors from instruments. summary(ivmodel) ## ## Call: ## ivreg(formula = wage ~ educ_iv | instrument, data = ivd) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.74777 -1.52934 0.04787 1.33048 6.78178 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.06622 0.21581 23.48 &lt;0.0000000000000002 ## educ_iv 3.02180 0.09399 32.15 &lt;0.0000000000000002 ## ## (Intercept) *** ## educ_iv *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.102 on 498 degrees of freedom ## Multiple R-Squared: 0.8951, Adjusted R-squared: 0.8949 ## Wald test: 1034 on 1 and 498 DF, p-value: &lt; 0.00000000000000022 A one-unit increase in education is associated with a 3.02 increase in wage, and is statistically significant. The p-value of 0.895 shows that there is strong evidence of a causal effect from the model. Despite \\(R^2\\) being a measure of goodness-of-fit, in IV, where it does not minimize residual variance, some \\(R^2\\) becomes negative. As a whole, \\(R^2\\) is not a useful tool for goodness of fit in IV. 14.0.1.3 Diagnostics summary(ivmodel, diagnostics=TRUE) ## ## Call: ## ivreg(formula = wage ~ educ_iv | instrument, data = ivd) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.74777 -1.52934 0.04787 1.33048 6.78178 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.06622 0.21581 23.48 &lt;0.0000000000000002 ## educ_iv 3.02180 0.09399 32.15 &lt;0.0000000000000002 ## ## (Intercept) *** ## educ_iv *** ## ## Diagnostic tests: ## df1 df2 statistic p-value ## Weak instruments 1 498 252.87 &lt;0.0000000000000002 *** ## Wu-Hausman 1 497 88.01 &lt;0.0000000000000002 *** ## Sargan 0 NA NA NA ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.102 on 498 degrees of freedom ## Multiple R-Squared: 0.8951, Adjusted R-squared: 0.8949 ## Wald test: 1034 on 1 and 498 DF, p-value: &lt; 0.00000000000000022 Weak instruments tests if instrument is weak or instrument is strong. The instrument in this case, does not strongly predict education. The IV here is weak, leading to biased estimates. Wu-Hausman tests for endogeneity. The alternative is that endogeneity is present. There is endogeneity here (education is endogenous in the model). Sargan tests for multiple instruments (to be discussed later) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
